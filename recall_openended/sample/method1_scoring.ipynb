{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3d2bc0",
   "metadata": {},
   "source": [
    "# Method 1: deterministic scoring method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90120fa",
   "metadata": {},
   "source": [
    "* **Inputs**\n",
    "\n",
    "  * Raw data: open-ended recall responses with `respondent`, `group`, `questionnaire`, `question_code`, `question`, `form` (Long/Short), `title`, `response`.\n",
    "  * Model answers: markdown file with **chronological numbered events** for *Mad Max – Long* and *Mad Max – Short*.\n",
    "\n",
    "* **Event mapping**\n",
    "\n",
    "  * Extract numbered events from the `Mad Max - Long Form` section (up to 46 events).\n",
    "  * Extract numbered events from the `Mad Max: Fury Road – Short Form Scene Events` section (up to 46 events; padded with `None` where fewer).\n",
    "  * For each row:\n",
    "\n",
    "    * If `title` contains “Mad Max” and `form == \"Short\"` → use short-form events.\n",
    "    * Otherwise (Mad Max Long) → use long-form events.\n",
    "\n",
    "* **Text preprocessing**\n",
    "\n",
    "  * Convert both responses and events to lowercase.\n",
    "  * Remove punctuation.\n",
    "  * Split into tokens and remove common stopwords.\n",
    "  * Represent each event and each response as a **set of tokens**.\n",
    "\n",
    "* **Per-event scoring (T1–T46)**\n",
    "\n",
    "  * For each event (T_i), compute:\n",
    "\n",
    "    * Token overlap between response and event: intersection + Jaccard similarity (`|intersection| / |event_tokens|`).\n",
    "  * **Accuracy (T_i_Accuracy)**:\n",
    "\n",
    "    * Set to `1` if:\n",
    "\n",
    "      * At least 2 overlapping tokens, **and**\n",
    "      * Jaccard similarity ≥ 0.12.\n",
    "    * Otherwise `0`.\n",
    "    * If no event defined for that index (padded), set `NaN`.\n",
    "  * **Detail (T_i_Detail)**:\n",
    "\n",
    "    * Only considered if `T_i_Accuracy == 1`.\n",
    "    * Set to `1` if:\n",
    "\n",
    "      * The response contains **any detail keyword** (e.g., character names, locations, key objects like “Furiosa”, “Immortan Joe”, “War Rig”, “Citadel”, etc.), **or**\n",
    "      * There are ≥ 4 overlapping tokens with the event.\n",
    "    * Otherwise `0`.\n",
    "    * If no event defined: `NaN`.\n",
    "\n",
    "* **Recall score**\n",
    "\n",
    "  * `recall_score` = sum of all `T*_Accuracy` and `T*_Detail` columns (T1–T46), treating `NaN` as 0.\n",
    "  * Higher scores = more events recalled and/or recalled with detail.\n",
    "\n",
    "* **Confidence score**\n",
    "\n",
    "  * For each response, collect Jaccard similarities for all events where `Accuracy == 1`.\n",
    "  * If no events matched → `confidence_score = 40` (low-moderate).\n",
    "  * Otherwise:\n",
    "\n",
    "    * Compute mean similarity across matched events.\n",
    "    * Map mean similarity to a range roughly from 60 to 100, then clamp to [30, 100].\n",
    "  * Interpreted as a **subjective confidence** (0–100) in how well the automatic method captured the event structure of the recall.\n",
    "\n",
    "* **Outputs**\n",
    "\n",
    "  * Original columns preserved.\n",
    "  * Added 92 event-level columns: `T1_Accuracy`–`T46_Accuracy`, `T1_Detail`–`T46_Detail`.\n",
    "  * Added `recall_score` and `confidence_score`.\n",
    "  * Exported as:\n",
    "\n",
    "    * `coded_responses_full.csv` (all rows).\n",
    "    * `coded_responses_preview.csv` (first 20 rows).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54dbd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full coded dataset to: C:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\recall_openended\\sample\\coded_responses_full_method1.csv\n",
      "Saved preview (first 20 rows) to: C:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\recall_openended\\sample\\coded_responses_preview.csv\n",
      "\n",
      "Head of coded data:\n",
      "   respondent group questionnaire question_code question   form    title  \\\n",
      "0           2     A          Post           Q18   Recall  Short  Mad Max   \n",
      "1           4     A          Post           Q18   Recall  Short  Mad Max   \n",
      "2           5     F          Post           Q18   Recall  Short  Mad Max   \n",
      "3           3     F          Post           Q18   Recall  Short  Mad Max   \n",
      "4           6     A          Post           Q18   Recall  Short  Mad Max   \n",
      "\n",
      "                                            response  T1_Accuracy  \\\n",
      "0  This is a high speed chase scene from the movi...            1   \n",
      "1  In this post apocalyptical wasteland fuel and ...            1   \n",
      "2  I remember the woman Furiosa speeding anddrivi...            0   \n",
      "3  It looked like theres some sort of chase going...            0   \n",
      "4  A couple guys in a car put on the creepy nun m...            0   \n",
      "\n",
      "   T2_Accuracy  ...  T39_Detail  T40_Detail  T41_Detail  T42_Detail  \\\n",
      "0            1  ...         NaN         NaN         NaN         NaN   \n",
      "1            1  ...         NaN         NaN         NaN         NaN   \n",
      "2            1  ...         NaN         NaN         NaN         NaN   \n",
      "3            0  ...         NaN         NaN         NaN         NaN   \n",
      "4            0  ...         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   T43_Detail  T44_Detail  T45_Detail  T46_Detail  recall_score  \\\n",
      "0         NaN         NaN         NaN         NaN            10   \n",
      "1         NaN         NaN         NaN         NaN             8   \n",
      "2         NaN         NaN         NaN         NaN             4   \n",
      "3         NaN         NaN         NaN         NaN             2   \n",
      "4         NaN         NaN         NaN         NaN             0   \n",
      "\n",
      "   confidence_score  \n",
      "0                81  \n",
      "1                80  \n",
      "2                73  \n",
      "3                83  \n",
      "4                40  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "\n",
      "Recall score summary by question_code:\n",
      "               count      mean       std  min  25%  50%  75%   max\n",
      "question_code                                                     \n",
      "Q18              6.0  6.666667  5.887841  0.0  2.5  6.0  9.5  16.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "\n",
    "# --------------------------------------------------\n",
    "# CONFIG: file paths (edit if needed)\n",
    "# --------------------------------------------------\n",
    "RAW_CSV = \"sample_open_ended_madmax_long.csv\"\n",
    "MODEL_MD = \"sample_model_answers_events.md\"\n",
    "FULL_OUT = \"coded_responses_full_method1.csv\"\n",
    "PREVIEW_OUT = \"coded_responses_preview.csv\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helpers\n",
    "# --------------------------------------------------\n",
    "def resolve_file(filename: str) -> Path:\n",
    "    \"\"\"Return the first matching path for filename across common notebook roots.\"\"\"\n",
    "    search_dirs = []\n",
    "    cwd = Path.cwd()\n",
    "    search_dirs.append(cwd)\n",
    "    sample_dir = cwd / \"recall_openended\" / \"sample\"\n",
    "    if sample_dir != cwd:\n",
    "        search_dirs.append(sample_dir)\n",
    "    if \"__file__\" in globals():\n",
    "        try:\n",
    "            search_dirs.append(Path(__file__).resolve().parent)\n",
    "        except Exception:\n",
    "            pass\n",
    "    resolved_roots = []\n",
    "    for directory in search_dirs:\n",
    "        if directory is None:\n",
    "            continue\n",
    "        try:\n",
    "            resolved_dir = directory.resolve()\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        if resolved_dir in resolved_roots:\n",
    "            continue\n",
    "        resolved_roots.append(resolved_dir)\n",
    "        candidate = resolved_dir / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate {filename!r}. Checked: \" + \", \".join(str(p) for p in resolved_roots)\n",
    "    )\n",
    "\n",
    "def read_csv_safely(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Try a couple of common encodings so Windows-generated CSVs load cleanly.\"\"\"\n",
    "    for encoding in (\"utf-8\", \"utf-8-sig\", \"cp1252\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=encoding)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise\n",
    "\n",
    "def read_text_safely(path: Path) -> str:\n",
    "    \"\"\"Read text using the same fallback encodings as the CSV helper.\"\"\"\n",
    "    for encoding in (\"utf-8\", \"utf-8-sig\", \"cp1252\"):\n",
    "        try:\n",
    "            return path.read_text(encoding=encoding)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------\n",
    "raw_path = resolve_file(RAW_CSV)\n",
    "md_path = resolve_file(MODEL_MD)\n",
    "output_dir = raw_path.parent\n",
    "\n",
    "df = read_csv_safely(raw_path)\n",
    "md_text = read_text_safely(md_path)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parse model answers into event lists\n",
    "# (matches the logic used in the ChatGPT run)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def extract_numbered(section: str):\n",
    "    \"\"\"Extract lines starting with '1.', '2.', ... as event strings.\"\"\"\n",
    "    events = []\n",
    "    for m in re.finditer(r\"\\n\\s*\\d+\\.\\s*(.+)\", section):\n",
    "        events.append(m.group(1).strip())\n",
    "    return events\n",
    "\n",
    "text = md_text\n",
    "\n",
    "# Long form section: between \"## Mad Max - Long Form\" and\n",
    "# \"Mad Max: Fury Road – Short Form Scene Events\" (note the en dash)\n",
    "start_long = text.find(\"## Mad Max - Long Form\")\n",
    "start_short_label = text.find(\"Mad Max: Fury Road – Short Form Scene Events\")\n",
    "\n",
    "if start_long == -1:\n",
    "    raise ValueError(\"Could not find '## Mad Max - Long Form' in model answers markdown.\")\n",
    "\n",
    "if start_short_label == -1:\n",
    "    raise ValueError(\"Could not find 'Mad Max: Fury Road – Short Form Scene Events' label in model answers markdown.\")\n",
    "\n",
    "long_section = text[start_long:start_short_label]\n",
    "short_section = text[start_short_label:]\n",
    "\n",
    "events_long = extract_numbered(long_section)\n",
    "events_short = extract_numbered(short_section)\n",
    "\n",
    "# Sanity: pad to 46 max events\n",
    "MAX_EVENTS = 46\n",
    "if len(events_long) > MAX_EVENTS:\n",
    "    events_long = events_long[:MAX_EVENTS]\n",
    "if len(events_short) > MAX_EVENTS:\n",
    "    events_short = events_short[:MAX_EVENTS]\n",
    "\n",
    "# For indexing convenience later, pad with None up to MAX_EVENTS\n",
    "events_long_padded = events_long + [None] * (MAX_EVENTS - len(events_long))\n",
    "events_short_padded = events_short + [None] * (MAX_EVENTS - len(events_short))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Text normalization and tokenization\n",
    "# --------------------------------------------------\n",
    "stopwords = {\n",
    "    \"the\",\"and\",\"a\",\"an\",\"to\",\"of\",\"in\",\"on\",\"at\",\"by\",\"for\",\"from\",\"with\",\"as\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"its\",\"this\",\"that\",\"these\",\n",
    "    \"those\",\"into\",\"their\",\"his\",\"her\",\"they\",\"them\",\"he\",\"she\",\"or\",\"but\",\"so\",\n",
    "    \"up\",\"out\",\"about\",\"over\",\"under\",\"through\",\"across\",\"off\",\"not\",\"no\",\"than\",\n",
    "    \"then\",\"there\",\"here\",\"when\",\"while\",\"after\",\"before\",\"also\",\"just\",\"very\",\n",
    "    \"own\",\"still\",\"now\",\"back\"\n",
    "}\n",
    "\n",
    "def normalize(text: str):\n",
    "    \"\"\"Lowercase, strip punctuation, remove stopwords, return token set.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    tokens = [t for t in text.split() if t and t not in stopwords]\n",
    "    return set(tokens)\n",
    "\n",
    "# Precompute event token sets\n",
    "event_tokens_long = [normalize(e) if e is not None else set() for e in events_long_padded]\n",
    "event_tokens_short = [normalize(e) if e is not None else set() for e in events_short_padded]\n",
    "\n",
    "# Keywords for \"detail\"\n",
    "detail_keywords = {\n",
    "    \"max\",\"rockatansky\",\"nux\",\"immortan\",\"joe\",\"furiosa\",\"war\",\"rig\",\"warboys\",\"war\",\"boys\",\n",
    "    \"citadel\",\"buzzards\",\"gas\",\"town\",\"bullet\",\"farm\",\"people\",\"eater\",\"rictus\",\"sandstorm\",\n",
    "    \"storm\",\"desert\",\"wasteland\",\"blood\",\"bag\",\"wives\",\"breeders\",\"convoy\",\"tanker\",\"truck\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Scoring function for a single row\n",
    "# --------------------------------------------------\n",
    "def score_response(row):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        acc_list: list of length MAX_EVENTS with 0/1/NaN\n",
    "        det_list: list of length MAX_EVENTS with 0/1/NaN\n",
    "        conf: int, 0–100\n",
    "    \"\"\"\n",
    "    text = row.get(\"response\", \"\")\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        text = \"\"\n",
    "    resp = str(text).strip()\n",
    "\n",
    "    # Blank / missing\n",
    "    if not resp:\n",
    "        return [0] * MAX_EVENTS, [0] * MAX_EVENTS, 40\n",
    "\n",
    "    resp_tokens = normalize(resp)\n",
    "    resp_lower = resp.lower()\n",
    "    form = str(row.get(\"form\", \"\")).strip().lower()\n",
    "    title = str(row.get(\"title\", \"\")).lower()\n",
    "\n",
    "    # Choose event list based on title + form\n",
    "    if \"mad max\" in title:\n",
    "        if form == \"short\":\n",
    "            ev_texts = events_short_padded\n",
    "            ev_tokens_list = event_tokens_short\n",
    "        else:\n",
    "            # default to Long if anything else\n",
    "            ev_texts = events_long_padded\n",
    "            ev_tokens_list = event_tokens_long\n",
    "    else:\n",
    "        # Unknown title: treat as no defined events\n",
    "        ev_texts = [None] * MAX_EVENTS\n",
    "        ev_tokens_list = [set()] * MAX_EVENTS\n",
    "\n",
    "    acc_list = []\n",
    "    det_list = []\n",
    "    sim_scores = []\n",
    "\n",
    "    for i in range(MAX_EVENTS):\n",
    "        ev_text = ev_texts[i]\n",
    "        ev_tokens = ev_tokens_list[i]\n",
    "\n",
    "        if ev_text is None or not ev_tokens:\n",
    "            acc_list.append(np.nan)\n",
    "            det_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        inter = resp_tokens.intersection(ev_tokens)\n",
    "        jacc = len(inter) / len(ev_tokens) if ev_tokens else 0.0\n",
    "\n",
    "        # Conservative accuracy rule:\n",
    "        # Accuracy = 1 if there's enough overlap (both absolute and relative)\n",
    "        if len(inter) >= 2 and jacc >= 0.12:\n",
    "            acc = 1\n",
    "            sim_scores.append(jacc)\n",
    "        else:\n",
    "            acc = 0\n",
    "\n",
    "        # Detail only if event is accurate\n",
    "        if acc == 1:\n",
    "            # Matching detail keywords OR substantial lexical overlap\n",
    "            has_detail_kw = any(k in resp_lower for k in detail_keywords)\n",
    "            if has_detail_kw or len(inter) >= 4:\n",
    "                det = 1\n",
    "            else:\n",
    "                det = 0\n",
    "        else:\n",
    "            det = 0\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        det_list.append(det)\n",
    "\n",
    "    # Confidence heuristic\n",
    "    if not sim_scores:\n",
    "        conf = 40\n",
    "    else:\n",
    "        mean_sim = float(np.mean(sim_scores))\n",
    "        # Start at 60 and increase up to +40 as similarity improves\n",
    "        conf = int(60 + min(40, 40 * (mean_sim / 0.4)))\n",
    "        # clip\n",
    "        conf = min(100, max(30, conf))\n",
    "\n",
    "    return acc_list, det_list, conf\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Apply scoring to all rows\n",
    "# --------------------------------------------------\n",
    "acc_cols = [f\"T{i}_Accuracy\" for i in range(1, MAX_EVENTS + 1)]\n",
    "det_cols = [f\"T{i}_Detail\" for i in range(1, MAX_EVENTS + 1)]\n",
    "\n",
    "acc_data = []\n",
    "det_data = []\n",
    "conf_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    acc_list, det_list, conf = score_response(row)\n",
    "    acc_data.append(acc_list)\n",
    "    det_data.append(det_list)\n",
    "    conf_data.append(conf)\n",
    "\n",
    "acc_df = pd.DataFrame(acc_data, columns=acc_cols)\n",
    "det_df = pd.DataFrame(det_data, columns=det_cols)\n",
    "\n",
    "coded_df = pd.concat([df.reset_index(drop=True), acc_df, det_df], axis=1)\n",
    "\n",
    "# recall_score: sum of all Accuracy + Detail, treating NaN as 0\n",
    "event_cols = acc_cols + det_cols\n",
    "coded_df[\"recall_score\"] = coded_df[event_cols].fillna(0).sum(axis=1).astype(int)\n",
    "coded_df[\"confidence_score\"] = conf_data\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Save outputs\n",
    "# --------------------------------------------------\n",
    "full_out_path = output_dir / FULL_OUT\n",
    "preview_out_path = output_dir / PREVIEW_OUT\n",
    "coded_df.to_csv(full_out_path, index=False)\n",
    "coded_df.head(20).to_csv(preview_out_path, index=False)\n",
    "\n",
    "print(f\"Saved full coded dataset to: {full_out_path}\")\n",
    "print(f\"Saved preview (first 20 rows) to: {preview_out_path}\")\n",
    "print(\"\\nHead of coded data:\")\n",
    "print(coded_df.head())\n",
    "print(\"\\nRecall score summary by question_code:\")\n",
    "print(coded_df.groupby(\"question_code\")[\"recall_score\"].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
