{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9edfa58",
   "metadata": {},
   "source": [
    "# LLM Recall Scoring – Sample Sandbox\n",
    "This notebook scaffolds Method 2 for recall scoring using the sampled assets and the guidance defined in `INSTRUCTIONS_RECALL.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bdb5b",
   "metadata": {},
   "source": [
    "## 1. Load Project Dependencies and Environment Configuration\n",
    "We initialise key libraries, point the working directory at the sample assets, and capture environment variables required for calling the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b29da073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\n",
      "Sample asset directory: c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\recall_openended\\sample\n",
      "OpenAI client initialised: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:  # Notebook can still run data prep without OpenAI SDK\n",
    "    OpenAI = None\n",
    "\n",
    "# Locate the project root regardless of the notebook launch directory.\n",
    "PROJECT_ROOT: Optional[Path] = None\n",
    "SAMPLE_DIR: Optional[Path] = None\n",
    "for candidate in [Path.cwd(), *Path.cwd().parents]:\n",
    "    potential = candidate / \"recall_openended\" / \"sample\"\n",
    "    if potential.exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        SAMPLE_DIR = potential\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None or SAMPLE_DIR is None:\n",
    "    raise FileNotFoundError(\"Could not locate 'recall_openended/sample' relative to the current working directory.\")\n",
    "\n",
    "\n",
    "def load_env_file(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        return\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\"').strip(\"'\")\n",
    "        if value:\n",
    "            os.environ[key] = value\n",
    "\n",
    "\n",
    "load_env_file(PROJECT_ROOT / \".env\")\n",
    "load_env_file(SAMPLE_DIR / \".env\")\n",
    "\n",
    "INSTRUCTIONS_PATH = SAMPLE_DIR / \"INSTRUCTIONS_RECALL.md\"\n",
    "MODEL_EVENTS_PATH = SAMPLE_DIR / \"sample_model_answers_events.md\"\n",
    "OPEN_ENDED_PATH = SAMPLE_DIR / \"sample_open_ended_madmax_long.csv\"\n",
    "OUTPUT_SAMPLE_PATH = SAMPLE_DIR / \"coded_responses_full_method2.csv\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) if (OpenAI and OPENAI_API_KEY) else None\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Sample asset directory: {SAMPLE_DIR}\")\n",
    "print(f\"OpenAI client initialised: {client is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1f1ff",
   "metadata": {},
   "source": [
    "## 2. Parse INSTRUCTIONS_RECALL.md for Experiment Metadata\n",
    "We load the context document to surface the scoring rubric, input expectations, and output schema within the notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c152ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded instructions summary with 348 lines\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Project Goal</th>\n",
       "      <td>We have a CSV of open-ended recall responses to video content.  \\nEach row = 1 participant’s response to a scene they watched in **long** or **short** forma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Input Data</th>\n",
       "      <td>### 2.1 Main CSV\\n\\nFile: `sample_open_ended_madmax_long.csv`\\n\\nColumns (at minimum):\\n\\n- `id` – unique integer per row (0, 1, 2, …).  \\n  - If not presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Scoring Targets (What the LLM Should Output)</th>\n",
       "      <td>For each row in the CSV, the LLM must output:\\n\\n- `recall_score` – **integer from 0 to 100**\\n  - 0 = no correct recall or completely off-topic.\\n  - 100 =...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          detail\n",
       "1. Project Goal                                  We have a CSV of open-ended recall responses to video content.  \\nEach row = 1 participant’s response to a scene they watched in **long** or **short** forma...\n",
       "2. Input Data                                    ### 2.1 Main CSV\\n\\nFile: `sample_open_ended_madmax_long.csv`\\n\\nColumns (at minimum):\\n\\n- `id` – unique integer per row (0, 1, 2, …).  \\n  - If not presen...\n",
       "3. Scoring Targets (What the LLM Should Output)  For each row in the CSV, the LLM must output:\\n\\n- `recall_score` – **integer from 0 to 100**\\n  - 0 = no correct recall or completely off-topic.\\n  - 100 =..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_instruction_sections(path: Path, headers: List[str]) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Return the full markdown text and selected sections keyed by header.\"\"\"\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    sections: Dict[str, str] = {}\n",
    "    for header in headers:\n",
    "        pattern = re.compile(rf\"^## {re.escape(header)}\\n(.*?)(?=^## |\\Z)\", re.MULTILINE | re.DOTALL)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            sections[header] = match.group(1).strip()\n",
    "    return text, sections\n",
    "\n",
    "sections_to_pull = [\n",
    "    \"1. Project Goal\",\n",
    "    \"2. Input Data\",\n",
    "    \"3. Scoring Targets (What the LLM Should Output)\",\n",
    "    \"4. Prompt Construction (Per Row / Batch)\",\n",
    "    \"5. Pipeline Requirements\",\n",
    "    \"6. OpenAI API Integration Notes\",\n",
    "    \"8. Quality Checks\"\n",
    "]\n",
    "\n",
    "instructions_text, instruction_sections = load_instruction_sections(INSTRUCTIONS_PATH, sections_to_pull)\n",
    "print(f\"Loaded instructions summary with {len(instructions_text.splitlines())} lines\")\n",
    "pd.Series(instruction_sections).to_frame(\"detail\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111da09b",
   "metadata": {},
   "source": [
    "## 3. Inspect Sample Data Assets\n",
    "We load the sampled open-ended responses and the model answer reference to confirm schemas, cardinalities, and basic data hygiene before running any scoring passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "922a2591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-ended sample shape: (6, 9)\n",
      "Unique titles: 1 | Unique forms: ['Short']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question</th>\n",
       "      <th>form</th>\n",
       "      <th>title</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>This is a high speed chase scene from the movie mad max furry road. The actor jumps from his vehicle with his spears and kills the actor driving the car. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>In this post apocalyptical wasteland fuel and machines are instruments of war between waring factions. The buzzards control their area and lay traps for wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>I remember the woman Furiosa speeding anddriving and everyone else fighting off everyone who was chasing them, including the guy in the passenger seat.They ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>It looked like theres some sort of chase going on between two factions and there was a lady driving one of the cars who I guess was in charge of the one fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>A couple guys in a car put on the creepy nun masks and then rob a money truck. Theres a shootout and a chase with the cops. During the chase, an accomplice ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  respondent group questionnaire question_code question   form    title  \\\n",
       "0   0           2     A          Post           Q18   Recall  Short  Mad Max   \n",
       "1   1           4     A          Post           Q18   Recall  Short  Mad Max   \n",
       "2   2           5     F          Post           Q18   Recall  Short  Mad Max   \n",
       "3   3           3     F          Post           Q18   Recall  Short  Mad Max   \n",
       "4   4           6     A          Post           Q18   Recall  Short  Mad Max   \n",
       "\n",
       "                                                                                                                                                          response  \n",
       "0  This is a high speed chase scene from the movie mad max furry road. The actor jumps from his vehicle with his spears and kills the actor driving the car. Th...  \n",
       "1  In this post apocalyptical wasteland fuel and machines are instruments of war between waring factions. The buzzards control their area and lay traps for wou...  \n",
       "2  I remember the woman Furiosa speeding anddriving and everyone else fighting off everyone who was chasing them, including the guy in the passenger seat.They ...  \n",
       "3  It looked like theres some sort of chase going on between two factions and there was a lady driving one of the cars who I guess was in charge of the one fac...  \n",
       "4  A couple guys in a car put on the creepy nun masks and then rob a money truck. Theres a shootout and a chase with the cops. During the chase, an accomplice ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "try:\n",
    "    open_ended_df = pd.read_csv(OPEN_ENDED_PATH)\n",
    "except UnicodeDecodeError:\n",
    "    with OPEN_ENDED_PATH.open(\"r\", encoding=\"cp1252\", errors=\"ignore\") as fh:\n",
    "        buffer = StringIO(fh.read())\n",
    "    open_ended_df = pd.read_csv(buffer)\n",
    "\n",
    "if \"id\" not in open_ended_df.columns:\n",
    "    open_ended_df.insert(0, \"id\", range(len(open_ended_df)))\n",
    "\n",
    "model_events_text = MODEL_EVENTS_PATH.read_text(encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Open-ended sample shape: {open_ended_df.shape}\")\n",
    "print(f\"Unique titles: {open_ended_df['title'].nunique()} | Unique forms: {open_ended_df['form'].unique().tolist()}\")\n",
    "open_ended_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7220ce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with blank responses: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th>form</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mad Max</th>\n",
       "      <th>Short</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "title   form        \n",
       "Mad Max Short      6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_response_rows = open_ended_df[open_ended_df[\"response\"].isna() | (open_ended_df[\"response\"].str.strip() == \"\")]\n",
    "print(f\"Rows with blank responses: {len(missing_response_rows)}\")\n",
    "open_ended_df.groupby([\"title\", \"form\"]).size().to_frame(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e58a283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '## Mad Max - Long Form', '', '1. In a post-apocalyptic desert wasteland, petrol (gasoline) and water are extremely scarce and precious resources.', '', '2. Max Rockatansky, a lone drifter, wanders this wasteland as a hardened survivor.', '', '3. Max is suddenly ambushed and captured by the War Boys, a group of feral, fanatical soldiers devoted to the warlord Immortan Joe.', '', '4. The War Boys drag Max to Immortan Joe’s desert stronghold, a fortress known as the Citadel.', '', '5. At the Citadel, Max is identified as a universal blood donor and is imprisoned.', '', '6. Max is strapped into medical apparatus and used as a living “blood bag,” providing continuous blood transfusions to a sickly War Boy named Nux.', '', '7. Max makes an early, frantic attempt to escape from the Citadel by fleeing through its twisting underground tunnels and caves.', '', '8. The War Boys quickly recapture Max and chain him up again, reinforcing his status as Nux’s captive blood donor.', '']\n"
     ]
    }
   ],
   "source": [
    "print(model_events_text.splitlines()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7936c8",
   "metadata": {},
   "source": [
    "## 4. Define LLM Scoring Utilities\n",
    "Helper utilities prepare model events, craft prompts, and wrap LLM calls with retry logic required for the scoring workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e51097df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded events for 2 title/format combinations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mad max', 'long'), ('mad max', 'short')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalise_title(title: str) -> str:\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", title.strip()).lower()\n",
    "    return cleaned.replace(\":\", \"\")\n",
    "\n",
    "\n",
    "def normalise_form(form: str) -> str:\n",
    "    form_norm = form.strip().lower().replace(\"-\", \" \")\n",
    "    form_norm = form_norm.replace(\" form\", \"\").strip()\n",
    "    alias_map = {\n",
    "        \"lf\": \"long\",\n",
    "        \"longform\": \"long\",\n",
    "        \"shortform\": \"short\",\n",
    "    }\n",
    "    return alias_map.get(form_norm, form_norm)\n",
    "\n",
    "\n",
    "def parse_model_events(markdown_text: str) -> Dict[Tuple[str, str], List[str]]:\n",
    "    sections = {}\n",
    "    pattern = re.compile(r\"^##\\s*(.+?)\\s*-\\s*(.+?)\\s*$\", re.MULTILINE)\n",
    "    matches = list(pattern.finditer(markdown_text))\n",
    "    for idx, match in enumerate(matches):\n",
    "        title_raw, form_raw = match.group(1), match.group(2)\n",
    "        start = match.end()\n",
    "        end = matches[idx + 1].start() if (idx + 1) < len(matches) else len(markdown_text)\n",
    "        section_text = markdown_text[start:end]\n",
    "        events = [evt.strip() for evt in re.findall(r\"^\\s*\\d+\\.\\s+(.*)$\", section_text, re.MULTILINE) if evt.strip()]\n",
    "        sections[(normalise_title(title_raw), normalise_form(form_raw))] = events\n",
    "    return sections\n",
    "\n",
    "model_events_lookup = parse_model_events(model_events_text)\n",
    "print(f\"Loaded events for {len(model_events_lookup)} title/format combinations\")\n",
    "list(model_events_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce585771",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    You are an expert qualitative coder in applied neuroscience. Evaluate each participant recall response.\n",
    "    For every row you receive:\n",
    "    - Compare the participant response against the chronological MODEL EVENTS list.\n",
    "    - Judge which events are recalled and in how much detail (names, locations, actions, chronology).\n",
    "    - Produce:\n",
    "      - \"recall_score\": integer 0-100 (0 = off-topic or blank, 100 = richly detailed and highly accurate).\n",
    "      - \"confidence_score\": integer 0-100 where higher means the mapping from response to events is clear.\n",
    "      - \"rationale\": 1-3 sentences summarising the recall quality.\n",
    "    - Be conservative when uncertain. If the response is blank or clearly states the participant does not remember, set recall_score to 0 and confidence_score to 95.\n",
    "    - Never invent events beyond the provided MODEL EVENTS.\n",
    "    - Answer in strict JSON only.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "\n",
    "def build_prompt_block(row: pd.Series, events: List[str]) -> str:\n",
    "    events_text = \"\\n\".join(f\"{idx + 1}. {event}\" for idx, event in enumerate(events)) if events else \"(No model events found.)\"\n",
    "    return textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        Title: {row['title']}\n",
    "        Format: {row['form']}\n",
    "        Question code: {row['question_code']}\n",
    "        Row ID: {row['id']}\n",
    "\n",
    "        MODEL EVENTS (chronological):\n",
    "        {events_text}\n",
    "\n",
    "        PARTICIPANT RESPONSE:\n",
    "        {row['response']}\n",
    "\n",
    "        Please evaluate this response and return a JSON object with keys id, recall_score, confidence_score, rationale.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def build_batch_prompt(batch_rows: pd.DataFrame, events_lookup: Dict[Tuple[str, str], List[str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    blocks = []\n",
    "    missing_keys: List[Tuple[str, str]] = []\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        key = (normalise_title(row[\"title\"]), normalise_form(row[\"form\"]))\n",
    "        events = events_lookup.get(key, [])\n",
    "        if not events:\n",
    "            missing_keys.append(key)\n",
    "        blocks.append(build_prompt_block(row, events))\n",
    "    return \"\\n\\n\".join(blocks), missing_keys\n",
    "\n",
    "\n",
    "def call_llm_batch(prompt: str, client_obj=client, model: str = \"gpt-4.1\", max_retries: int = 3, sleep_seconds: float = 2.0) -> str:\n",
    "    \"\"\"Call the OpenAI Responses API with basic retry support.\"\"\"\n",
    "    if client_obj is None:\n",
    "        raise RuntimeError(\"OpenAI client is not initialised. Set OPENAI_API_KEY before calling the model.\")\n",
    "\n",
    "    last_error: Optional[Exception] = None\n",
    "    payload = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": prompt}]}\n",
    "    ]\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = client_obj.responses.create(\n",
    "                model=model,\n",
    "                input=payload,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            return response.output_text\n",
    "        except Exception as exc:  # noqa: BLE001 broad to keep notebook lightweight\n",
    "            last_error = exc\n",
    "            wait_for = sleep_seconds * (2 ** (attempt - 1))\n",
    "            print(f\"Attempt {attempt} failed: {exc}. Retrying in {wait_for:.1f}s\")\n",
    "            import time\n",
    "            time.sleep(wait_for)\n",
    "    raise RuntimeError(\"Failed to retrieve LLM response\") from last_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "786540af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_json(raw_output: str) -> List[Dict[str, object]]:\n",
    "    raw_output = raw_output.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "    except json.JSONDecodeError as exc:\n",
    "        raise ValueError(f\"Model returned non-JSON payload: {raw_output[:200]}\") from exc\n",
    "    if isinstance(parsed, dict):\n",
    "        parsed = [parsed]\n",
    "    if not isinstance(parsed, list):\n",
    "        raise ValueError(\"Expected list of JSON objects from model output\")\n",
    "    cleaned = []\n",
    "    for entry in parsed:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        required = {\"id\", \"recall_score\", \"confidence_score\", \"rationale\"}\n",
    "        if not required.issubset(entry):\n",
    "            continue\n",
    "        cleaned.append({key: entry[key] for key in required})\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def enrich_dataframe_with_scores(df: pd.DataFrame, scored_rows: List[Dict[str, object]]) -> pd.DataFrame:\n",
    "    scored_df = pd.DataFrame(scored_rows).set_index(\"id\")\n",
    "    merged = df.set_index(\"id\").join(scored_df, how=\"left\")\n",
    "    return merged.reset_index()\n",
    "\n",
    "\n",
    "def heuristic_stub_scores(batch_rows: pd.DataFrame, events_lookup: Dict[Tuple[str, str], List[str]]) -> List[Dict[str, object]]:\n",
    "    \"\"\"Fallback deterministic scorer when no API key is available.\"\"\"\n",
    "    results: List[Dict[str, object]] = []\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        key = (normalise_title(row[\"title\"]), normalise_form(row[\"form\"]))\n",
    "        events = events_lookup.get(key, [])\n",
    "        response = row[\"response\"] or \"\"\n",
    "        token_count = len(response.split())\n",
    "        coverage_ratio = min(len(events), max(response.lower().count(\" \"), 1)) / (len(events) or 1)\n",
    "        recall_score = int(min(100, coverage_ratio * 40 + min(token_count, 120) * 0.3))\n",
    "        confidence_score = int(max(30, min(95, 60 + coverage_ratio * 30)))\n",
    "        rationale = \"Stub score generated locally; replace with real LLM call.\"\n",
    "        results.append(\n",
    "            {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"recall_score\": recall_score,\n",
    "                \"confidence_score\": confidence_score,\n",
    "                \"rationale\": rationale,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe4cd9",
   "metadata": {},
   "source": [
    "## 5. Execute Full Scoring Workflow\n",
    "We batch the entire dataset, call the LLM for each chunk, merge the resulting scores, and save the final coded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "694e367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total responses to score: 6\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt-4.1\"\n",
    "BATCH_SIZE = 5  # adjust based on token budget and latency requirements\n",
    "\n",
    "print(f\"Total responses to score: {len(open_ended_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45af2250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question</th>\n",
       "      <th>form</th>\n",
       "      <th>title</th>\n",
       "      <th>response</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>This is a high speed chase scene from the movie mad max furry road. The actor jumps from his vehicle with his spears and kills the actor driving the car. Th...</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "      <td>The participant recalls the high-speed chase, Furiosa and her crew defending the rig, and a character jumping from a vehicle with spears, which loosely maps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>In this post apocalyptical wasteland fuel and machines are instruments of war between waring factions. The buzzards control their area and lay traps for wou...</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>The response captures the general setting (post-apocalyptic, warring factions, Buzzards laying traps) and mentions a fight involving spikes and bombs, as we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>I remember the woman Furiosa speeding anddriving and everyone else fighting off everyone who was chasing them, including the guy in the passenger seat.They ...</td>\n",
       "      <td>40</td>\n",
       "      <td>70</td>\n",
       "      <td>The participant recalls Furiosa driving, a high-speed chase, fighting, and general chaos, which aligns with the overall scene. However, details are vague, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>It looked like theres some sort of chase going on between two factions and there was a lady driving one of the cars who I guess was in charge of the one fac...</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>This response accurately recalls several key events: the chase between factions, Furiosa as the leader, a man tied to a car, vehicles being blown up, and a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q18</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>A couple guys in a car put on the creepy nun masks and then rob a money truck. Theres a shootout and a chase with the cops. During the chase, an accomplice ...</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>The response is entirely off-topic, describing a bank robbery and police chase unrelated to the provided model events. No relevant details are recalled.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  respondent group questionnaire question_code question   form    title  \\\n",
       "0   0           2     A          Post           Q18   Recall  Short  Mad Max   \n",
       "1   1           4     A          Post           Q18   Recall  Short  Mad Max   \n",
       "2   2           5     F          Post           Q18   Recall  Short  Mad Max   \n",
       "3   3           3     F          Post           Q18   Recall  Short  Mad Max   \n",
       "4   4           6     A          Post           Q18   Recall  Short  Mad Max   \n",
       "\n",
       "                                                                                                                                                          response  \\\n",
       "0  This is a high speed chase scene from the movie mad max furry road. The actor jumps from his vehicle with his spears and kills the actor driving the car. Th...   \n",
       "1  In this post apocalyptical wasteland fuel and machines are instruments of war between waring factions. The buzzards control their area and lay traps for wou...   \n",
       "2  I remember the woman Furiosa speeding anddriving and everyone else fighting off everyone who was chasing them, including the guy in the passenger seat.They ...   \n",
       "3  It looked like theres some sort of chase going on between two factions and there was a lady driving one of the cars who I guess was in charge of the one fac...   \n",
       "4  A couple guys in a car put on the creepy nun masks and then rob a money truck. Theres a shootout and a chase with the cops. During the chase, an accomplice ...   \n",
       "\n",
       "   recall_score  confidence_score  \\\n",
       "0            55                80   \n",
       "1            45                75   \n",
       "2            40                70   \n",
       "3            80                90   \n",
       "4             0                95   \n",
       "\n",
       "                                                                                                                                                         rationale  \n",
       "0  The participant recalls the high-speed chase, Furiosa and her crew defending the rig, and a character jumping from a vehicle with spears, which loosely maps...  \n",
       "1  The response captures the general setting (post-apocalyptic, warring factions, Buzzards laying traps) and mentions a fight involving spikes and bombs, as we...  \n",
       "2  The participant recalls Furiosa driving, a high-speed chase, fighting, and general chaos, which aligns with the overall scene. However, details are vague, a...  \n",
       "3  This response accurately recalls several key events: the chase between factions, Furiosa as the leader, a man tied to a car, vehicles being blown up, and a ...  \n",
       "4         The response is entirely off-topic, describing a bank robbery and police chase unrelated to the provided model events. No relevant details are recalled.  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results: List[Dict[str, object]] = []\n",
    "missing_keys_overall: set[Tuple[str, str]] = set()\n",
    "\n",
    "if open_ended_df.empty:\n",
    "    raise ValueError(\"No responses found in the input dataset.\")\n",
    "\n",
    "for start in range(0, len(open_ended_df), BATCH_SIZE):\n",
    "    batch_df = open_ended_df.iloc[start : start + BATCH_SIZE]\n",
    "    prompt_text, missing_keys = build_batch_prompt(batch_df, model_events_lookup)\n",
    "    if missing_keys:\n",
    "        missing_keys_overall.update(missing_keys)\n",
    "\n",
    "    if client is not None:\n",
    "        raw_response = call_llm_batch(prompt_text, client_obj=client, model=MODEL_NAME)\n",
    "        batch_results = parse_llm_json(raw_response)\n",
    "    else:\n",
    "        print(\"OPENAI_API_KEY not detected; using heuristic stub for demonstration.\")\n",
    "        batch_results = heuristic_stub_scores(batch_df, model_events_lookup)\n",
    "\n",
    "    for entry in batch_results:\n",
    "        entry[\"id\"] = int(entry[\"id\"])\n",
    "        entry[\"recall_score\"] = int(entry[\"recall_score\"])\n",
    "        entry[\"confidence_score\"] = int(entry[\"confidence_score\"])\n",
    "\n",
    "    all_results.extend(batch_results)\n",
    "\n",
    "if missing_keys_overall:\n",
    "    print(f\"Warning: missing model events for {sorted(missing_keys_overall)}\")\n",
    "\n",
    "scored_full_df = enrich_dataframe_with_scores(open_ended_df, all_results)\n",
    "scored_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a411d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows scored: 6 | Rows missing scores: 0\n",
      "Saved scored dataset to coded_responses_full_method2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall_score</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.333333</td>\n",
       "      <td>85.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34.592870</td>\n",
       "      <td>11.83216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.250000</td>\n",
       "      <td>76.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>85.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.750000</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       recall_score  confidence_score\n",
       "count      6.000000           6.00000\n",
       "mean      53.333333          85.00000\n",
       "std       34.592870          11.83216\n",
       "min        0.000000          70.00000\n",
       "25%       41.250000          76.25000\n",
       "50%       50.000000          85.00000\n",
       "75%       73.750000          93.75000\n",
       "max      100.000000         100.00000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_columns = [\"recall_score\", \"confidence_score\"]\n",
    "valid_scores = scored_full_df.dropna(subset=score_columns)\n",
    "missing_count = len(scored_full_df) - len(valid_scores)\n",
    "\n",
    "scored_full_df.to_csv(OUTPUT_SAMPLE_PATH, index=False)\n",
    "print(f\"Rows scored: {len(valid_scores)} | Rows missing scores: {missing_count}\")\n",
    "print(f\"Saved scored dataset to {OUTPUT_SAMPLE_PATH.name}\")\n",
    "valid_scores[score_columns].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
