{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a9680a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f48a0",
   "metadata": {},
   "source": [
    "## Function to read iMotions sensor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _extract_imotions_metadata(path, metadata=None):\n",
    "    \"\"\"Read leading metadata lines from an iMotions CSV without loading the data.\"\"\"\n",
    "    metadata = metadata or []\n",
    "    requested = set(metadata) if metadata else None\n",
    "    meta_lines = []\n",
    "    header_rows = 0\n",
    "    with open(path, \"r\", encoding=\"latin1\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            first_cell = line.split(\",\", 1)[0]\n",
    "            if \"#\" in first_cell:\n",
    "                meta_lines.append(line)\n",
    "                header_rows += 1\n",
    "            else:\n",
    "                break\n",
    "    meta_dict = {}\n",
    "    for raw_line in meta_lines:\n",
    "        segments = raw_line.strip().split(\"#\", 1)\n",
    "        if len(segments) < 2:\n",
    "            continue\n",
    "        cleaned = segments[1]\n",
    "        parts = cleaned.split(\",\")\n",
    "        if len(parts) > 1:\n",
    "            key = parts[0].strip()\n",
    "            value = \",\".join(parts[1:]).strip()\n",
    "            if requested is None or key in requested:\n",
    "                meta_dict[key] = value\n",
    "    return meta_dict, header_rows\n",
    "\n",
    "def read_imotions_metadata(path, metadata=None):\n",
    "    \"\"\"Return only the requested metadata from an iMotions CSV.\"\"\"\n",
    "    meta_dict, _ = _extract_imotions_metadata(path, metadata)\n",
    "    return meta_dict\n",
    "\n",
    "def read_imotions(path, metadata=None):\n",
    "    \"\"\"\n",
    "    Reads an iMotions CSV file while extracting optional metadata fields.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the iMotions CSV file.\n",
    "        metadata (list[str], optional): List of metadata keys to extract.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The data as a DataFrame.\n",
    "        meta_dict (dict): Dictionary containing requested metadata fields.\n",
    "    \"\"\"\n",
    "    meta_dict, header_rows = _extract_imotions_metadata(path, metadata)\n",
    "    df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
    "    return df, meta_dict\n",
    "\n",
    "def get_files(folder, tags=['',]):\n",
    "    return [f for f in os.listdir(folder) if not f.startswith('.') and all(x in f for x in tags)] \n",
    "\n",
    "\n",
    "def get_biometric_data(in_folder, results_folder):\n",
    "\n",
    "    ######## Define ########\n",
    "    # Define paths\n",
    "    out_path = f\"{results_folder}/\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    respondents = [1,2,3] #define list of respondent ids\n",
    "\n",
    "    # Define signal columns\n",
    "    cols_afdex = [\n",
    "                \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "                \"Surprise\", \"Engagement\", \"Valence\", \"Sentimentality\",\n",
    "                \"Confusion\", \"Neutral\"\n",
    "        ]\n",
    "    cols_eeg = ['High Engagement',\n",
    "        'Low Engagement',\n",
    "        'Distraction',\n",
    "        'Drowsy',\n",
    "        'Workload Average',\n",
    "        'Frontal Asymmetry Alpha',\n",
    "        ]\n",
    "\n",
    "\n",
    "    #Define window lengths in seconds\n",
    "    window_lengths = [3,]\n",
    "\n",
    "    ######## Read Inputs #######\n",
    "    #Get input files\n",
    "    sensor_files = get_files(f'{in_folder}/Sensors/',tags=['.csv',])\n",
    "\n",
    "    ### Begin ###\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    for respondent in respondents:\n",
    "        error = {'respondent':respondent, 'FAC':None, 'EEG':None, 'GSR':None, 'Blinks':None, 'ET':None}\n",
    "        interaction = {'respondent':respondent}\n",
    "        try:\n",
    "            file = [f for f in sensor_files if respondent in f][0] #may need adjustment\n",
    "            df_sens_resp,_ = read_imotions(f'{in_folder}/Sensors/{file}')\n",
    "\n",
    "            # Get sensor data per stimulus\n",
    "            for task in df_sens_resp['SourceStimuliName'].unique():\n",
    "                df_sens_task = df_sens_resp.loc[(df_sens_resp['SourceStimuliName']==task)]\n",
    "                window = task\n",
    "\n",
    "                # Get facial coding data\n",
    "                for a in cols_afdex:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_FAC_{a}_mean']=df_sens_task[a].dropna().mean()\n",
    "                        auc_data = df_sens_task[['Timestamp',a]].dropna()\n",
    "                        interaction[f'sens_{window}_FAC_{a}_AUC']=np.trapz(auc_data[a],x=auc_data['Timestamp'])/1000\n",
    "                        interaction[f'sens_{window}_FAC_{a}_Binary']=df_sens_task[a].dropna().max()>= 50\n",
    "                    except:\n",
    "                        error['FAC']='Missing'\n",
    "\n",
    "                for e in cols_eeg:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_EEG_{e}_mean']=df_sens_task[e].dropna()[df_sens_int[e] > -9000].mean()\n",
    "                        auc_data = df_sens_task.loc[df_sens_task[e].notna() & (df_sens_task[e] > -9000), ['Timestamp', e]]\n",
    "                        interaction[f'sens_{window}_EEG_{e}_AUC']=np.trapz(auc_data[e],x=auc_data['Timestamp'])/1000\n",
    "                    except:\n",
    "                        error['EEG']='Missing'\n",
    "\n",
    "                try:\n",
    "                    interaction[f'sens_{window}_GSR_PeakDetected_Binary'] =1 if df_sens_task['Peak Detected'].sum()>0 else 0\n",
    "                    gsr_data = df_sens_task[['Timestamp','Peak Detected']].dropna()\n",
    "                    mask = gsr_data['Peak Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = gsr_data.loc[mask, 'Peak Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_GSR_Peaks_Count'] =count_patches\n",
    "                except:\n",
    "                    error['GSR']='Missing'\n",
    "\n",
    "                try:\n",
    "                    blink_data = df_sens_task[['Timestamp','Blink Detected']].dropna()\n",
    "                    mask = blink_data['Blink Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = blink_data.loc[mask, 'Blink Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_ET_Blink_Count'] =count_patches\n",
    "                    interaction[f'sens_{window}_ET_Blink_Rate'] =count_patches/((df_sens_task['Timestamp'].values[-1]-df_sens_task['Timestamp'].values[0])/(1000 * 60))\n",
    "                except:\n",
    "                    error['ET']='Missing'\n",
    "\n",
    "            # TODO Get sensor data for non-interaction\n",
    "            ##################################### Add this in\n",
    "            results.append(interaction)\n",
    "            errors.append(error)\n",
    "\n",
    "            pass\n",
    "        except IndexError:\n",
    "            print(f'>>> Could not find {respondent} sensor data')\n",
    "        except:\n",
    "            print(f'>>> Failed {respondent}')\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(f'{out_path}biometric_results.csv')\n",
    "\n",
    "    errors = pd.DataFrame(errors)\n",
    "    errors.to_csv(f'{out_path}errors_biometric.csv')\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_export_dir = project_root / \"data\" / \"Export\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef61f8",
   "metadata": {},
   "source": [
    "## Explanation of functions\n",
    "\n",
    "The above functions are used to read in the sesor data files, one csv at a time, and extract single features per stimulus, and write these features to a simple results file.\n",
    "\n",
    "The functions must be adjusted to:\n",
    "- Discern between long form and short form\n",
    "- Isolate key moments from timings file provided by client\n",
    "- Extract time series\n",
    "- Compute group-wide features such as inter-subject correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84624884",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- Create naming dictionary for all stims\n",
    "- Get total times of all stims\n",
    "- Prepare key_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6248d4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sensor_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>001_116.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>001_58.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>001_114.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>001_102.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>001_108.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>001_107.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  sensor_file\n",
       "0  Group A  001_116.csv\n",
       "1  Group B   001_58.csv\n",
       "2  Group C  001_114.csv\n",
       "3  Group D  001_102.csv\n",
       "4  Group E  001_108.csv\n",
       "5  Group F  001_107.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate one sensor export per group for duration scanning\n",
    "\n",
    "\n",
    "group_sensor_files = {}\n",
    "for group_dir in sorted(data_export_dir.glob(\"Group *\")):\n",
    "    if not group_dir.is_dir():\n",
    "        continue\n",
    "    sensor_dirs = sorted(group_dir.glob(\"Analyses/*/Sensor Data\"))\n",
    "    csv_candidates = []\n",
    "    for sensor_dir in sensor_dirs:\n",
    "        csv_candidates.extend(sorted(sensor_dir.glob(\"*.csv\")))\n",
    "    group_sensor_files[group_dir.name] = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "sensor_selection = pd.DataFrame([\n",
    "    {\n",
    "        \"group\": group,\n",
    "        \"sensor_file\": path.name if path else None\n",
    "    }\n",
    "    for group, path in group_sensor_files.items()\n",
    "]).sort_values(\"group\").reset_index(drop=True)\n",
    "\n",
    "sensor_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99e8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4177351198.py:55: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n"
     ]
    }
   ],
   "source": [
    "# Collect per-group stimulus durations without aggregating across groups\n",
    "duration_tables = []\n",
    "issues = {}\n",
    "\n",
    "for group, path in group_sensor_files.items():\n",
    "    if path is None:\n",
    "        issues[group] = \"No sensor CSV found\"\n",
    "        continue\n",
    "    try:\n",
    "        df_group, _ = read_imotions(path)\n",
    "    except Exception as exc:\n",
    "        issues[group] = f\"read_imotions failed: {exc}\"\n",
    "        continue\n",
    "\n",
    "    required_cols = {\"SourceStimuliName\", \"Timestamp\"}\n",
    "    if not required_cols.issubset(df_group.columns):\n",
    "        issues[group] = \"Missing SourceStimuliName or Timestamp\"\n",
    "        continue\n",
    "    df_clean = df_group[[\"SourceStimuliName\", \"Timestamp\"]].copy()\n",
    "    df_clean = df_clean.dropna(subset=[\"SourceStimuliName\"])\n",
    "    df_clean[\"Timestamp\"] = pd.to_numeric(df_clean[\"Timestamp\"], errors=\"coerce\")\n",
    "    df_clean = df_clean.dropna(subset=[\"Timestamp\"])\n",
    "    if df_clean.empty:\n",
    "        issues[group] = \"No valid timestamp data\"\n",
    "        continue\n",
    "\n",
    "    group_duration = (\n",
    "        df_clean.groupby(\"SourceStimuliName\")[\"Timestamp\"]\n",
    "        .apply(lambda s: s.max() - s.min())\n",
    "        .reset_index(name=\"duration_ms\")\n",
    "    )\n",
    "\n",
    "    if group_duration.empty:\n",
    "        issues[group] = \"No stimuli with duration\"\n",
    "        continue\n",
    "\n",
    "    group_duration[\"duration_seconds\"] = group_duration[\"duration_ms\"] / 1000.0\n",
    "    group_duration[\"duration_minutes\"] = group_duration[\"duration_seconds\"] / 60.0\n",
    "    group_duration.insert(0, \"group\", group)\n",
    "    group_duration.rename(columns={\"SourceStimuliName\": \"stimulus_name\"}, inplace=True)\n",
    "    duration_tables.append(group_duration[[\"group\", \"stimulus_name\", \"duration_seconds\", \"duration_minutes\"]])\n",
    "\n",
    "if duration_tables:\n",
    "    stimulus_summary = pd.concat(duration_tables, ignore_index=True)\n",
    "    stimulus_summary.sort_values([\"group\", \"stimulus_name\"], inplace=True)\n",
    "    stimulus_summary[\"duration_seconds\"] = stimulus_summary[\"duration_seconds\"].round(2)\n",
    "    stimulus_summary[\"duration_minutes\"] = stimulus_summary[\"duration_minutes\"].round(2)\n",
    "    stimulus_summary.reset_index(drop=True, inplace=True)\n",
    "    stimulus_summary\n",
    "else:\n",
    "    print(\"No duration records computed.\")\n",
    "\n",
    "if issues:\n",
    "    pd.DataFrame(\n",
    "        {\"group\": list(issues.keys()), \"issue\": list(issues.values())}\n",
    "    ).sort_values(\"group\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9548d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>07 The Notebook</td>\n",
       "      <td>65.39</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group C</td>\n",
       "      <td>09 I Am Legend - Infected encounter</td>\n",
       "      <td>118.49</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>10 The Town - Bank robbery in nun masks</td>\n",
       "      <td>263.21</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group C</td>\n",
       "      <td>Abbott Elementary - S1E9 - Step Class</td>\n",
       "      <td>1291.75</td>\n",
       "      <td>21.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group C</td>\n",
       "      <td>HOME ALONE</td>\n",
       "      <td>115.09</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group                            stimulus_name  duration_seconds  \\\n",
       "0  Group C                          07 The Notebook             65.39   \n",
       "1  Group C      09 I Am Legend - Infected encounter            118.49   \n",
       "2  Group C  10 The Town - Bank robbery in nun masks            263.21   \n",
       "3  Group C    Abbott Elementary - S1E9 - Step Class           1291.75   \n",
       "4  Group C                               HOME ALONE            115.09   \n",
       "\n",
       "   duration_minutes  \n",
       "0              1.09  \n",
       "1              1.97  \n",
       "2              4.39  \n",
       "3             21.53  \n",
       "4              1.92  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38df137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group A': 'read_imotions failed: Unable to allocate 861. MiB for an array with shape (212, 532611) and data type float64',\n",
       " 'Group B': 'read_imotions failed: Unable to allocate 838. MiB for an array with shape (212, 518407) and data type float64',\n",
       " 'Group D': 'read_imotions failed: Unable to allocate 843. MiB for an array with shape (221, 499863) and data type float64',\n",
       " 'Group E': 'read_imotions failed: Unable to allocate 825. MiB for an array with shape (212, 510043) and data type float64'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ec5417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd2d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      59.82\n",
       "max    1291.77\n",
       "Name: duration_seconds, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary['duration_seconds'].agg(['min','max']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6057f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>unique_stimuli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group F</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  unique_stimuli\n",
       "0  Group C               6\n",
       "1  Group F               6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_per_group = stimulus_summary.groupby('group')['stimulus_name'].nunique().reset_index(name='unique_stimuli')\n",
    "stimuli_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_summary.to_csv(project_root / \"results\" / \"stimulus_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322effdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf37ed",
   "metadata": {},
   "source": [
    "## Stimulus Annotation Overview\n",
    "- `stimulus_rename` links each group-specific stimulus from `stimulus_summary` to a clean `title` and its presentation `Form` (`Long` or `Short`).\n",
    "- Some titles appear in both forms; the long cut (Ã¢â€°Ë†30 min) includes the short-form key moment as an embedded segment.\n",
    "- `key_moments` pinpoints, for every long-form title, when the key moment begins (`Lead-up Duration`) and how long it lasts (`Key moment Duration_LF`).\n",
    "- These tables let us align short-form clips with the corresponding segment inside the long-form presentation for downstream comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f5419",
   "metadata": {},
   "source": [
    "## Stage 1: Demographics\n",
    "We extract respondent-level identifiers and timing information from the metadata embedded in each sensor export to seed the unified view (UV). This pass scans every sensor CSV, captures study name, respondent attributes, and recording timestamps, and prepares the foundation for later feature merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bff719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03\n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05\n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40\n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42\n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22\n",
       "..          ...   ...        ...         ...        ...\n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03\n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06\n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14\n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41\n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00\n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "metadata_keys = [\n",
    "    \"Study name\",\n",
    "    \"Respondent Name\",\n",
    "    \"Respondent Group\",\n",
    "    \"Recording time\"\n",
    "]\n",
    "\n",
    "sensor_file_paths = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    ")\n",
    "\n",
    "def first_segment(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    return str(value).split(',')[0].strip()\n",
    "\n",
    "def parse_gender(raw_gender):\n",
    "    if not raw_gender:\n",
    "        return None\n",
    "    gender_lower = raw_gender.lower()\n",
    "    if \"female\" in gender_lower:\n",
    "        return \"Female\"\n",
    "    if \"male\" in gender_lower:\n",
    "        return \"Male\"\n",
    "    if \"other\" in gender_lower:\n",
    "        return \"Other\"\n",
    "    return raw_gender.title()\n",
    "\n",
    "def extract_group_letter(study_value, fallback_values):\n",
    "    if study_value:\n",
    "        terminal_match = re.search(r\"([A-Za-z])$\", study_value.strip())\n",
    "        if terminal_match:\n",
    "            return terminal_match.group(1).upper()\n",
    "        letters = re.findall(r\"[A-Za-z]\", study_value)\n",
    "        if letters:\n",
    "            return letters[-1].upper()\n",
    "    for candidate in fallback_values:\n",
    "        if candidate:\n",
    "            match = re.search(r\"Group\\s*([A-F])\", str(candidate), flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "demographic_records = []\n",
    "\n",
    "for csv_path in sensor_file_paths:\n",
    "    meta = read_imotions_metadata(csv_path, metadata=metadata_keys)\n",
    "\n",
    "    study_clean = first_segment(meta.get(\"Study name\"))\n",
    "    respondent_group_clean = first_segment(meta.get(\"Respondent Group\"))\n",
    "    group_letter = extract_group_letter(study_clean, [respondent_group_clean, csv_path.as_posix()])\n",
    "\n",
    "    respondent_raw = first_segment(meta.get(\"Respondent Name\"))\n",
    "    respondent_value = csv_path.stem\n",
    "    if respondent_raw:\n",
    "        respondent_digits = re.search(r\"\\d+\", respondent_raw)\n",
    "        if respondent_digits:\n",
    "            respondent_value = respondent_digits.group(0)\n",
    "        else:\n",
    "            respondent_value = respondent_raw\n",
    "\n",
    "    recording_raw = meta.get(\"Recording time\")\n",
    "    date_study = None\n",
    "    time_study = None\n",
    "    if recording_raw:\n",
    "        fragments = [frag.strip() for frag in str(recording_raw).split(',') if frag.strip()]\n",
    "        date_part = None\n",
    "        time_part = None\n",
    "        for fragment in fragments:\n",
    "            if fragment.lower().startswith(\"date:\"):\n",
    "                date_part = fragment.split(':', 1)[1].strip()\n",
    "            elif fragment.lower().startswith(\"time:\"):\n",
    "                time_part = fragment.split(':', 1)[1].strip()\n",
    "        if date_part and time_part:\n",
    "            dt_string = f\"{date_part} {time_part}\"\n",
    "            ts = pd.to_datetime(dt_string, utc=True, errors=\"coerce\")\n",
    "            if pd.isna(ts):\n",
    "                ts = pd.to_datetime(dt_string, errors=\"coerce\")\n",
    "                if pd.notna(ts) and ts.tzinfo is None:\n",
    "                    try:\n",
    "                        ts = ts.tz_localize(\"America/Chicago\")\n",
    "                    except Exception:\n",
    "                        ts = ts.tz_localize(\"UTC\")\n",
    "            if pd.notna(ts):\n",
    "                if ts.tzinfo is None:\n",
    "                    ts = ts.tz_localize(\"America/Chicago\")\n",
    "                else:\n",
    "                    ts = ts.tz_convert(\"America/Chicago\")\n",
    "                date_study = ts.strftime(\"%m/%d/%Y\")\n",
    "                time_study = ts.strftime(\"%H:%M:%S\")\n",
    "            else:\n",
    "                date_study = date_part\n",
    "        elif date_part:\n",
    "            date_study = date_part\n",
    "\n",
    "    demographic_records.append({\n",
    "        \"source_file\": csv_path.name,\n",
    "        \"group\": group_letter,\n",
    "        \"respondent\": respondent_value,\n",
    "        \"date_study\": date_study,\n",
    "        \"time_study\": time_study\n",
    "    })\n",
    "\n",
    "uv_stage1 = pd.DataFrame(demographic_records)\n",
    "\n",
    "if not uv_stage1.empty:\n",
    "    uv_stage1 = uv_stage1.sort_values([\"group\", \"respondent\"]).reset_index(drop=True)\n",
    "    uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da774bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>44-59</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-27</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>Male</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  age  gender age_group                      ethnicity  \\\n",
       "0        104   59    Male     44-59                          White   \n",
       "1        106   30    Male     28-43                          White   \n",
       "2        116   19    Male     18-27                          White   \n",
       "3         14   33    Male     28-43  Hispanic/Latino/Latina/Latinx   \n",
       "4          3   34  Female     28-43                          White   \n",
       "\n",
       "                 income_group          content_consumption  \\\n",
       "0    $60,000 or more per year  More than 24 hours per week   \n",
       "1    $60,000 or more per year       3 to 12 hours per week   \n",
       "2  $35,000  $60,000 per year       3 to 12 hours per week   \n",
       "3    $60,000 or more per year  More than 24 hours per week   \n",
       "4    $60,000 or more per year      12 to 24 hours per week   \n",
       "\n",
       "   content_consumption_movies  content_consumption_series  \\\n",
       "0                          10                          90   \n",
       "1                          25                          50   \n",
       "2                          25                          50   \n",
       "3                          20                          40   \n",
       "4                          10                          70   \n",
       "\n",
       "   content_consumption_short grid_comments  \n",
       "0                          0           NaN  \n",
       "1                         25           NaN  \n",
       "2                         25           NaN  \n",
       "3                         40       No EEG.  \n",
       "4                         20       No EEG.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach supplemental demographics from grid.csv\n",
    "grid_path = project_root / \"data\" / \"grid.csv\"\n",
    "grid_rename_map = {\n",
    "    \"QB2. Age\": \"age\",\n",
    "    \"QB2. Age.1\": \"age_group\",\n",
    "    \"QA2. Gender\": \"gender\",\n",
    "    \"QC. Ethnicity\": \"ethnicity\",\n",
    "    \"QD. Income\": \"income_group\",\n",
    "    \"Q1. Content Hours Per Week\": \"content_consumption\",\n",
    "    \"Q2. Program Type %- Movies\": \"content_consumption_movies\",\n",
    "    \"Q2. Program Type %- Series\": \"content_consumption_series\",\n",
    "    \"Q2. Program Type %- Short\": \"content_consumption_short\",\n",
    "    \"Comments\": \"grid_comments\",\n",
    "}\n",
    "\n",
    "grid_raw_full = pd.read_csv(grid_path, encoding=\"latin1\")\n",
    "grid_raw_full.columns = [col.strip() for col in grid_raw_full.columns]\n",
    "\n",
    "id_column = next((col for col in [\"respondent\", \"Respondent\", \"No.\", \"No\", \"Participant\", \"Participant #\"] if col in grid_raw_full.columns), None)\n",
    "if id_column is None:\n",
    "    raise ValueError(\"Unable to locate a respondent identifier column in grid.csv\")\n",
    "\n",
    "grid_raw_full = grid_raw_full.rename(columns={id_column: \"respondent\"})\n",
    "\n",
    "available_rename_map = {orig: dest for orig, dest in grid_rename_map.items() if orig in grid_raw_full.columns}\n",
    "missing_columns = sorted(set(grid_rename_map.keys()) - set(available_rename_map.keys()))\n",
    "if missing_columns:\n",
    "    print(f\"Warning: The following columns were not found in grid.csv and will be skipped: {missing_columns}\")\n",
    "\n",
    "grid_columns = [\"respondent\", *available_rename_map.keys()]\n",
    "grid_raw = grid_raw_full.loc[:, grid_columns].copy()\n",
    "\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "\n",
    "grid_raw[\"respondent\"] = pd.to_numeric(grid_raw[\"respondent\"], errors=\"coerce\")\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "grid_raw[\"respondent\"] = grid_raw[\"respondent\"].astype(int).astype(str)\n",
    "\n",
    "grid_subset = grid_raw.rename(columns=available_rename_map)\n",
    "\n",
    "text_cols = [\"age_group\", \"gender\", \"ethnicity\", \"income_group\", \"content_consumption\", \"grid_comments\"]\n",
    "for col in text_cols:\n",
    "    if col in grid_subset.columns:\n",
    "        grid_subset[col] = grid_subset[col].apply(lambda value: value.strip() if isinstance(value, str) else value)\n",
    "\n",
    "numeric_cols = [\"content_consumption_movies\", \"content_consumption_series\", \"content_consumption_short\", \"age\"]\n",
    "for col in numeric_cols:\n",
    "    if col in grid_subset.columns:\n",
    "        grid_subset[col] = pd.to_numeric(grid_subset[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "grid_subset = grid_subset.drop_duplicates(subset=\"respondent\", keep=\"first\")\n",
    "\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1 = uv_stage1.merge(grid_subset, on=\"respondent\", how=\"left\", validate=\"many_to_one\")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "display_columns = [\n",
    "    \"respondent\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"age_group\",\n",
    "    \"ethnicity\",\n",
    "    \"income_group\",\n",
    "    \"content_consumption\",\n",
    "    \"content_consumption_movies\",\n",
    "    \"content_consumption_series\",\n",
    "    \"content_consumption_short\",\n",
    "    \"grid_comments\",\n",
    "]\n",
    "existing_display_columns = [col for col in display_columns if col in uv_stage1.columns]\n",
    "uv_stage1.loc[:, existing_display_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a43b0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group respondent Short Form Long Form\n",
       "0     A        104    Mad Max  The Town\n",
       "1     A        106    Mad Max  The Town\n",
       "2     A        116    Mad Max  The Town\n",
       "3     A         14    Mad Max  The Town\n",
       "4     A          3    Mad Max  The Town"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_short_long_map = pd.DataFrame(\n",
    "    [\n",
    "        (\"A\", \"Mad Max\", \"The Town\"),\n",
    "        (\"B\", \"The Town\", \"Mad Max\"),\n",
    "        (\"C\", \"The Town\", \"Abbot Elementary\"),\n",
    "        (\"D\", \"Abbot Elementary\", \"The Town\"),\n",
    "        (\"E\", \"Abbot Elementary\", \"Mad Max\"),\n",
    "        (\"F\", \"Mad Max\", \"Abbot Elementary\"),\n",
    "    ],\n",
    "    columns=[\"group\", \"Short Form\", \"Long Form\"],\n",
    ")\n",
    "for column in [\"Short Form\", \"Long Form\"]:\n",
    "    group_short_long_map[column] = group_short_long_map[column].str.strip().str.rstrip(\",\")\n",
    "\n",
    "uv_stage1 = uv_stage1.merge(\n",
    "    group_short_long_map,\n",
    "    on=\"group\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1.loc[:, [\"group\", \"respondent\", \"Short Form\", \"Long Form\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb769cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate respondents detected.\n"
     ]
    }
   ],
   "source": [
    "duplicate_respondents = uv_stage1[uv_stage1.duplicated(subset=\"respondent\", keep=False)]\n",
    "if duplicate_respondents.empty:\n",
    "    print(\"No duplicate respondents detected.\")\n",
    "else:\n",
    "    duplicate_respondents.sort_values(\"respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92662888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>66</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>61</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03   59     44-59   \n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05   30     28-43   \n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40   19     18-27   \n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42   33     28-43   \n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22   34     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03   63     60-69   \n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06   66     60-69   \n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14   61     60-69   \n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41   34     28-43   \n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "\n",
       "    gender                      ethnicity                income_group  \\\n",
       "0     Male                          White    $60,000 or more per year   \n",
       "1     Male                          White    $60,000 or more per year   \n",
       "2     Male                          White  $35,000  $60,000 per year   \n",
       "3     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year   \n",
       "4   Female                          White    $60,000 or more per year   \n",
       "..     ...                            ...                         ...   \n",
       "78    Male         Black/African American    $60,000 or more per year   \n",
       "79    Male                          White  $35,000  $60,000 per year   \n",
       "80  Female         Black/African American  $35,000  $60,000 per year   \n",
       "81  Female                          White    $60,000 or more per year   \n",
       "82  Female                          White    $60,000 or more per year   \n",
       "\n",
       "            content_consumption  content_consumption_movies  \\\n",
       "0   More than 24 hours per week                          10   \n",
       "1        3 to 12 hours per week                          25   \n",
       "2        3 to 12 hours per week                          25   \n",
       "3   More than 24 hours per week                          20   \n",
       "4       12 to 24 hours per week                          10   \n",
       "..                          ...                         ...   \n",
       "78      12 to 24 hours per week                          70   \n",
       "79       3 to 12 hours per week                          10   \n",
       "80      12 to 24 hours per week                          30   \n",
       "81      12 to 24 hours per week                          40   \n",
       "82      12 to 24 hours per week                          20   \n",
       "\n",
       "    content_consumption_series  content_consumption_short grid_comments  \\\n",
       "0                           90                          0           NaN   \n",
       "1                           50                         25           NaN   \n",
       "2                           50                         25           NaN   \n",
       "3                           40                         40       No EEG.   \n",
       "4                           70                         20       No EEG.   \n",
       "..                         ...                        ...           ...   \n",
       "78                          20                         10           NaN   \n",
       "79                          80                         10           NaN   \n",
       "80                          30                         40           NaN   \n",
       "81                          60                          0           NaN   \n",
       "82                          60                         20           NaN   \n",
       "\n",
       "   Short Form         Long Form  \n",
       "0     Mad Max          The Town  \n",
       "1     Mad Max          The Town  \n",
       "2     Mad Max          The Town  \n",
       "3     Mad Max          The Town  \n",
       "4     Mad Max          The Town  \n",
       "..        ...               ...  \n",
       "78    Mad Max  Abbot Elementary  \n",
       "79    Mad Max  Abbot Elementary  \n",
       "80    Mad Max  Abbot Elementary  \n",
       "81    Mad Max  Abbot Elementary  \n",
       "82    Mad Max  Abbot Elementary  \n",
       "\n",
       "[83 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv_stage1.to_csv(project_root / \"results\" / \"uv_stage1.csv\", index=False)\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe8719",
   "metadata": {},
   "source": [
    "## Stage 2: Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b2525",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Stage 2 ingests the exposure-day survey responses while treating the Stage 1 export as the immutable base. Respondent metadata (age, gender, group, and the long/short assignment) is always reloaded from `results/uv_stage1.csv` before any survey joins. The section engineers Likert scores, familiarity composites, and open-ended extracts so downstream modeling can combine perceptual metrics with the demographic scaffold without relying on sensor features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd25fa2",
   "metadata": {},
   "source": [
    "### Workflow Summary\n",
    "1. Reload `results/uv_stage1.csv` to obtain the canonical respondent roster and long/short stimulus assignment.\n",
    "2. Build the survey metadata lookup (question map, polarity, subscales) to drive scoring logic.\n",
    "3. Ingest every group-level `MERGED_SURVEY_RESPONSE_MATRIX` export, harmonize respondent IDs, and apply the rename map.\n",
    "4. Score Likert, familiarity, and recency responses; compute enjoyment composites; and archive open-ended text separately.\n",
    "5. Merge the numeric survey features onto the Stage 1 base, logging missing or duplicate respondents to `uv_stage2_issues.csv`.\n",
    "6. Write Stage 2 deliverables: `uv_stage2_features.csv`, `uv_stage2_open_ended.csv`, and the combined `uv_stage2.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c179d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_file group respondent  date_study time_study  age age_group  gender  \\\n",
       "0  003_104.csv     A        104  10/16/2025   18:09:03   59     44-59    Male   \n",
       "1  002_106.csv     A        106  10/16/2025   19:35:05   30     28-43    Male   \n",
       "2  001_116.csv     A        116  10/18/2025   12:37:40   19     18-27    Male   \n",
       "3   006_14.csv     A         14  10/11/2025   09:32:42   33     28-43    Male   \n",
       "4    007_3.csv     A          3  10/10/2025   09:19:22   34     28-43  Female   \n",
       "\n",
       "                       ethnicity                income_group  \\\n",
       "0                          White    $60,000 or more per year   \n",
       "1                          White    $60,000 or more per year   \n",
       "2                          White  $35,000  $60,000 per year   \n",
       "3  Hispanic/Latino/Latina/Latinx    $60,000 or more per year   \n",
       "4                          White    $60,000 or more per year   \n",
       "\n",
       "           content_consumption  content_consumption_movies  \\\n",
       "0  More than 24 hours per week                          10   \n",
       "1       3 to 12 hours per week                          25   \n",
       "2       3 to 12 hours per week                          25   \n",
       "3  More than 24 hours per week                          20   \n",
       "4      12 to 24 hours per week                          10   \n",
       "\n",
       "   content_consumption_series  content_consumption_short grid_comments  \\\n",
       "0                          90                          0           NaN   \n",
       "1                          50                         25           NaN   \n",
       "2                          50                         25           NaN   \n",
       "3                          40                         40       No EEG.   \n",
       "4                          70                         20       No EEG.   \n",
       "\n",
       "  Short Form Long Form  \n",
       "0    Mad Max  The Town  \n",
       "1    Mad Max  The Town  \n",
       "2    Mad Max  The Town  \n",
       "3    Mad Max  The Town  \n",
       "4    Mad Max  The Town  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1 = uv_stage1_base.copy()\n",
    "uv_stage1_base.shape\n",
    "uv_stage1_base.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbea40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "survey_rename_map = pd.read_csv(project_root / \"data\" / \"survey_column_rename_stage3.csv\")\n",
    "survey_questions = pd.read_csv(project_root / \"data\" / \"survey_questions.csv\")\n",
    "\n",
    "survey_questions[\"question_code\"] = survey_questions[\"question_code\"].astype(str).str.strip()\n",
    "survey_questions[\"question_type\"] = survey_questions[\"question_type\"].str.lower()\n",
    "survey_questions[\"subscale\"] = survey_questions[\"subscale\"].fillna(\"\")\n",
    "survey_questions[\"polarity\"] = survey_questions[\"polarity\"].fillna(\"\")\n",
    "\n",
    "survey_metadata = (\n",
    "    survey_rename_map\n",
    "    .merge(survey_questions, on=\"question_code\", how=\"left\", suffixes=(\"\", \"_details\"))\n",
    ")\n",
    "\n",
    "survey_metadata[\"question_type\"] = survey_metadata[\"question_type\"].fillna(\"likert\")\n",
    "survey_metadata[\"subscale\"] = survey_metadata[\"subscale\"].fillna(\"\")\n",
    "survey_metadata[\"polarity\"] = survey_metadata[\"polarity\"].fillna(\"\")\n",
    "survey_metadata_lookup = (\n",
    "    survey_metadata\n",
    "    .drop_duplicates(subset=[\"target_column\"])\n",
    "    .set_index(\"target_column\")\n",
    ")\n",
    "\n",
    "survey_files = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Survey/MERGED_SURVEY_RESPONSE_MATRIX-*.txt\")\n",
    ")\n",
    "\n",
    "if not survey_files:\n",
    "    raise FileNotFoundError(\"No survey response text files detected under data/Export/*/Survey/.\")\n",
    "\n",
    "len(survey_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04435f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIKERT_PATTERN = re.compile(r\"^\\s*(\\d+)(?:\\.\\d+)?\")\n",
    "\n",
    "LIKERT_KEYWORDS = [\n",
    "    (\"strongly disagree\", 1.0),\n",
    "    (\"disagree\", 2.0),\n",
    "    (\"neither agree nor disagree\", 3.0),\n",
    "    (\"strongly agree\", 5.0),\n",
    "    (\"agree\", 4.0),\n",
    "]\n",
    "\n",
    "FAMILIARITY_KEY_PATTERNS = [\n",
    "    (0.0, (\"never heard\", \"not familiar\")),\n",
    "    (1.0, (\"heard of it, but never watched\", \"heard of it only\")),\n",
    "    (2.0, (\"seen a clip\", \"seen clips\", \"seen part\")),\n",
    "    (3.0, (\"watched it in full\", \"just once\")),\n",
    "    (4.0, (\"watched multiple\", \"very familiar\")),\n",
    "]\n",
    "\n",
    "LASTWATCHED_KEY_PATTERNS = [\n",
    "    (4.0, (\"past week\",)),\n",
    "    (3.0, (\"past month\", \"past 6 months\", \"past six months\")),\n",
    "    (2.0, (\"past 3 months\", \"past three months\")),\n",
    "    (1.0, (\"more than 3 months\", \"over 3 months\")),\n",
    "    (0.0, (\"more than 6 months\", \"don't remember\", \"never watched this movie in full\")),\n",
    "]\n",
    "\n",
    "\n",
    "def _clean_response(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if not text or text.upper() == \"EMPTY FIELD\":\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "\n",
    "def _parse_likert_value(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    lowered = text.lower()\n",
    "    for keyword, score in LIKERT_KEYWORDS:\n",
    "        if keyword in lowered:\n",
    "            return score\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _score_familiarity(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        numeric = float(match.group(1))\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    lowered = text.lower()\n",
    "    for score, patterns in FAMILIARITY_KEY_PATTERNS:\n",
    "        if any(pattern in lowered for pattern in patterns):\n",
    "            return score\n",
    "    try:\n",
    "        numeric = float(text)\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _score_last_watched(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        numeric = float(match.group(1))\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    lowered = text.lower()\n",
    "    for score, patterns in LASTWATCHED_KEY_PATTERNS:\n",
    "        if any(pattern in lowered for pattern in patterns):\n",
    "            return score\n",
    "    try:\n",
    "        numeric = float(text)\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _reverse_likert(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    return 6.0 - float(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7119b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E12 Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "1                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "2                           4 = Agree                          4 = Agree   \n",
       "3                           4 = Agree                 5 = Strongly Agree   \n",
       "4                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                  5 = Strongly Agree  ...   \n",
       "1                  5 = Strongly Agree  ...   \n",
       "2                           4 = Agree  ...   \n",
       "3                  5 = Strongly Agree  ...   \n",
       "4                  5 = Strongly Agree  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "81                                          NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _extract_group_letter(path: Path) -> str:\n",
    "    for part in path.parts:\n",
    "        if part.startswith(\"Group \") and \"-\" not in part:\n",
    "            return part.split()[-1].strip().upper()\n",
    "    raise ValueError(f\"Unable to determine group letter from path: {path}\")\n",
    "\n",
    "\n",
    "def _rename_survey_columns(df: pd.DataFrame, group_letter: str) -> pd.DataFrame:\n",
    "    rename_subset = survey_metadata.loc[survey_metadata[\"group\"] == group_letter]\n",
    "    rename_dict = {\n",
    "        raw: target\n",
    "        for raw, target in zip(rename_subset[\"raw_column\"], rename_subset[\"target_column\"])\n",
    "        if raw in df.columns\n",
    "    }\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    columns_to_keep = [\n",
    "        \"respondent\",\n",
    "        \"survey_group\",\n",
    "        \"survey_study\",\n",
    "        \"survey_gender\",\n",
    "        \"survey_age\",\n",
    "        \"survey_file\",\n",
    "        *sorted(rename_dict.values()),\n",
    "    ]\n",
    "    existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "    return df.loc[:, existing_columns]\n",
    "\n",
    "\n",
    "def _load_survey_file(path: Path) -> pd.DataFrame:\n",
    "    # Some open-ended answers contain newline characters and stray quotes; use python engine with minimal parsing assumptions.\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        dtype=str,\n",
    "        engine=\"python\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        encoding=\"utf-8\",\n",
    "        on_bad_lines=\"warn\",\n",
    "    )\n",
    "\n",
    "\n",
    "survey_frames = []\n",
    "\n",
    "for survey_path in survey_files:\n",
    "    group_letter = _extract_group_letter(survey_path)\n",
    "    df = _load_survey_file(survey_path)\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    df = df.replace({\"EMPTY FIELD\": np.nan})\n",
    "    df[\"RESPONDENT\"] = df[\"RESPONDENT\"].astype(str).str.strip()\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"RESPONDENT\": \"respondent\",\n",
    "            \"GROUP\": \"survey_group\",\n",
    "            \"STUDY\": \"survey_study\",\n",
    "            \"GENDER\": \"survey_gender\",\n",
    "            \"AGE\": \"survey_age\",\n",
    "        }\n",
    "    )\n",
    "    if \"survey_group\" not in df.columns:\n",
    "        df[\"survey_group\"] = group_letter\n",
    "    df[\"survey_group\"] = df[\"survey_group\"].fillna(group_letter).astype(str).str.strip()\n",
    "    if \"survey_study\" in df.columns:\n",
    "        df[\"survey_study\"] = df[\"survey_study\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_study\"] = np.nan\n",
    "    if \"survey_gender\" in df.columns:\n",
    "        df[\"survey_gender\"] = df[\"survey_gender\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_gender\"] = np.nan\n",
    "    df[\"survey_file\"] = survey_path.name\n",
    "    if \"survey_age\" in df.columns:\n",
    "        df[\"survey_age\"] = pd.to_numeric(df[\"survey_age\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"survey_age\"] = np.nan\n",
    "    df = _rename_survey_columns(df, group_letter)\n",
    "    survey_frames.append(df)\n",
    "\n",
    "survey_responses = pd.concat(survey_frames, ignore_index=True)\n",
    "survey_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d5a7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E12 Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "1                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "2                           4 = Agree                          4 = Agree   \n",
       "3                           4 = Agree                 5 = Strongly Agree   \n",
       "4                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                  5 = Strongly Agree  ...   \n",
       "1                  5 = Strongly Agree  ...   \n",
       "2                           4 = Agree  ...   \n",
       "3                  5 = Strongly Agree  ...   \n",
       "4                  5 = Strongly Agree  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "81                                          NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_responses[\"respondent\"] = survey_responses[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "duplicate_ids = sorted(survey_responses.loc[survey_responses[\"respondent\"].duplicated(), \"respondent\"].unique())\n",
    "if duplicate_ids:\n",
    "    print(f\"Warning: duplicate survey rows detected for respondents: {duplicate_ids}\")\n",
    "\n",
    "survey_numeric = survey_responses.drop_duplicates(subset=[\"respondent\"], keep=\"first\").copy()\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e556ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "81                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "81                                 NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "81                                         NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                            NaN  \n",
       "1                                            NaN  \n",
       "2                                            NaN  \n",
       "3                                            NaN  \n",
       "4                                            NaN  \n",
       "..                                           ...  \n",
       "77                                           NaN  \n",
       "78                                           NaN  \n",
       "79                                           NaN  \n",
       "80                                           NaN  \n",
       "81                                           NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "\n",
    "    question_code = (meta.get(\"question_code\") or \"\").strip().upper()\n",
    "    question_type = (meta.get(\"question_type\") or \"\").strip().lower()\n",
    "    polarity = (meta.get(\"polarity\") or \"\").strip().lower()\n",
    "\n",
    "    if question_code in {\"F1\", \"F3\"} or column.endswith(\"_Survey_Familiarity_F1\") or column.endswith(\"_Survey_Familiarity_F3\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_score_familiarity)\n",
    "    elif question_code == \"F2\" or column.endswith(\"_Survey_Familiarity_F2\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_score_last_watched)\n",
    "    elif question_type == \"likert\":\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_parse_likert_value)\n",
    "        if polarity == \"negative\":\n",
    "            survey_numeric[column] = survey_numeric[column].apply(_reverse_likert)\n",
    "\n",
    "def _clip_zero_to_four(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    try:\n",
    "        numeric = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return np.nan\n",
    "    return float(np.clip(numeric, 0.0, 4.0))\n",
    "\n",
    "familiarity_columns = [\n",
    "    column\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F2\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F3\")\n",
    "]\n",
    "\n",
    "for column in familiarity_columns:\n",
    "    survey_numeric[column] = survey_numeric[column].apply(_clip_zero_to_four)\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6336772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "81                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "81                                 NaN  ...   \n",
       "\n",
       "   Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN          \n",
       "1                                                 NaN          \n",
       "2                                                 NaN          \n",
       "3                                                 NaN          \n",
       "4                                                 NaN          \n",
       "..                                                ...          \n",
       "77                                               43.0          \n",
       "78                                               59.0          \n",
       "79                                               53.0          \n",
       "80                                               47.0          \n",
       "81                                               45.0          \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "77                                           3.583333      \n",
       "78                                           4.916667      \n",
       "79                                           4.416667      \n",
       "80                                           3.916667      \n",
       "81                                           3.750000      \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "77                                           0.687500            \n",
       "78                                           0.895833            \n",
       "79                                           0.770833            \n",
       "80                                           0.645833            \n",
       "81                                           0.645833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "77                                           0.645833                     \n",
       "78                                           0.979167                     \n",
       "79                                           0.854167                     \n",
       "80                                           0.729167                     \n",
       "81                                           0.687500                     \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "0                                                   0        \n",
       "1                                                   0        \n",
       "2                                                   0        \n",
       "3                                                   0        \n",
       "4                                                   0        \n",
       "..                                                ...        \n",
       "77                                                  0        \n",
       "78                                                  0        \n",
       "79                                                  0        \n",
       "80                                                  0        \n",
       "81                                                  0        \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "77                                                NaN      \n",
       "78                                                NaN      \n",
       "79                                                NaN      \n",
       "80                                                NaN      \n",
       "81                                                NaN      \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "77                                                NaN            \n",
       "78                                                NaN            \n",
       "79                                                NaN            \n",
       "80                                                NaN            \n",
       "81                                                NaN            \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN       \n",
       "1                                                 NaN       \n",
       "2                                                 NaN       \n",
       "3                                                 NaN       \n",
       "4                                                 NaN       \n",
       "..                                                ...       \n",
       "77                                                NaN       \n",
       "78                                                NaN       \n",
       "79                                                NaN       \n",
       "80                                                NaN       \n",
       "81                                                NaN       \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN             \n",
       "1                                                 NaN             \n",
       "2                                                 NaN             \n",
       "3                                                 NaN             \n",
       "4                                                 NaN             \n",
       "..                                                ...             \n",
       "77                                                NaN             \n",
       "78                                                NaN             \n",
       "79                                                NaN             \n",
       "80                                                NaN             \n",
       "81                                                NaN             \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "77                                                NaN                     \n",
       "78                                                NaN                     \n",
       "79                                                NaN                     \n",
       "80                                                NaN                     \n",
       "81                                                NaN                     \n",
       "\n",
       "[82 rows x 336 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscale_columns = defaultdict(list)\n",
    "overall_enjoyment_columns = defaultdict(list)\n",
    "\n",
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "    if meta.get(\"topic\") != \"enjoyment\":\n",
    "        continue\n",
    "    if (meta.get(\"question_type\") or \"\").strip().lower() != \"likert\":\n",
    "        continue\n",
    "    prefix = column.split(\"_Survey_\")[0]\n",
    "    subscale = (meta.get(\"subscale\") or \"\").strip()\n",
    "    overall_enjoyment_columns[prefix].append(column)\n",
    "    if subscale:\n",
    "        subscale_columns[(prefix, subscale)].append(column)\n",
    "\n",
    "for (prefix, subscale), cols in subscale_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_{subscale}_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_{subscale}_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_{subscale}_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_{subscale}_Normalized\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "    sum_values = values.sum(axis=1, min_count=1)\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = sum_values\n",
    "    survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "for prefix, cols in overall_enjoyment_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_EnjoymentComposite_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_EnjoymentComposite_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_EnjoymentComposite_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_EnjoymentComposite_Normalized\"\n",
    "    norm_corrected_col = f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\"\n",
    "    corrected_col = f\"{prefix}_Survey_EnjoymentComposite_Corrected\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "\n",
    "    raw_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "    corrected_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "\n",
    "    for column in cols:\n",
    "        polarity_meta = \"\"\n",
    "        if column in survey_metadata_lookup.index:\n",
    "            polarity_meta = (survey_metadata_lookup.loc[column].get(\"polarity\") or \"\").strip().lower()\n",
    "        if column in survey_responses.columns:\n",
    "            raw_series = survey_responses.loc[values.index, column]\n",
    "            parsed_series = raw_series.apply(_parse_likert_value)\n",
    "        else:\n",
    "            fallback_series = pd.to_numeric(survey_numeric.loc[values.index, column], errors=\"coerce\")\n",
    "            if polarity_meta == \"negative\":\n",
    "                parsed_series = fallback_series.apply(_reverse_likert)\n",
    "            else:\n",
    "                parsed_series = fallback_series\n",
    "        raw_components[column] = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        corrected_series = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        if polarity_meta == \"negative\":\n",
    "            corrected_series = corrected_series.apply(_reverse_likert)\n",
    "        corrected_components[column] = corrected_series\n",
    "\n",
    "    raw_sum_values = raw_components.sum(axis=1, min_count=1)\n",
    "    corrected_sum = corrected_components.sum(axis=1, min_count=1)\n",
    "    raw_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((raw_sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((corrected_sum - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_mean = np.where(count_values > 0, corrected_sum / count_values, np.nan)\n",
    "\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = raw_sum_values\n",
    "    survey_numeric[corrected_col] = corrected_sum\n",
    "    survey_numeric[mean_col] = corrected_mean\n",
    "    survey_numeric[norm_col] = raw_normalized\n",
    "    survey_numeric[norm_corrected_col] = corrected_normalized\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c762d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_33984\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "81                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "81                                 NaN  ...   \n",
       "\n",
       "   Long_The Town_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                           0.000   \n",
       "1                                           0.750   \n",
       "2                                           0.000   \n",
       "3                                           0.000   \n",
       "4                                           0.125   \n",
       "..                                            ...   \n",
       "77                                            NaN   \n",
       "78                                            NaN   \n",
       "79                                            NaN   \n",
       "80                                            NaN   \n",
       "81                                            NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "..                                           ...   \n",
       "77                                           0.0   \n",
       "78                                           0.0   \n",
       "79                                           0.0   \n",
       "80                                           0.0   \n",
       "81                                           0.0   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Count  \\\n",
       "0                                                   0    \n",
       "1                                                   0    \n",
       "2                                                   0    \n",
       "3                                                   0    \n",
       "4                                                   0    \n",
       "..                                                ...    \n",
       "77                                                  0    \n",
       "78                                                  0    \n",
       "79                                                  0    \n",
       "80                                                  0    \n",
       "81                                                  0    \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                                 NaN         \n",
       "1                                                 NaN         \n",
       "2                                                 NaN         \n",
       "3                                                 NaN         \n",
       "4                                                 NaN         \n",
       "..                                                ...         \n",
       "77                                                NaN         \n",
       "78                                                NaN         \n",
       "79                                                NaN         \n",
       "80                                                NaN         \n",
       "81                                                NaN         \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1  \\\n",
       "0                                   0.0   \n",
       "1                                   4.0   \n",
       "2                                   1.0   \n",
       "3                                   0.0   \n",
       "4                                   3.0   \n",
       "..                                  ...   \n",
       "77                                  5.0   \n",
       "78                                  0.0   \n",
       "79                                  3.0   \n",
       "80                                  0.0   \n",
       "81                                  1.0   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Count  \\\n",
       "0                                           2   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "3                                           2   \n",
       "4                                           2   \n",
       "..                                        ...   \n",
       "77                                          2   \n",
       "78                                          2   \n",
       "79                                          2   \n",
       "80                                          2   \n",
       "81                                          2   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                            0.000   \n",
       "1                                            0.500   \n",
       "2                                            0.125   \n",
       "3                                            0.000   \n",
       "4                                            0.375   \n",
       "..                                             ...   \n",
       "77                                           0.625   \n",
       "78                                           0.000   \n",
       "79                                           0.375   \n",
       "80                                           0.000   \n",
       "81                                           0.125   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "..                                   ...   \n",
       "77                                   0.0   \n",
       "78                                   0.0   \n",
       "79                                   0.0   \n",
       "80                                   0.0   \n",
       "81                                   0.0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Count  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "77                                           0   \n",
       "78                                           0   \n",
       "79                                           0   \n",
       "80                                           0   \n",
       "81                                           0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Normalized  \n",
       "0                                               NaN  \n",
       "1                                               NaN  \n",
       "2                                               NaN  \n",
       "3                                               NaN  \n",
       "4                                               NaN  \n",
       "..                                              ...  \n",
       "77                                              NaN  \n",
       "78                                              NaN  \n",
       "79                                              NaN  \n",
       "80                                              NaN  \n",
       "81                                              NaN  \n",
       "\n",
       "[82 rows x 354 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "familiarity_prefixes = sorted({\n",
    "    column.split(\"_Survey_\")[0]\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "})\n",
    "\n",
    "for prefix in familiarity_prefixes:\n",
    "    f1_col = f\"{prefix}_Survey_Familiarity_F1\"\n",
    "    f2_col = f\"{prefix}_Survey_Familiarity_F2\"\n",
    "    if f1_col not in survey_numeric.columns or f2_col not in survey_numeric.columns:\n",
    "        continue\n",
    "    c1_col = f\"{prefix}_Survey_Familiarity_C1\"\n",
    "    count_col = f\"{prefix}_Survey_Familiarity_C1_Count\"\n",
    "    norm_col = f\"{prefix}_Survey_Familiarity_C1_Normalized\"\n",
    "    pair = survey_numeric[[f1_col, f2_col]]\n",
    "    sum_values = pair.fillna(0).sum(axis=1)\n",
    "    count_values = pair.notna().sum(axis=1)\n",
    "    survey_numeric[c1_col] = sum_values\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip(sum_values / (4 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a062347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "81                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "81                                 NaN  ...   \n",
       "\n",
       "    Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      2.0   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "77                                     NaN   \n",
       "78                                     NaN   \n",
       "79                                     NaN   \n",
       "80                                     NaN   \n",
       "81                                     NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               5.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               4.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               4.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               3.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               1.0   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                      1.0   \n",
       "1                                      4.0   \n",
       "2                                      4.0   \n",
       "3                                      2.0   \n",
       "4                                      4.0   \n",
       "..                                     ...   \n",
       "77                                     4.0   \n",
       "78                                     1.0   \n",
       "79                                     4.0   \n",
       "80                                     4.0   \n",
       "81                                     1.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                      1.0   \n",
       "1                                      3.0   \n",
       "2                                      3.0   \n",
       "3                                      2.0   \n",
       "4                                      3.0   \n",
       "..                                     ...   \n",
       "77                                     3.0   \n",
       "78                                     1.0   \n",
       "79                                     3.0   \n",
       "80                                     3.0   \n",
       "81                                     1.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "77                                     1.0   \n",
       "78                                     NaN   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     NaN   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      4.0   \n",
       "80                                      4.0   \n",
       "81                                      0.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      3.0   \n",
       "80                                      3.0   \n",
       "81                                      0.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F2  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                       NaN  \n",
       "..                                      ...  \n",
       "77                                      NaN  \n",
       "78                                      NaN  \n",
       "79                                      1.0  \n",
       "80                                      1.0  \n",
       "81                                      NaN  \n",
       "\n",
       "[82 rows x 357 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_ended_columns = [\n",
    "    column\n",
    "    for column, meta in survey_metadata_lookup.iterrows()\n",
    "    if column in survey_numeric.columns and (meta.get(\"question_type\") or \"\").strip().lower() == \"open ended\"\n",
    "]\n",
    "\n",
    "survey_open_ended = survey_responses[[\n",
    "    \"respondent\",\n",
    "    \"survey_group\",\n",
    "    \"survey_study\",\n",
    "    \"survey_gender\",\n",
    "    \"survey_age\",\n",
    "    \"survey_file\",\n",
    "    *open_ended_columns,\n",
    "]].copy()\n",
    "\n",
    "survey_features = survey_numeric.drop(columns=[col for col in open_ended_columns if col in survey_numeric.columns])\n",
    "\n",
    "# Integrate screening familiarity composites\n",
    "screening_path = project_root / \"results\" / \"individual_composite_scores.csv\"\n",
    "if screening_path.exists():\n",
    "    screening_raw = pd.read_csv(screening_path)\n",
    "    screening_raw[\"respondent\"] = screening_raw[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "    screening_value_columns = [\n",
    "        col\n",
    "        for col in screening_raw.columns\n",
    "        if col.endswith(\"_Survey_Familiarity_F1\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F2\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F3\")\n",
    "        or col.endswith(\"_Survey_Familiarity_C1\")\n",
    "    ]\n",
    "\n",
    "    respondent_groups = (\n",
    "        uv_stage1\n",
    "        .loc[:, [\"respondent\", \"group\"]]\n",
    "        .assign(group=lambda df: df[\"group\"].astype(str).str.strip().str.upper())\n",
    "        .set_index(\"respondent\")\n",
    "        .to_dict()\n",
    "    )[\"group\"]\n",
    "\n",
    "    title_normalization = {\n",
    "        \"mad max fury road\": \"Mad Max\",\n",
    "        \"mad max\": \"Mad Max\",\n",
    "        \"the town\": \"The Town\",\n",
    "        \"abbot elementary\": \"Abbot Elementary\",\n",
    "        \"abbott elementary\": \"Abbot Elementary\",\n",
    "    }\n",
    "\n",
    "    def canonicalize_title(raw_title: str) -> str:\n",
    "        cleaned = str(raw_title).strip()\n",
    "        return title_normalization.get(cleaned.lower(), cleaned)\n",
    "\n",
    "    stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "    stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"([A-F])\", expand=False)\n",
    "    stimulus_map[\"title_clean\"] = stimulus_map[\"title\"].astype(str).str.strip()\n",
    "\n",
    "    group_title_form_lookup = {}\n",
    "    default_form_per_title = {}\n",
    "\n",
    "    for row in stimulus_map.itertuples():\n",
    "        if pd.isna(row.group_letter) or pd.isna(row.title_clean) or pd.isna(row.form):\n",
    "            continue\n",
    "        canonical_title = canonicalize_title(row.title_clean)\n",
    "        form_value = str(row.form).title()\n",
    "        group_title_form_lookup[(row.group_letter, canonical_title)] = form_value\n",
    "        default_form_per_title.setdefault(canonical_title, form_value)\n",
    "\n",
    "    screening_records = []\n",
    "\n",
    "    for _, row in screening_raw.iterrows():\n",
    "        respondent_id = row.get(\"respondent\")\n",
    "        if respondent_id is None:\n",
    "            continue\n",
    "        respondent_id = str(respondent_id).strip()\n",
    "        group_letter = respondent_groups.get(respondent_id)\n",
    "        for column in screening_value_columns:\n",
    "            value = row.get(column)\n",
    "            if pd.isna(value) or value == \"\":\n",
    "                continue\n",
    "            base_part, _, suffix_part = column.partition(\"_Survey_Familiarity_\")\n",
    "            if not suffix_part:\n",
    "                continue\n",
    "            question_code = suffix_part.strip()\n",
    "            canonical_title = canonicalize_title(base_part.strip())\n",
    "            form_value = None\n",
    "            if group_letter:\n",
    "                form_value = group_title_form_lookup.get((group_letter, canonical_title))\n",
    "            if form_value is None:\n",
    "                form_value = default_form_per_title.get(canonical_title, \"Long\")\n",
    "            target_column = f\"{form_value}_{canonical_title}_Screening_Familiarity_{question_code}\"\n",
    "            screening_records.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"target_column\": target_column,\n",
    "                \"value\": pd.to_numeric(value, errors=\"coerce\")\n",
    "            })\n",
    "\n",
    "    if screening_records:\n",
    "        screening_features = (\n",
    "            pd.DataFrame(screening_records)\n",
    "            .pivot_table(index=\"respondent\", columns=\"target_column\", values=\"value\", aggfunc=\"first\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        screening_features.columns.name = None\n",
    "        survey_features = survey_features.merge(screening_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "survey_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef38cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 1 Stage 2 issues to uv_stage2_issues.csv.\n",
      "Stage 2 survey features saved to uv_stage2_features.csv with 82 respondents and 356 feature columns.\n",
      "Open-ended responses archived to uv_stage2_open_ended.csv.\n",
      "Unified view with Stage 2 survey data exported to uv_stage2.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>66</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>61</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 373 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03   59     44-59   \n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05   30     28-43   \n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40   19     18-27   \n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42   33     28-43   \n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22   34     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03   63     60-69   \n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06   66     60-69   \n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14   61     60-69   \n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41   34     28-43   \n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "\n",
       "    gender                      ethnicity                income_group  ...  \\\n",
       "0     Male                          White    $60,000 or more per year  ...   \n",
       "1     Male                          White    $60,000 or more per year  ...   \n",
       "2     Male                          White  $35,000  $60,000 per year  ...   \n",
       "3     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year  ...   \n",
       "4   Female                          White    $60,000 or more per year  ...   \n",
       "..     ...                            ...                         ...  ...   \n",
       "78    Male         Black/African American    $60,000 or more per year  ...   \n",
       "79    Male                          White  $35,000  $60,000 per year  ...   \n",
       "80  Female         Black/African American  $35,000  $60,000 per year  ...   \n",
       "81  Female                          White    $60,000 or more per year  ...   \n",
       "82  Female                          White    $60,000 or more per year  ...   \n",
       "\n",
       "   Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                     1.0   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     1.0   \n",
       "4                                     NaN   \n",
       "..                                    ...   \n",
       "78                                    NaN   \n",
       "79                                    NaN   \n",
       "80                                    NaN   \n",
       "81                                    NaN   \n",
       "82                                    NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               0.0   \n",
       "1                                               4.0   \n",
       "2                                               1.0   \n",
       "3                                               8.0   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               0.0   \n",
       "1                                               3.0   \n",
       "2                                               1.0   \n",
       "3                                               4.0   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               NaN   \n",
       "1                                               1.0   \n",
       "2                                               NaN   \n",
       "3                                               4.0   \n",
       "4                                               NaN   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                     1.0   \n",
       "1                                     4.0   \n",
       "2                                     4.0   \n",
       "3                                     8.0   \n",
       "4                                     5.0   \n",
       "..                                    ...   \n",
       "78                                    6.0   \n",
       "79                                    1.0   \n",
       "80                                    1.0   \n",
       "81                                    4.0   \n",
       "82                                    4.0   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                     1.0   \n",
       "1                                     3.0   \n",
       "2                                     3.0   \n",
       "3                                     4.0   \n",
       "4                                     3.0   \n",
       "..                                    ...   \n",
       "78                                    4.0   \n",
       "79                                    1.0   \n",
       "80                                    1.0   \n",
       "81                                    3.0   \n",
       "82                                    3.0   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                     NaN   \n",
       "1                                     1.0   \n",
       "2                                     1.0   \n",
       "3                                     4.0   \n",
       "4                                     2.0   \n",
       "..                                    ...   \n",
       "78                                    2.0   \n",
       "79                                    NaN   \n",
       "80                                    NaN   \n",
       "81                                    1.0   \n",
       "82                                    1.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "..                                     ...   \n",
       "78                                     6.0   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     0.0   \n",
       "82                                     4.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "..                                     ...   \n",
       "78                                     3.0   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     0.0   \n",
       "82                                     3.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_F2  \n",
       "0                                      NaN  \n",
       "1                                      NaN  \n",
       "2                                      NaN  \n",
       "3                                      NaN  \n",
       "4                                      NaN  \n",
       "..                                     ...  \n",
       "78                                     3.0  \n",
       "79                                     NaN  \n",
       "80                                     NaN  \n",
       "81                                     NaN  \n",
       "82                                     1.0  \n",
       "\n",
       "[83 rows x 373 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def _safe_write_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except PermissionError:\n",
    "        fallback = path.with_name(f\"{path.stem}_{pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')}.csv\")\n",
    "        df.to_csv(fallback, index=False)\n",
    "        print(f\"Permission denied for {path}. Saved to {fallback.name} instead.\")\n",
    "        return fallback\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stage1_path = results_dir / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "survey_features = survey_features.copy()\n",
    "survey_features[\"respondent\"] = survey_features[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage2 = uv_stage1_base.merge(survey_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "issues_records: list[dict] = []\n",
    "feature_ids = survey_features[\"respondent\"].dropna().astype(str).str.strip()\n",
    "expected_ids = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "missing_ids = sorted(set(expected_ids) - set(feature_ids))\n",
    "for respondent in missing_ids:\n",
    "    issues_records.append({\n",
    "        \"respondent\": respondent,\n",
    "        \"issue\": \"stage2_features_missing\",\n",
    "    })\n",
    "\n",
    "duplicate_mask = feature_ids.duplicated(keep=False)\n",
    "if duplicate_mask.any():\n",
    "    duplicates = feature_ids[duplicate_mask]\n",
    "    for respondent in sorted(duplicates.unique()):\n",
    "        count = int((feature_ids == respondent).sum())\n",
    "        issues_records.append({\n",
    "            \"respondent\": respondent,\n",
    "            \"issue\": f\"stage2_duplicate_records_count_{count}\",\n",
    "        })\n",
    "\n",
    "issues_df = pd.DataFrame(issues_records)\n",
    "\n",
    "features_path = _safe_write_csv(survey_features, results_dir / \"uv_stage2_features.csv\")\n",
    "open_ended_path = _safe_write_csv(survey_open_ended, results_dir / \"uv_stage2_open_ended.csv\")\n",
    "uv_path = _safe_write_csv(uv_stage2, results_dir / \"uv_stage2.csv\")\n",
    "\n",
    "issues_path = results_dir / \"uv_stage2_issues.csv\"\n",
    "if issues_df.empty:\n",
    "    if issues_path.exists():\n",
    "        issues_path.unlink()\n",
    "    print(\"No Stage 2 survey issues detected.\")\n",
    "else:\n",
    "    _safe_write_csv(issues_df, issues_path)\n",
    "    print(f\"Logged {issues_df.shape[0]} Stage 2 issues to {issues_path.name}.\")\n",
    "\n",
    "print(\n",
    "    f\"Stage 2 survey features saved to {features_path.name} with {survey_features.shape[0]} respondents and \"\n",
    "    f\"{survey_features.shape[1] - 1} feature columns.\"\n",
    ")\n",
    "print(f\"Open-ended responses archived to {open_ended_path.name}.\")\n",
    "print(f\"Unified view with Stage 2 survey data exported to {uv_path.name}.\")\n",
    "\n",
    "uv_stage2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72330f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31    25.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Sum, dtype: float64\n",
      "31    2.0\n",
      "Name: Long_Abbot Elementary_Survey_Enjoyment_E18, dtype: float64\n",
      "31    23.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Sum'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_Enjoyment_E18'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15074b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>normalized_from_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "22                                               41.0     \n",
       "23                                               51.0     \n",
       "24                                               50.0     \n",
       "25                                               45.0     \n",
       "26                                               51.0     \n",
       "27                                               35.0     \n",
       "28                                               19.0     \n",
       "29                                               19.0     \n",
       "30                                               55.0     \n",
       "31                                               25.0     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "22                                               41.0           \n",
       "23                                               53.0           \n",
       "24                                               54.0           \n",
       "25                                               49.0           \n",
       "26                                               55.0           \n",
       "27                                               37.0           \n",
       "28                                               15.0           \n",
       "29                                               15.0           \n",
       "30                                               55.0           \n",
       "31                                               23.0           \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "22                                                 12       \n",
       "23                                                 12       \n",
       "24                                                 12       \n",
       "25                                                 12       \n",
       "26                                                 12       \n",
       "27                                                 12       \n",
       "28                                                 12       \n",
       "29                                                 12       \n",
       "30                                                 12       \n",
       "31                                                 12       \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "22                                           0.604167            \n",
       "23                                           0.812500            \n",
       "24                                           0.791667            \n",
       "25                                           0.687500            \n",
       "26                                           0.812500            \n",
       "27                                           0.479167            \n",
       "28                                           0.145833            \n",
       "29                                           0.145833            \n",
       "30                                           0.895833            \n",
       "31                                           0.270833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "22                                           0.604167                     \n",
       "23                                           0.854167                     \n",
       "24                                           0.875000                     \n",
       "25                                           0.770833                     \n",
       "26                                           0.895833                     \n",
       "27                                           0.520833                     \n",
       "28                                           0.062500                     \n",
       "29                                           0.062500                     \n",
       "30                                           0.895833                     \n",
       "31                                           0.229167                     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "22                                           3.416667      \n",
       "23                                           4.416667      \n",
       "24                                           4.500000      \n",
       "25                                           4.083333      \n",
       "26                                           4.583333      \n",
       "27                                           3.083333      \n",
       "28                                           1.250000      \n",
       "29                                           1.250000      \n",
       "30                                           4.583333      \n",
       "31                                           1.916667      \n",
       "\n",
       "    normalized_from_corrected  \n",
       "22                   0.604167  \n",
       "23                   0.854167  \n",
       "24                   0.875000  \n",
       "25                   0.770833  \n",
       "26                   0.895833  \n",
       "27                   0.520833  \n",
       "28                   0.062500  \n",
       "29                   0.062500  \n",
       "30                   0.895833  \n",
       "31                   0.229167  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Long_Abbot Elementary\"\n",
    "columns_to_show = [\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Sum\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Corrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Count\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Normalized\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Mean\",\n",
    "]\n",
    "comparison = survey_features.loc[\n",
    "    survey_features[f\"{prefix}_Survey_EnjoymentComposite_Count\"].gt(0),\n",
    "    columns_to_show\n",
    "].head(10).copy()\n",
    "comparison[\"normalized_from_corrected\"] = np.clip(\n",
    "    (comparison[f\"{prefix}_Survey_EnjoymentComposite_Corrected\"]\n",
    "     - comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"])\n",
    "    / (4.0 * comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"]),\n",
    "    0,\n",
    "    1,\n",
    " )\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b2306",
   "metadata": {},
   "source": [
    "## Stage 3: Post Questionnaire\n",
    "The seven-day follow-up instrument captures delayed recall, comprehension, and broader post-viewing perceptions. This section documents how the Post questionnaire exports extend the unified view (`uv`) with respondent-level memory, confidence, and engagement measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97e60a",
   "metadata": {},
   "source": [
    "### Planned Workflow\n",
    "- Inventory every group-level Post questionnaire export under `data/Recall/`, ensuring respondent IDs stay string-typed for clean joins.\n",
    "- Harmonize column names via `data/post_survey_map.csv` so variants or duplicate headers collapse to a single `question_code`.\n",
    "- Engineer response features (e.g., accuracy, confidence, free-text summaries) while following the `{form}_{title}_Post_{metric}_{method}` naming pattern.\n",
    "- Merge the Post feature frame onto the Stage 1 base (preserving respondent metadata) and log gaps to a dedicated issues table.\n",
    "- Validate the enriched unified view and emit Stage 3 outputs (`uv_stage3.csv`, `uv_stage3_issues.csv`) to `results/` with timestamped fallbacks when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcfd5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated post_survey_map.csv with question_code column; 7 rows are missing a parsed code.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_code</th>\n",
       "      <th>type</th>\n",
       "      <th>subscale</th>\n",
       "      <th>category</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Group A</th>\n",
       "      <th>Group B</th>\n",
       "      <th>Group C</th>\n",
       "      <th>Group D</th>\n",
       "      <th>Group E</th>\n",
       "      <th>Group F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>likert</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3. This scene was part of the videos you wat...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>multiple</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Wonder Woman</td>\n",
       "      <td>Wonder Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>likert</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Wonder Woman</td>\n",
       "      <td>Wonder Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.3. This scene was not in the videos you watc...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>multiple</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Wonder Woman</td>\n",
       "      <td>Wonder Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>likert</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.3. This scene was part of the videos you wat...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>likert</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question question_code      type  \\\n",
       "0  1.1. Did you see this scene in the videos you ...           1.1    binary   \n",
       "1  1.2. How confident are you in your answer? Ple...           1.2    likert   \n",
       "2  1.3. This scene was part of the videos you wat...           1.3  multiple   \n",
       "3  2.1. Did you see this scene in the videos you ...           2.1    binary   \n",
       "4  2.2. How confident are you in your answer? Ple...           2.2    likert   \n",
       "5  2.3. This scene was not in the videos you watc...           2.3  multiple   \n",
       "6  3.1. Did you see this scene in the videos you ...           3.1    binary   \n",
       "7  3.2. How confident are you in your answer? Ple...           3.2    likert   \n",
       "8  3.3. This scene was part of the videos you wat...           3.3    likert   \n",
       "9  4.1. Did you see this scene in the videos you ...           4.1    binary   \n",
       "\n",
       "      subscale    category accuracy          Group A          Group B  \\\n",
       "0  recognition         key      hit          Mad Max         The Town   \n",
       "1  recognition         key      hit          Mad Max         The Town   \n",
       "2  recognition         key      hit          Mad Max         The Town   \n",
       "3  recognition        fake     miss  The Dark Knight  The Dark Knight   \n",
       "4  recognition        fake     miss  The Dark Knight  The Dark Knight   \n",
       "5  recognition        fake     miss  The Dark Knight  The Dark Knight   \n",
       "6  recognition  distractor      hit          Titanic     The Notebook   \n",
       "7  recognition  distractor      hit          Titanic     The Notebook   \n",
       "8  recognition  distractor      hit          Titanic     The Notebook   \n",
       "9  recognition         key      hit         The Town          Mad Max   \n",
       "\n",
       "            Group C           Group D           Group E           Group F  \n",
       "0          The Town  Abbot Elementary           Mad Max           Mad Max  \n",
       "1          The Town  Abbot Elementary           Mad Max           Mad Max  \n",
       "2          The Town  Abbot Elementary           Mad Max           Mad Max  \n",
       "3        Goodfellas        Goodfellas      Wonder Woman      Wonder Woman  \n",
       "4        Goodfellas        Goodfellas      Wonder Woman      Wonder Woman  \n",
       "5        Goodfellas        Goodfellas      Wonder Woman      Wonder Woman  \n",
       "6      The Notebook     Schitts Creek     Schitts Creek           Titanic  \n",
       "7      The Notebook     Schitts Creek     Schitts Creek           Titanic  \n",
       "8      The Notebook     Schitts Creek     Schitts Creek           Titanic  \n",
       "9  Abbot Elementary          The Town  Abbot Elementary  Abbot Elementary  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "post_map_path = project_root / \"data\" / \"post_survey_map.csv\"\n",
    "post_map_df = pd.read_csv(post_map_path)\n",
    "\n",
    "code_pattern = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
    "\n",
    "def extract_question_code(text: str):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = code_pattern.match(str(text))\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "if \"question_code\" not in post_map_df.columns:\n",
    "    post_map_df.insert(1, \"question_code\", post_map_df[\"question\"].apply(extract_question_code))\n",
    "else:\n",
    "    post_map_df[\"question_code\"] = post_map_df[\"question\"].apply(extract_question_code)\n",
    "\n",
    "post_map_df.to_csv(post_map_path, index=False)\n",
    "missing_codes = post_map_df[\"question_code\"].isna().sum()\n",
    "print(\n",
    "    f\"Updated {post_map_path.name} with question_code column; \"\n",
    "    f\"{missing_codes} rows are missing a parsed code.\"\n",
    ")\n",
    "post_map_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eddef5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 3 respondent(s) due to post group mismatch: 6, 116, 117.\n",
      "Parsed 2880 recognition feature rows across 39 composite columns and 110 raw response columns for 80 respondents.\n",
      "Sample feature columns:\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q07\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q04</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q07</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q09</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q4-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q4-2</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q7-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q7-2</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q9-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q9-2</th>\n",
       "      <th>...</th>\n",
       "      <th>distractor_Post_Recognition_Sum</th>\n",
       "      <th>fake_Post_Recognition_Count</th>\n",
       "      <th>fake_Post_Recognition_Mean</th>\n",
       "      <th>fake_Post_Recognition_NormalizedMean</th>\n",
       "      <th>fake_Post_Recognition_Sum</th>\n",
       "      <th>unseen_Post_Recognition_Count</th>\n",
       "      <th>unseen_Post_Recognition_Mean</th>\n",
       "      <th>unseen_Post_Recognition_NormalizedMean</th>\n",
       "      <th>unseen_Post_Recognition_Sum</th>\n",
       "      <th>post_survey_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>data/Post/Group B_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group C_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  Long_Abbot Elementary_Post_RecognitionComposite_Q04  \\\n",
       "0          1                                                1.0     \n",
       "1         10                                                NaN     \n",
       "2        100                                                NaN     \n",
       "3        101                                                1.0     \n",
       "4        102                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_RecognitionComposite_Q07  \\\n",
       "0                                               0.75     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                               1.00     \n",
       "4                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_RecognitionComposite_Q09  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                                1.0     \n",
       "4                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q4-1  \\\n",
       "0                                          1.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q4-2  \\\n",
       "0                                          4.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          4.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q7-1  \\\n",
       "0                                          1.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q7-2  \\\n",
       "0                                          3.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          4.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q9-1  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q9-2  ...  \\\n",
       "0                                          NaN  ...   \n",
       "1                                          NaN  ...   \n",
       "2                                          NaN  ...   \n",
       "3                                          4.0  ...   \n",
       "4                                          NaN  ...   \n",
       "\n",
       "   distractor_Post_Recognition_Sum  fake_Post_Recognition_Count  \\\n",
       "0                             0.00                          2.0   \n",
       "1                             1.75                          4.0   \n",
       "2                             2.00                          4.0   \n",
       "3                             1.00                          4.0   \n",
       "4                             2.00                          4.0   \n",
       "\n",
       "   fake_Post_Recognition_Mean  fake_Post_Recognition_NormalizedMean  \\\n",
       "0                      0.3750                                0.3750   \n",
       "1                      0.8125                                0.8125   \n",
       "2                      1.0000                                1.0000   \n",
       "3                      0.6250                                0.6250   \n",
       "4                      1.0000                                1.0000   \n",
       "\n",
       "   fake_Post_Recognition_Sum  unseen_Post_Recognition_Count  \\\n",
       "0                       0.75                            1.0   \n",
       "1                       3.25                            1.0   \n",
       "2                       4.00                            1.0   \n",
       "3                       2.50                            1.0   \n",
       "4                       4.00                            1.0   \n",
       "\n",
       "   unseen_Post_Recognition_Mean  unseen_Post_Recognition_NormalizedMean  \\\n",
       "0                          0.00                                    0.00   \n",
       "1                          0.75                                    0.75   \n",
       "2                          1.00                                    1.00   \n",
       "3                          0.50                                    0.50   \n",
       "4                          0.00                                    0.00   \n",
       "\n",
       "   unseen_Post_Recognition_Sum  \\\n",
       "0                         0.00   \n",
       "1                         0.75   \n",
       "2                         1.00   \n",
       "3                         0.50   \n",
       "4                         0.00   \n",
       "\n",
       "                             post_survey_source_path  \n",
       "0  data/Post/Group C_Post Viewing Questionnaire P...  \n",
       "1  data/Post/Group B_ Post Viewing Questionnaire ...  \n",
       "2  data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "3  data/Post/Group C_ Post Viewing Questionnaire ...  \n",
       "4  data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "post_data_dir = project_root / \"data\" / \"Post\"\n",
    "if not post_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Post questionnaire directory not found at {post_data_dir}.\")\n",
    "post_files = sorted(\n",
    "    path\n",
    "    for path in post_data_dir.rglob(\"*.csv\")\n",
    "    if path.is_file() and not path.name.startswith(\"~$\")\n",
    ")\n",
    "if not post_files:\n",
    "    raise FileNotFoundError(f\"No post questionnaire CSV files found under {post_data_dir}.\")\n",
    "\n",
    "def merge_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _sanitize(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, str):\n",
    "            cleaned = value.strip()\n",
    "            return cleaned if cleaned else np.nan\n",
    "        return value\n",
    "    merged = {}\n",
    "    column_order = []\n",
    "    for column in df.columns:\n",
    "        normalized = re.sub(r\"\\s+\", \" \", str(column)).strip()\n",
    "        series = df[column].astype(\"object\").map(_sanitize)\n",
    "        if normalized not in merged:\n",
    "            merged[normalized] = series\n",
    "            column_order.append(normalized)\n",
    "        else:\n",
    "            merged[normalized] = merged[normalized].combine_first(series)\n",
    "    return pd.DataFrame({name: merged[name] for name in column_order})\n",
    "\n",
    "post_map_full = pd.read_csv(project_root / \"data\" / \"post_survey_map.csv\")\n",
    "if \"question_code\" not in post_map_full.columns:\n",
    "    post_map_full.insert(1, \"question_code\", post_map_full[\"question\"].apply(extract_question_code))\n",
    "else:\n",
    "    post_map_full[\"question_code\"] = post_map_full[\"question\"].apply(extract_question_code)\n",
    "post_map_full[\"subscale\"] = post_map_full[\"subscale\"].astype(str)\n",
    "\n",
    "CATEGORY_RENAME = {\n",
    "    \"key\": \"wb-key\",\n",
    "    \"seen\": \"wb-notKeySeen\",\n",
    "    \"unseen\": \"wb-notKeyUnseen\",\n",
    "    \"fake\": \"distractor\",\n",
    "    \"distractor\": \"comp-key\",\n",
    "    \"distractor2\": \"comp-notKeySeen\",\n",
    "}\n",
    "\n",
    "STAT_LABELS = {\n",
    "    \"count\": \"Count\",\n",
    "    \"sum\": \"Sum\",\n",
    "    \"mean\": \"Mean\",\n",
    "    \"normalized_mean\": \"NormalizedMean\",\n",
    "}\n",
    "RECOGNITION_BINARY_MAX = 2.0\n",
    "RECOGNITION_CONFIDENCE_MAX = 4.0\n",
    "RECOGNITION_COMPOSITE_MAX = RECOGNITION_BINARY_MAX * RECOGNITION_CONFIDENCE_MAX\n",
    "FORM_CATEGORY_KEYS = {\"key\", \"seen\"}\n",
    "NON_FORM_CATEGORY_KEYS = {\"unseen\", \"fake\", \"distractor\", \"distractor2\"}\n",
    "\n",
    "uv_stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not uv_stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 dataset not found at {uv_stage1_path}.\")\n",
    "uv_stage1 = pd.read_csv(uv_stage1_path)\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1[\"group\"] = uv_stage1[\"group\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "uv_stage2_path = project_root / \"results\" / \"uv_stage2.csv\"\n",
    "uv_stage2_cached = None\n",
    "if uv_stage2_path.exists():\n",
    "    uv_stage2_cached = pd.read_csv(uv_stage2_path)\n",
    "    uv_stage2_cached[\"respondent\"] = uv_stage2_cached[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage1_lookup_df = (\n",
    "    uv_stage1\n",
    "    .loc[:, [\"respondent\", \"group\", \"Short Form\", \"Long Form\"]]\n",
    "    .dropna(subset=[\"respondent\"])\n",
    "    .assign(respondent=lambda df: df[\"respondent\"].astype(str).str.strip())\n",
    "    .drop_duplicates(\"respondent\", keep=\"last\")\n",
    ")\n",
    "uv_stage1_lookup = uv_stage1_lookup_df.set_index(\"respondent\").to_dict(\"index\")\n",
    "\n",
    "recognition_map = post_map_full.loc[\n",
    "    post_map_full[\"subscale\"].str.lower() == \"recognition\",\n",
    "    :,\n",
    "]\n",
    "recognition_map = recognition_map.loc[recognition_map[\"question_code\"].notna()].copy()\n",
    "recognition_map[\"question_code\"] = recognition_map[\"question_code\"].astype(str).str.strip()\n",
    "recognition_map = recognition_map.loc[recognition_map[\"question_code\"].str.match(r\"^\\d+\\.[12]$\")]\n",
    "\n",
    "valid_question_codes = set(recognition_map[\"question_code\"].unique())\n",
    "\n",
    "group_columns = [col for col in recognition_map.columns if col.startswith(\"Group \")]\n",
    "\n",
    "TITLE_NORMALIZATION = {\n",
    "    \"abbott elementary\": \"Abbot Elementary\",\n",
    "    \"abbot elementary\": \"Abbot Elementary\",\n",
    "    \"schitts creek\": \"Schittss Creek\",\n",
    "    \"schittss creek\": \"Schittss Creek\",\n",
    "    \"mad max fury road\": \"Mad Max\",\n",
    "    \"mad max\": \"Mad Max\",\n",
    "}\n",
    "\n",
    "def canonicalize_title(raw_title: str) -> str:\n",
    "    if pd.isna(raw_title):\n",
    "        return \"\"\n",
    "    cleaned = str(raw_title).strip()\n",
    "    if not cleaned:\n",
    "        return \"\"\n",
    "    lookup_key = cleaned.lower()\n",
    "    return TITLE_NORMALIZATION.get(lookup_key, cleaned)\n",
    "\n",
    "respondent_exposures: dict[str, dict[str, set[str]]] = {}\n",
    "for respondent_id, info in uv_stage1_lookup.items():\n",
    "    exposures: dict[str, set[str]] = {}\n",
    "    long_title = canonicalize_title(info.get(\"Long Form\", \"\"))\n",
    "    if long_title:\n",
    "        exposures.setdefault(\"Long\", set()).add(long_title)\n",
    "    short_title = canonicalize_title(info.get(\"Short Form\", \"\"))\n",
    "    if short_title:\n",
    "        exposures.setdefault(\"Short\", set()).add(short_title)\n",
    "    if exposures:\n",
    "        respondent_exposures[str(respondent_id)] = exposures\n",
    "\n",
    "meta_lookup = {}\n",
    "for _, row in recognition_map.iterrows():\n",
    "    q_code = row[\"question_code\"]\n",
    "    q_root, q_suffix = q_code.split(\".\")\n",
    "    category = str(row.get(\"category\", \"\")).strip()\n",
    "    accuracy = str(row.get(\"accuracy\", \"\")).strip().lower()\n",
    "    for group_col in group_columns:\n",
    "        group_letter = group_col.replace(\"Group \", \"\").strip().upper()\n",
    "        title_value = canonicalize_title(row.get(group_col, \"\"))\n",
    "        meta_lookup[(group_letter, q_root, q_suffix)] = {\n",
    "            \"title\": title_value,\n",
    "            \"category\": category,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"([A-F])\", expand=False)\n",
    "stimulus_map[\"title_clean\"] = stimulus_map[\"title\"].map(canonicalize_title)\n",
    "stimulus_map[\"form_clean\"] = stimulus_map[\"form\"].astype(str).str.title()\n",
    "\n",
    "group_title_form_lookup = {\n",
    "    (row.group_letter, row.title_clean): row.form_clean\n",
    "    for row in stimulus_map.itertuples()\n",
    "    if isinstance(row.group_letter, str) and isinstance(row.title_clean, str) and row.title_clean\n",
    "}\n",
    "\n",
    "if uv_stage2_cached is not None:\n",
    "    uv_columns = pd.Index(uv_stage2_cached.columns)\n",
    "else:\n",
    "    uv_columns = pd.Index(uv_stage1.columns)\n",
    "\n",
    "yes_values = {\"yes\", \"y\", \"true\", \"1\"}\n",
    "no_values = {\"no\", \"n\", \"false\", \"0\"}\n",
    "\n",
    "confidence_pattern = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\")\n",
    "\n",
    "def extract_scalar(value):\n",
    "    if isinstance(value, pd.Series):\n",
    "        non_null = value.dropna()\n",
    "        if non_null.empty:\n",
    "            return np.nan\n",
    "        return non_null.iloc[0]\n",
    "    return value\n",
    "\n",
    "def parse_yes_no(value) -> float:\n",
    "    value = extract_scalar(value)\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip().lower()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    if text in yes_values:\n",
    "        return 1.0\n",
    "    if text in no_values:\n",
    "        return 0.0\n",
    "    return np.nan\n",
    "\n",
    "def parse_confidence(value) -> float:\n",
    "    value = extract_scalar(value)\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    match = confidence_pattern.match(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def resolve_form(respondent_id: str, group_letter: str, title: str) -> tuple[str, list[str]]:\n",
    "    notes: list[str] = []\n",
    "    exposures = uv_stage1_lookup.get(respondent_id, {})\n",
    "    long_title = canonicalize_title(exposures.get(\"Long Form\", \"\")) if exposures else \"\"\n",
    "    short_title = canonicalize_title(exposures.get(\"Short Form\", \"\")) if exposures else \"\"\n",
    "    canonical = canonicalize_title(title)\n",
    "    if canonical and canonical == long_title:\n",
    "        return \"Long\", notes\n",
    "    if canonical and canonical == short_title:\n",
    "        return \"Short\", notes\n",
    "    group_clean = (group_letter or \"\").strip().upper()\n",
    "    if canonical:\n",
    "        mapped = group_title_form_lookup.get((group_clean, canonical))\n",
    "        if mapped:\n",
    "            notes.append(\"form_from_group_stimulus_map\")\n",
    "            return mapped, notes\n",
    "        has_short = any(col.startswith(f\"Short_{canonical}\") for col in uv_columns)\n",
    "        has_long = any(col.startswith(f\"Long_{canonical}\") for col in uv_columns)\n",
    "        if has_short and not has_long:\n",
    "            notes.append(\"form_inferred_short_from_uv\")\n",
    "            return \"Short\", notes\n",
    "        if has_long and not has_short:\n",
    "            notes.append(\"form_inferred_long_from_uv\")\n",
    "            return \"Long\", notes\n",
    "        if has_short and has_long:\n",
    "            notes.append(\"form_ambiguous_default_short\")\n",
    "            return \"Short\", notes\n",
    "    notes.append(\"form_unresolved_default_short\")\n",
    "    return \"Short\", notes\n",
    "\n",
    "recognition_records: list[dict] = []\n",
    "issue_records: list[dict] = []\n",
    "respondent_post_paths: dict[str, str] = {}\n",
    "excluded_group_mismatch = set()\n",
    "\n",
    "for csv_path in post_files:\n",
    "    file_group_match = re.search(r\"Group\\s+([A-F])\", csv_path.stem, re.IGNORECASE)\n",
    "    fallback_group_letter = file_group_match.group(1).upper() if file_group_match else \"\"\n",
    "    df_raw = pd.read_csv(csv_path, dtype=str)\n",
    "    df_merged = merge_duplicate_columns(df_raw)\n",
    "    rename_map = {}\n",
    "    for column in df_merged.columns:\n",
    "        q_code = extract_question_code(column)\n",
    "        if q_code and q_code in valid_question_codes:\n",
    "            rename_map[column] = q_code\n",
    "    df_standard = df_merged.rename(columns=rename_map)\n",
    "    respondent_col = next(((\n",
    "        col for col in df_standard.columns\n",
    "        if \"participant number\" in col.lower()\n",
    "    )), None)\n",
    "    if respondent_col is None:\n",
    "        raise KeyError(f\"Participant identifier column not found in {csv_path.name}\")\n",
    "    df_standard[\"respondent\"] = df_standard[respondent_col].astype(str).str.strip()\n",
    "    df_standard[\"respondent\"] = df_standard[\"respondent\"].replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    df_standard[\"respondent\"] = df_standard[\"respondent\"].str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if \"Timestamp\" in df_standard.columns:\n",
    "        df_standard[\"timestamp_iso\"] = pd.to_datetime(df_standard[\"Timestamp\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df_standard[\"timestamp_iso\"] = pd.NaT\n",
    "\n",
    "    for _, row in df_standard.iterrows():\n",
    "        respondent_id = row.get(\"respondent\")\n",
    "        if pd.isna(respondent_id):\n",
    "            continue\n",
    "        respondent_id = str(respondent_id).strip()\n",
    "        if not respondent_id:\n",
    "            continue\n",
    "        respondent_info = uv_stage1_lookup.get(respondent_id, {})\n",
    "        stage1_group = str(respondent_info.get(\"group\", \"\")).strip().upper()\n",
    "        post_group_letter = fallback_group_letter\n",
    "        if stage1_group and post_group_letter and stage1_group != post_group_letter:\n",
    "            if respondent_id not in excluded_group_mismatch:\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": stage1_group,\n",
    "                    \"question_number\": np.nan,\n",
    "                    \"title\": \"\",\n",
    "                    \"issue\": f\"post_group_mismatch_uv_{stage1_group}_file_{post_group_letter}\",\n",
    "                    \"source_file\": csv_path.name,\n",
    "                })\n",
    "            excluded_group_mismatch.add(respondent_id)\n",
    "            continue\n",
    "        group_letter = stage1_group or post_group_letter\n",
    "        if not group_letter:\n",
    "            issue_records.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"group\": \"\",\n",
    "                \"question_number\": np.nan,\n",
    "                \"title\": \"\",\n",
    "                \"issue\": \"group_missing_in_stage1_and_filename\",\n",
    "                \"source_file\": csv_path.name,\n",
    "            })\n",
    "            continue\n",
    "        if respondent_id not in respondent_post_paths:\n",
    "            try:\n",
    "                relative_path = csv_path.relative_to(project_root)\n",
    "                respondent_post_paths[respondent_id] = relative_path.as_posix()\n",
    "            except ValueError:\n",
    "                respondent_post_paths[respondent_id] = csv_path.as_posix()\n",
    "        for q_num in map(str, range(1, 13)):\n",
    "            base_meta = meta_lookup.get((group_letter, q_num, \"1\"))\n",
    "            conf_meta = meta_lookup.get((group_letter, q_num, \"2\"))\n",
    "            if base_meta is None or conf_meta is None:\n",
    "                continue\n",
    "            binary_value = row.get(f\"{q_num}.1\")\n",
    "            confidence_value = row.get(f\"{q_num}.2\")\n",
    "            yes_no = parse_yes_no(binary_value)\n",
    "            confidence = parse_confidence(confidence_value)\n",
    "            issues_here: list[str] = []\n",
    "            if np.isnan(yes_no):\n",
    "                issues_here.append(\"binary_response_missing_or_unrecognized\")\n",
    "            accuracy = base_meta.get(\"accuracy\", \"\")\n",
    "            expected_yes = accuracy == \"hit\"\n",
    "            base_score_raw = np.nan\n",
    "            base_score = np.nan\n",
    "            if not np.isnan(yes_no):\n",
    "                answered_yes = bool(yes_no)\n",
    "                base_score_raw = RECOGNITION_BINARY_MAX if answered_yes == expected_yes else 0.0\n",
    "                base_score = base_score_raw / RECOGNITION_BINARY_MAX\n",
    "            if np.isnan(confidence):\n",
    "                issues_here.append(\"confidence_missing_or_unrecognized\")\n",
    "            composite_raw = np.nan\n",
    "            composite = np.nan\n",
    "            if not np.isnan(base_score_raw) and not np.isnan(confidence):\n",
    "                composite_raw = base_score_raw * confidence\n",
    "                composite = composite_raw / RECOGNITION_COMPOSITE_MAX\n",
    "            form_value, form_notes = resolve_form(respondent_id, group_letter, base_meta.get(\"title\", \"\"))\n",
    "            issues_here.extend(form_notes)\n",
    "            question_int = int(float(q_num))\n",
    "            title_segment = base_meta.get(\"title\", \"\").strip() or f\"Question {question_int}\"\n",
    "            composite_name = f\"{form_value}_{title_segment}_Post_RecognitionComposite_Q{question_int:02d}\"\n",
    "            base_name = f\"{form_value}_{title_segment}_Post_Recognition_Q{question_int}-1\"\n",
    "            confidence_name = f\"{form_value}_{title_segment}_Post_Recognition_Q{question_int}-2\"\n",
    "            records_to_add = [\n",
    "                (\"composite\", composite_name, composite),\n",
    "                (\"binary\", base_name, base_score),\n",
    "                (\"confidence\", confidence_name, confidence),\n",
    "            ]\n",
    "            for metric_label, feature_name, feature_value in records_to_add:\n",
    "                recognition_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": group_letter,\n",
    "                    \"question_number\": question_int,\n",
    "                    \"title\": base_meta.get(\"title\", \"\"),\n",
    "                    \"category\": base_meta.get(\"category\", \"\"),\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"form\": form_value,\n",
    "                    \"metric\": metric_label,\n",
    "                    \"column_name\": feature_name,\n",
    "                    \"value\": feature_value,\n",
    "                    \"timestamp\": row.get(\"timestamp_iso\", pd.NaT),\n",
    "                })\n",
    "            for issue in issues_here:\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": group_letter,\n",
    "                    \"question_number\": question_int,\n",
    "                    \"title\": base_meta.get(\"title\", \"\"),\n",
    "                    \"issue\": issue,\n",
    "                    \"source_file\": csv_path.name,\n",
    "                })\n",
    "\n",
    "recognition_df = pd.DataFrame(recognition_records)\n",
    "if recognition_df.empty:\n",
    "    raise ValueError(\"No recognition responses were parsed from post questionnaire files.\")\n",
    "recognition_df = recognition_df.sort_values([\"respondent\", \"column_name\", \"timestamp\"])\n",
    "recognition_df = recognition_df.drop_duplicates([\"respondent\", \"column_name\"], keep=\"last\")\n",
    "\n",
    "recognition_features = (\n",
    "    recognition_df\n",
    "    .pivot(index=\"respondent\", columns=\"column_name\", values=\"value\")\n",
    "    .sort_index(axis=1)\n",
    "    .reset_index()\n",
    ")\n",
    "recognition_features.columns.name = None\n",
    "\n",
    "composite_subset = recognition_df.loc[recognition_df[\"metric\"] == \"composite\"].copy()\n",
    "if not composite_subset.empty:\n",
    "    composite_subset[\"category_lower\"] = (\n",
    "        composite_subset[\"category\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace(\"\", np.nan)\n",
    "    )\n",
    "    composite_subset = composite_subset.loc[composite_subset[\"category_lower\"].notna()]\n",
    "    composite_subset[\"title_clean\"] = composite_subset[\"title\"].map(canonicalize_title)\n",
    "    composite_subset[\"respondent\"] = composite_subset[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "    form_subset = composite_subset.loc[composite_subset[\"category_lower\"].isin(FORM_CATEGORY_KEYS)].copy()\n",
    "    if not form_subset.empty:\n",
    "        def _match_form(row) -> str | None:\n",
    "            exposures = respondent_exposures.get(row[\"respondent\"], {})\n",
    "            for form_label, titles in exposures.items():\n",
    "                if row[\"title_clean\"] in titles:\n",
    "                    return form_label\n",
    "            return None\n",
    "\n",
    "        form_subset[\"aligned_form\"] = form_subset.apply(_match_form, axis=1)\n",
    "        unmatched_mask = form_subset[\"aligned_form\"].isna()\n",
    "        if unmatched_mask.any():\n",
    "            for row in form_subset.loc[unmatched_mask, [\"respondent\", \"question_number\", \"title\", \"category_lower\"]].itertuples(index=False):\n",
    "                respondent_key = str(row.respondent)\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_key,\n",
    "                    \"group\": uv_stage1_lookup.get(respondent_key, {}).get(\"group\", \"\"),\n",
    "                    \"question_number\": int(row.question_number) if not pd.isna(row.question_number) else np.nan,\n",
    "                    \"title\": row.title,\n",
    "                    \"issue\": \"post_form_title_not_in_stage1_exposures\",\n",
    "                    \"source_file\": respondent_post_paths.get(respondent_key, \"\"),\n",
    "                })\n",
    "            form_subset = form_subset.loc[~unmatched_mask]\n",
    "        if not form_subset.empty:\n",
    "            form_agg = (\n",
    "                form_subset\n",
    "                .groupby([\"respondent\", \"aligned_form\", \"category_lower\"])[\"value\"]\n",
    "                .agg(count=\"count\", sum=\"sum\", mean=\"mean\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            if not form_agg.empty:\n",
    "                form_agg[\"normalized_mean\"] = form_agg[\"mean\"].astype(float).clip(0, 1)\n",
    "                form_long = form_agg.melt(\n",
    "                    id_vars=[\"respondent\", \"aligned_form\", \"category_lower\"],\n",
    "                    value_vars=[\"count\", \"sum\", \"mean\", \"normalized_mean\"],\n",
    "                    var_name=\"statistic\",\n",
    "                    value_name=\"stat_value\",\n",
    "                )\n",
    "                form_long[\"column_name\"] = form_long.apply(\n",
    "                    lambda r: f\"{r['aligned_form']}_{r['category_lower']}_Post_Recognition_{STAT_LABELS[r['statistic']]}\"\n",
    "                    , axis=1,\n",
    "                )\n",
    "                form_wide = form_long.pivot(index=\"respondent\", columns=\"column_name\", values=\"stat_value\").reset_index()\n",
    "                form_wide.columns.name = None\n",
    "                recognition_features = recognition_features.merge(form_wide, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    non_form_subset = composite_subset.loc[composite_subset[\"category_lower\"].isin(NON_FORM_CATEGORY_KEYS)].copy()\n",
    "    if not non_form_subset.empty:\n",
    "        non_form_agg = (\n",
    "            non_form_subset\n",
    "            .groupby([\"respondent\", \"category_lower\"])[\"value\"]\n",
    "            .agg(count=\"count\", sum=\"sum\", mean=\"mean\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        if not non_form_agg.empty:\n",
    "            non_form_agg[\"normalized_mean\"] = non_form_agg[\"mean\"].astype(float).clip(0, 1)\n",
    "            non_form_long = non_form_agg.melt(\n",
    "                id_vars=[\"respondent\", \"category_lower\"],\n",
    "                value_vars=[\"count\", \"sum\", \"mean\", \"normalized_mean\"],\n",
    "                var_name=\"statistic\",\n",
    "                value_name=\"stat_value\",\n",
    "            )\n",
    "            non_form_long[\"column_name\"] = non_form_long.apply(\n",
    "                lambda r: f\"{r['category_lower']}_Post_Recognition_{STAT_LABELS[r['statistic']]}\"\n",
    "                , axis=1,\n",
    "            )\n",
    "            non_form_wide = non_form_long.pivot(index=\"respondent\", columns=\"column_name\", values=\"stat_value\").reset_index()\n",
    "            non_form_wide.columns.name = None\n",
    "            recognition_features = recognition_features.merge(non_form_wide, on=\"respondent\", how=\"left\")\n",
    "\n",
    "if respondent_post_paths:\n",
    "    post_path_df = (\n",
    "        pd.Series(respondent_post_paths, name=\"post_survey_source_path\")\n",
    "        .to_frame()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"respondent\"})\n",
    "    )\n",
    "    recognition_features = recognition_features.merge(post_path_df, on=\"respondent\", how=\"left\")\n",
    "\n",
    "feature_columns = [col for col in recognition_features.columns if col != \"respondent\"]\n",
    "composite_columns = [col for col in feature_columns if \"_Post_RecognitionComposite_\" in col]\n",
    "raw_columns = [\n",
    "    col\n",
    "    for col in feature_columns\n",
    "    if \"_Post_Recognition_\" in col and \"Composite\" not in col and not col.endswith(\"source_path\")\n",
    "]\n",
    "\n",
    "issues_df = pd.DataFrame(issue_records)\n",
    "if not issues_df.empty:\n",
    "    issues_df = issues_df.sort_values([\"respondent\", \"question_number\", \"issue\"])\n",
    "\n",
    "if excluded_group_mismatch:\n",
    "    def _mismatch_sort_key(value: str):\n",
    "        text = str(value)\n",
    "        if text.isdigit():\n",
    "            return (0, int(text))\n",
    "        return (1, text)\n",
    "    excluded_display = \", \".join(sorted(excluded_group_mismatch, key=_mismatch_sort_key))\n",
    "    print(\n",
    "        f\"Skipped {len(excluded_group_mismatch)} respondent(s) due to post group mismatch: {excluded_display}.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Parsed {recognition_df.shape[0]} recognition feature rows across \"\n",
    "    f\"{len(composite_columns)} composite columns and {len(raw_columns)} raw response columns \"\n",
    "    f\"for {recognition_features.shape[0]} respondents.\",\n",
    ")\n",
    "sample_columns = composite_columns[:3] + raw_columns[:3]\n",
    "if sample_columns:\n",
    "    print(\"Sample feature columns:\")\n",
    "    for name in sample_columns:\n",
    "        print(f\"  {name}\")\n",
    "recognition_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df111fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recognition feature columns: 150\n",
      "Composite columns (39 total):\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q07\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q01\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q04\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q07\n",
      "  Long_The Town_Post_RecognitionComposite_Q04\n",
      "  Long_The Town_Post_RecognitionComposite_Q07\n",
      "  Long_The Town_Post_RecognitionComposite_Q09\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q10\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q11\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q12\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q01\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q05\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Short_Friends_Post_RecognitionComposite_Q06\n",
      "  Short_Friends_Post_RecognitionComposite_Q08\n",
      "  Short_Goodfellas_Post_RecognitionComposite_Q02\n",
      "  Short_I am Legend_Post_RecognitionComposite_Q06\n",
      "  ...\n",
      "Raw recognition columns (110 total):\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q9-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q9-2\n",
      "  Long_Mad Max_Post_Recognition_Q1-1\n",
      "  Long_Mad Max_Post_Recognition_Q1-2\n",
      "  Long_Mad Max_Post_Recognition_Q4-1\n",
      "  Long_Mad Max_Post_Recognition_Q4-2\n",
      "  Long_Mad Max_Post_Recognition_Q7-1\n",
      "  Long_Mad Max_Post_Recognition_Q7-2\n",
      "  Long_The Town_Post_Recognition_Q4-1\n",
      "  Long_The Town_Post_Recognition_Q4-2\n",
      "  Long_The Town_Post_Recognition_Q7-1\n",
      "  Long_The Town_Post_Recognition_Q7-2\n",
      "  Long_The Town_Post_Recognition_Q9-1\n",
      "  Long_The Town_Post_Recognition_Q9-2\n",
      "  Short_A Star Is Born_Post_Recognition_Q10-1\n",
      "  Short_A Star Is Born_Post_Recognition_Q10-2\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [col for col in recognition_features.columns if col != \"respondent\"]\n",
    "composite_columns = [col for col in feature_columns if \"_Post_RecognitionComposite_\" in col]\n",
    "raw_columns = [col for col in feature_columns if \"_Post_Recognition_\" in col and \"Composite\" not in col]\n",
    "\n",
    "print(f\"Total recognition feature columns: {len(feature_columns)}\")\n",
    "print(f\"Composite columns ({len(composite_columns)} total):\")\n",
    "for name in composite_columns[:20]:\n",
    "    print(f\"  {name}\")\n",
    "if len(composite_columns) > 20:\n",
    "    print(\"  ...\")\n",
    "print(f\"Raw recognition columns ({len(raw_columns)} total):\")\n",
    "for name in raw_columns[:20]:\n",
    "    print(f\"  {name}\")\n",
    "if len(raw_columns) > 20:\n",
    "    print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ae4c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 UV shape: (83, 167); saved to uv_stage3.csv.\n",
      "Logged 599 recognition parsing issues to uv_stage3_issues.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>distractor_Post_Recognition_Sum</th>\n",
       "      <th>fake_Post_Recognition_Count</th>\n",
       "      <th>fake_Post_Recognition_Mean</th>\n",
       "      <th>fake_Post_Recognition_NormalizedMean</th>\n",
       "      <th>fake_Post_Recognition_Sum</th>\n",
       "      <th>unseen_Post_Recognition_Count</th>\n",
       "      <th>unseen_Post_Recognition_Mean</th>\n",
       "      <th>unseen_Post_Recognition_NormalizedMean</th>\n",
       "      <th>unseen_Post_Recognition_Sum</th>\n",
       "      <th>post_survey_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008_1.csv</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>10/07/2025</td>\n",
       "      <td>10:32:16</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005_10.csv</td>\n",
       "      <td>B</td>\n",
       "      <td>10</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>13:17:14</td>\n",
       "      <td>65</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>data/Post/Group B_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005_100.csv</td>\n",
       "      <td>E</td>\n",
       "      <td>100</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>19:14:06</td>\n",
       "      <td>25</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003_101.csv</td>\n",
       "      <td>C</td>\n",
       "      <td>101</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>14:58:27</td>\n",
       "      <td>67</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group C_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001_102.csv</td>\n",
       "      <td>D</td>\n",
       "      <td>102</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>16:35:40</td>\n",
       "      <td>37</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>004_94.csv</td>\n",
       "      <td>D</td>\n",
       "      <td>94</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>11:33:19</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group F_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>004_97.csv</td>\n",
       "      <td>E</td>\n",
       "      <td>97</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>17:41:01</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>004_98.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>98</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>17:41:49</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>003_99.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>19:09:48</td>\n",
       "      <td>25</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0     008_1.csv     C          1  10/07/2025   10:32:16   63     60-69   \n",
       "1    005_10.csv     B         10  10/10/2025   13:17:14   65     60-69   \n",
       "2   005_100.csv     E        100  10/15/2025   19:14:06   25     18-27   \n",
       "3   003_101.csv     C        101  10/16/2025   14:58:27   67     60-69   \n",
       "4   001_102.csv     D        102  10/16/2025   16:35:40   37     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   004_94.csv     D         94  10/15/2025   11:33:19   32     28-43   \n",
       "79   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "80   004_97.csv     E         97  10/15/2025   17:41:01   32     28-43   \n",
       "81   004_98.csv     A         98  10/15/2025   17:41:49   32     28-43   \n",
       "82   003_99.csv     A         99  10/15/2025   19:09:48   25     18-27   \n",
       "\n",
       "    gender                      ethnicity                income_group  ...  \\\n",
       "0     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year  ...   \n",
       "1     Male                          White  $35,000  $60,000 per year  ...   \n",
       "2   Female                          White  $35,000  $60,000 per year  ...   \n",
       "3     Male         Black/African American    $60,000 or more per year  ...   \n",
       "4     Male         Black/African American  $35,000  $60,000 per year  ...   \n",
       "..     ...                            ...                         ...  ...   \n",
       "78    Male                          White  $35,000  $60,000 per year  ...   \n",
       "79  Female                          White    $60,000 or more per year  ...   \n",
       "80  Female                          White    $60,000 or more per year  ...   \n",
       "81    Male                          White  $35,000  $60,000 per year  ...   \n",
       "82  Female                          White    $60,000 or more per year  ...   \n",
       "\n",
       "   distractor_Post_Recognition_Sum  fake_Post_Recognition_Count  \\\n",
       "0                             0.00                          2.0   \n",
       "1                             1.75                          4.0   \n",
       "2                             2.00                          4.0   \n",
       "3                             1.00                          4.0   \n",
       "4                             2.00                          4.0   \n",
       "..                             ...                          ...   \n",
       "78                            2.00                          4.0   \n",
       "79                            2.00                          4.0   \n",
       "80                            2.00                          4.0   \n",
       "81                            2.00                          4.0   \n",
       "82                            2.00                          4.0   \n",
       "\n",
       "    fake_Post_Recognition_Mean  fake_Post_Recognition_NormalizedMean  \\\n",
       "0                       0.3750                                0.3750   \n",
       "1                       0.8125                                0.8125   \n",
       "2                       1.0000                                1.0000   \n",
       "3                       0.6250                                0.6250   \n",
       "4                       1.0000                                1.0000   \n",
       "..                         ...                                   ...   \n",
       "78                      1.0000                                1.0000   \n",
       "79                      1.0000                                1.0000   \n",
       "80                      1.0000                                1.0000   \n",
       "81                      0.8750                                0.8750   \n",
       "82                      1.0000                                1.0000   \n",
       "\n",
       "   fake_Post_Recognition_Sum unseen_Post_Recognition_Count  \\\n",
       "0                       0.75                           1.0   \n",
       "1                       3.25                           1.0   \n",
       "2                       4.00                           1.0   \n",
       "3                       2.50                           1.0   \n",
       "4                       4.00                           1.0   \n",
       "..                       ...                           ...   \n",
       "78                      4.00                           1.0   \n",
       "79                      4.00                           1.0   \n",
       "80                      4.00                           1.0   \n",
       "81                      3.50                           1.0   \n",
       "82                      4.00                           1.0   \n",
       "\n",
       "   unseen_Post_Recognition_Mean  unseen_Post_Recognition_NormalizedMean  \\\n",
       "0                          0.00                                    0.00   \n",
       "1                          0.75                                    0.75   \n",
       "2                          1.00                                    1.00   \n",
       "3                          0.50                                    0.50   \n",
       "4                          0.00                                    0.00   \n",
       "..                          ...                                     ...   \n",
       "78                         0.00                                    0.00   \n",
       "79                         1.00                                    1.00   \n",
       "80                         0.00                                    0.00   \n",
       "81                         0.50                                    0.50   \n",
       "82                         1.00                                    1.00   \n",
       "\n",
       "    unseen_Post_Recognition_Sum  \\\n",
       "0                          0.00   \n",
       "1                          0.75   \n",
       "2                          1.00   \n",
       "3                          0.50   \n",
       "4                          0.00   \n",
       "..                          ...   \n",
       "78                         0.00   \n",
       "79                         1.00   \n",
       "80                         0.00   \n",
       "81                         0.50   \n",
       "82                         1.00   \n",
       "\n",
       "                              post_survey_source_path  \n",
       "0   data/Post/Group C_Post Viewing Questionnaire P...  \n",
       "1   data/Post/Group B_ Post Viewing Questionnaire ...  \n",
       "2   data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "3   data/Post/Group C_ Post Viewing Questionnaire ...  \n",
       "4   data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "..                                                ...  \n",
       "78  data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "79  data/Post/Group F_ Post Viewing Questionnaire ...  \n",
       "80  data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "81  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "82  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "\n",
       "[83 rows x 167 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "form_columns = [col for col in (\"Short Form\", \"Long Form\") if col in uv_stage1_base.columns]\n",
    "forms_lookup = uv_stage1_base[[\"respondent\", *form_columns]].copy() if form_columns else None\n",
    "if forms_lookup is not None:\n",
    "    forms_lookup[\"respondent\"] = forms_lookup[\"respondent\"].astype(str).str.strip()\n",
    "    if forms_lookup[\"respondent\"].duplicated().any():\n",
    "        dup_count = forms_lookup[\"respondent\"].duplicated(keep=False).sum()\n",
    "        print(f\"Warning: {dup_count} duplicate form records detected; using the last occurrence per respondent.\")\n",
    "        forms_lookup = forms_lookup.drop_duplicates(subset=\"respondent\", keep=\"last\")\n",
    "\n",
    "recognition_features = recognition_features.copy()\n",
    "recognition_features[\"respondent\"] = recognition_features[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage3 = uv_stage1_base.copy()\n",
    "if forms_lookup is not None:\n",
    "    uv_stage3 = uv_stage3.drop(columns=form_columns, errors=\"ignore\")\n",
    "    uv_stage3 = uv_stage3.merge(forms_lookup, on=\"respondent\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "uv_stage3 = uv_stage3.merge(recognition_features, on=\"respondent\", how=\"left\")\n",
    "uv_stage3 = uv_stage3.sort_values(\"respondent\").reset_index(drop=True)\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "stage3_path = results_dir / \"uv_stage3.csv\"\n",
    "uv_stage3.to_csv(stage3_path, index=False)\n",
    "print(f\"Stage 3 UV shape: {uv_stage3.shape}; saved to {stage3_path.name}.\")\n",
    "\n",
    "issues_path = results_dir / \"uv_stage3_issues.csv\"\n",
    "if issues_df.empty:\n",
    "    if issues_path.exists():\n",
    "        issues_path.unlink()\n",
    "    print(\"No Stage 3 recognition parsing issues detected.\")\n",
    "else:\n",
    "    issues_df.to_csv(issues_path, index=False)\n",
    "    print(f\"Logged {issues_df.shape[0]} recognition parsing issues to {issues_path.name}.\")\n",
    "\n",
    "uv_stage3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a67da1",
   "metadata": {},
   "source": [
    "## UV Merge\n",
    "This consolidation step reloads the Stage 2 and Stage 3 outputs, checks their respondent-level metadata against the Stage 1 base, and publishes a merged UV file plus an issues log highlighting any discrepancies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81e3e759",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UV merge completed with no issues detected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>distractor_Post_Recognition_Sum</th>\n",
       "      <th>fake_Post_Recognition_Count</th>\n",
       "      <th>fake_Post_Recognition_Mean</th>\n",
       "      <th>fake_Post_Recognition_NormalizedMean</th>\n",
       "      <th>fake_Post_Recognition_Sum</th>\n",
       "      <th>unseen_Post_Recognition_Count</th>\n",
       "      <th>unseen_Post_Recognition_Mean</th>\n",
       "      <th>unseen_Post_Recognition_NormalizedMean</th>\n",
       "      <th>unseen_Post_Recognition_Sum</th>\n",
       "      <th>post_survey_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_file group respondent  date_study time_study  age age_group  gender  \\\n",
       "0  003_104.csv     A        104  10/16/2025   18:09:03   59     44-59    Male   \n",
       "1  002_106.csv     A        106  10/16/2025   19:35:05   30     28-43    Male   \n",
       "2  001_116.csv     A        116  10/18/2025   12:37:40   19     18-27    Male   \n",
       "3   006_14.csv     A         14  10/11/2025   09:32:42   33     28-43    Male   \n",
       "4    007_3.csv     A          3  10/10/2025   09:19:22   34     28-43  Female   \n",
       "\n",
       "                       ethnicity                income_group  ...  \\\n",
       "0                          White    $60,000 or more per year  ...   \n",
       "1                          White    $60,000 or more per year  ...   \n",
       "2                          White  $35,000  $60,000 per year  ...   \n",
       "3  Hispanic/Latino/Latina/Latinx    $60,000 or more per year  ...   \n",
       "4                          White    $60,000 or more per year  ...   \n",
       "\n",
       "  distractor_Post_Recognition_Sum  fake_Post_Recognition_Count  \\\n",
       "0                             1.0                          4.0   \n",
       "1                             2.0                          4.0   \n",
       "2                             NaN                          NaN   \n",
       "3                             1.0                          4.0   \n",
       "4                             2.0                          4.0   \n",
       "\n",
       "   fake_Post_Recognition_Mean  fake_Post_Recognition_NormalizedMean  \\\n",
       "0                      0.6875                                0.6875   \n",
       "1                      1.0000                                1.0000   \n",
       "2                         NaN                                   NaN   \n",
       "3                      0.8125                                0.8125   \n",
       "4                      1.0000                                1.0000   \n",
       "\n",
       "  fake_Post_Recognition_Sum unseen_Post_Recognition_Count  \\\n",
       "0                      2.75                           1.0   \n",
       "1                      4.00                           1.0   \n",
       "2                       NaN                           NaN   \n",
       "3                      3.25                           1.0   \n",
       "4                      4.00                           1.0   \n",
       "\n",
       "  unseen_Post_Recognition_Mean unseen_Post_Recognition_NormalizedMean  \\\n",
       "0                         0.00                                   0.00   \n",
       "1                         0.75                                   0.75   \n",
       "2                          NaN                                    NaN   \n",
       "3                         1.00                                   1.00   \n",
       "4                         1.00                                   1.00   \n",
       "\n",
       "  unseen_Post_Recognition_Sum  \\\n",
       "0                        0.00   \n",
       "1                        0.75   \n",
       "2                         NaN   \n",
       "3                        1.00   \n",
       "4                        1.00   \n",
       "\n",
       "                             post_survey_source_path  \n",
       "0  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "1  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "2                                                NaN  \n",
       "3  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "4  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "\n",
       "[5 rows x 523 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "stage1_path = results_dir / \"uv_stage1.csv\"\n",
    "stage2_path = results_dir / \"uv_stage2.csv\"\n",
    "stage3_path = results_dir / \"uv_stage3.csv\"\n",
    "\n",
    "for required_path in [stage1_path, stage2_path, stage3_path]:\n",
    "    if not required_path.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {required_path}\")\n",
    "\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage2 = pd.read_csv(stage2_path)\n",
    "uv_stage3 = pd.read_csv(stage3_path)\n",
    "\n",
    "for df in (uv_stage1_base, uv_stage2, uv_stage3):\n",
    "    df[\"respondent\"] = df[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "baseline_cols = list(uv_stage1_base.columns)\n",
    "baseline_set = set(baseline_cols)\n",
    "\n",
    "issues_records: list[dict] = []\n",
    "\n",
    "def log_issue(respondent: str, issue: str) -> None:\n",
    "    issues_records.append({\"respondent\": respondent, \"issue\": issue})\n",
    "\n",
    "for dataset_name, df in ((\"stage2\", uv_stage2), (\"stage3\", uv_stage3)):\n",
    "    duplicated = df[df[\"respondent\"].duplicated(keep=False)][\"respondent\"].unique()\n",
    "    for respondent in sorted(duplicated):\n",
    "        log_issue(respondent, f\"duplicate_in_{dataset_name}\")\n",
    "\n",
    "ids_stage1 = set(uv_stage1_base[\"respondent\"])\n",
    "ids_stage2 = set(uv_stage2[\"respondent\"])\n",
    "ids_stage3 = set(uv_stage3[\"respondent\"])\n",
    "\n",
    "for respondent in sorted(ids_stage1 - ids_stage2):\n",
    "    log_issue(respondent, \"missing_in_stage2\")\n",
    "for respondent in sorted(ids_stage1 - ids_stage3):\n",
    "    log_issue(respondent, \"missing_in_stage3\")\n",
    "\n",
    "shared_ids = sorted(ids_stage2 & ids_stage3)\n",
    "stage2_baseline = uv_stage2.set_index(\"respondent\")\n",
    "stage3_baseline = uv_stage3.set_index(\"respondent\")\n",
    "baseline_columns = sorted((baseline_set - {\"respondent\"}) & set(stage2_baseline.columns) & set(stage3_baseline.columns))\n",
    "\n",
    "for respondent in shared_ids:\n",
    "    if respondent not in stage2_baseline.index or respondent not in stage3_baseline.index:\n",
    "        continue\n",
    "    if not baseline_columns:\n",
    "        break\n",
    "    row_stage2 = stage2_baseline.loc[respondent, baseline_columns]\n",
    "    row_stage3 = stage3_baseline.loc[respondent, baseline_columns]\n",
    "    for column in baseline_columns:\n",
    "        value_stage2 = row_stage2[column]\n",
    "        value_stage3 = row_stage3[column]\n",
    "        if pd.isna(value_stage2) and pd.isna(value_stage3):\n",
    "            continue\n",
    "        if pd.isna(value_stage2) != pd.isna(value_stage3) or str(value_stage2) != str(value_stage3):\n",
    "            log_issue(respondent, f\"baseline_mismatch_{column}\")\n",
    "\n",
    "stage2_feature_cols = [col for col in uv_stage2.columns if col not in baseline_set]\n",
    "stage3_feature_cols = [col for col in uv_stage3.columns if col not in baseline_set]\n",
    "\n",
    "uv_merged = uv_stage1_base.copy()\n",
    "uv_merged = uv_merged.merge(\n",
    "    uv_stage2[[\"respondent\", *stage2_feature_cols]],\n",
    "    on=\"respondent\", how=\"left\"\n",
    " )\n",
    "uv_merged = uv_merged.merge(\n",
    "    uv_stage3[[\"respondent\", *stage3_feature_cols]],\n",
    "    on=\"respondent\", how=\"left\"\n",
    " )\n",
    "\n",
    "merged_path = results_dir / \"uv_merged.csv\"\n",
    "uv_merged.to_csv(merged_path, index=False)\n",
    "\n",
    "issues_df = (\n",
    "    pd.DataFrame(issues_records).sort_values([\"respondent\", \"issue\"])\n",
    "    if issues_records\n",
    "    else pd.DataFrame(columns=[\"respondent\", \"issue\"])\n",
    " )\n",
    "issues_path = results_dir / \"merge_issues.csv\"\n",
    "if issues_df.empty:\n",
    "    if issues_path.exists():\n",
    "        issues_path.unlink()\n",
    "    print(\"UV merge completed with no issues detected.\")\n",
    "else:\n",
    "    issues_df.to_csv(issues_path, index=False)\n",
    "    print(f\"UV merge completed; logged {issues_df.shape[0]} issues to {issues_path.name}.\")\n",
    "\n",
    "uv_merged.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
