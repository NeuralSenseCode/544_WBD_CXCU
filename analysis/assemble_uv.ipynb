{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a9680a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f48a0",
   "metadata": {},
   "source": [
    "## Function to read iMotions sensor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wbdlib import read_imotions, read_imotions_metadata\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_export_dir = project_root / \"data\" / \"Export\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef61f8",
   "metadata": {},
   "source": [
    "## Explanation of functions\n",
    "\n",
    "The above functions are used to read in the sesor data files, one csv at a time, and extract single features per stimulus, and write these features to a simple results file.\n",
    "\n",
    "The functions must be adjusted to:\n",
    "- Discern between long form and short form\n",
    "- Isolate key moments from timings file provided by client\n",
    "- Extract time series\n",
    "- Compute group-wide features such as inter-subject correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84624884",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- Create naming dictionary for all stims\n",
    "- Get total times of all stims\n",
    "- Prepare key_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6248d4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sensor_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>001_116.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>001_58.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>001_114.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>001_102.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>001_108.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>001_107.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  sensor_file\n",
       "0  Group A  001_116.csv\n",
       "1  Group B   001_58.csv\n",
       "2  Group C  001_114.csv\n",
       "3  Group D  001_102.csv\n",
       "4  Group E  001_108.csv\n",
       "5  Group F  001_107.csv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate one sensor export per group for duration scanning\n",
    "\n",
    "\n",
    "group_sensor_files = {}\n",
    "for group_dir in sorted(data_export_dir.glob(\"Group *\")):\n",
    "    if not group_dir.is_dir():\n",
    "        continue\n",
    "    sensor_dirs = sorted(group_dir.glob(\"Analyses/*/Sensor Data\"))\n",
    "    csv_candidates = []\n",
    "    for sensor_dir in sensor_dirs:\n",
    "        csv_candidates.extend(sorted(sensor_dir.glob(\"*.csv\")))\n",
    "    group_sensor_files[group_dir.name] = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "sensor_selection = pd.DataFrame([\n",
    "    {\n",
    "        \"group\": group,\n",
    "        \"sensor_file\": path.name if path else None\n",
    "    }\n",
    "    for group, path in group_sensor_files.items()\n",
    "]).sort_values(\"group\").reset_index(drop=True)\n",
    "\n",
    "sensor_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99e8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n",
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n",
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n",
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n",
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n",
      "c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\imotions.py:63: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, **csv_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Collect per-group stimulus durations without aggregating across groups\n",
    "duration_tables = []\n",
    "issues = {}\n",
    "\n",
    "for group, path in group_sensor_files.items():\n",
    "    if path is None:\n",
    "        issues[group] = \"No sensor CSV found\"\n",
    "        continue\n",
    "    try:\n",
    "        df_group, _ = read_imotions(path)\n",
    "    except Exception as exc:\n",
    "        issues[group] = f\"read_imotions failed: {exc}\"\n",
    "        continue\n",
    "\n",
    "    required_cols = {\"SourceStimuliName\", \"Timestamp\"}\n",
    "    if not required_cols.issubset(df_group.columns):\n",
    "        issues[group] = \"Missing SourceStimuliName or Timestamp\"\n",
    "        continue\n",
    "    df_clean = df_group[[\"SourceStimuliName\", \"Timestamp\"]].copy()\n",
    "    df_clean = df_clean.dropna(subset=[\"SourceStimuliName\"])\n",
    "    df_clean[\"Timestamp\"] = pd.to_numeric(df_clean[\"Timestamp\"], errors=\"coerce\")\n",
    "    df_clean = df_clean.dropna(subset=[\"Timestamp\"])\n",
    "    if df_clean.empty:\n",
    "        issues[group] = \"No valid timestamp data\"\n",
    "        continue\n",
    "\n",
    "    group_duration = (\n",
    "        df_clean.groupby(\"SourceStimuliName\")[\"Timestamp\"]\n",
    "        .apply(lambda s: s.max() - s.min())\n",
    "        .reset_index(name=\"duration_ms\")\n",
    "    )\n",
    "\n",
    "    if group_duration.empty:\n",
    "        issues[group] = \"No stimuli with duration\"\n",
    "        continue\n",
    "\n",
    "    group_duration[\"duration_seconds\"] = group_duration[\"duration_ms\"] / 1000.0\n",
    "    group_duration[\"duration_minutes\"] = group_duration[\"duration_seconds\"] / 60.0\n",
    "    group_duration.insert(0, \"group\", group)\n",
    "    group_duration.rename(columns={\"SourceStimuliName\": \"stimulus_name\"}, inplace=True)\n",
    "    duration_tables.append(group_duration[[\"group\", \"stimulus_name\", \"duration_seconds\", \"duration_minutes\"]])\n",
    "\n",
    "if duration_tables:\n",
    "    stimulus_summary = pd.concat(duration_tables, ignore_index=True)\n",
    "    stimulus_summary.sort_values([\"group\", \"stimulus_name\"], inplace=True)\n",
    "    stimulus_summary[\"duration_seconds\"] = stimulus_summary[\"duration_seconds\"].round(2)\n",
    "    stimulus_summary[\"duration_minutes\"] = stimulus_summary[\"duration_minutes\"].round(2)\n",
    "    stimulus_summary.reset_index(drop=True, inplace=True)\n",
    "    stimulus_summary\n",
    "else:\n",
    "    print(\"No duration records computed.\")\n",
    "\n",
    "if issues:\n",
    "    pd.DataFrame(\n",
    "        {\"group\": list(issues.keys()), \"issue\": list(issues.values())}\n",
    "    ).sort_values(\"group\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9548d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>07 The Notebook</td>\n",
       "      <td>65.39</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group C</td>\n",
       "      <td>09 I Am Legend - Infected encounter</td>\n",
       "      <td>118.49</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>10 The Town - Bank robbery in nun masks</td>\n",
       "      <td>263.21</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group C</td>\n",
       "      <td>Abbott Elementary - S1E9 - Step Class</td>\n",
       "      <td>1291.75</td>\n",
       "      <td>21.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group C</td>\n",
       "      <td>HOME ALONE</td>\n",
       "      <td>115.09</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group                            stimulus_name  duration_seconds  \\\n",
       "0  Group C                          07 The Notebook             65.39   \n",
       "1  Group C      09 I Am Legend - Infected encounter            118.49   \n",
       "2  Group C  10 The Town - Bank robbery in nun masks            263.21   \n",
       "3  Group C    Abbott Elementary - S1E9 - Step Class           1291.75   \n",
       "4  Group C                               HOME ALONE            115.09   \n",
       "\n",
       "   duration_minutes  \n",
       "0              1.09  \n",
       "1              1.97  \n",
       "2              4.39  \n",
       "3             21.53  \n",
       "4              1.92  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d38df137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group A': 'read_imotions failed: Unable to allocate 853. MiB for an array with shape (210, 532611) and data type float64',\n",
       " 'Group B': 'read_imotions failed: Unable to allocate 831. MiB for an array with shape (210, 518407) and data type float64',\n",
       " 'Group D': 'read_imotions failed: Unable to allocate 835. MiB for an array with shape (219, 499863) and data type float64',\n",
       " 'Group E': 'read_imotions failed: Unable to allocate 825. MiB for an array with shape (212, 510043) and data type float64',\n",
       " 'Group F': 'read_imotions failed: Unable to allocate 701. MiB for an array with shape (212, 433145) and data type float64'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7ec5417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd2d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      65.39\n",
       "max    1291.75\n",
       "Name: duration_seconds, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary['duration_seconds'].agg(['min','max']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6057f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>unique_stimuli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  unique_stimuli\n",
       "0  Group C               6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_per_group = stimulus_summary.groupby('group')['stimulus_name'].nunique().reset_index(name='unique_stimuli')\n",
    "stimuli_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fbddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_summary.to_csv(project_root / \"results\" / \"stimulus_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322effdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf37ed",
   "metadata": {},
   "source": [
    "## Stimulus Annotation Overview\n",
    "- `stimulus_rename` links each group-specific stimulus from `stimulus_summary` to a clean `title` and its presentation `Form` (`Long` or `Short`).\n",
    "- Some titles appear in both forms; the long cut (Ã¢â€°Ë†30 min) includes the short-form key moment as an embedded segment.\n",
    "- `key_moments` pinpoints, for every long-form title, when the key moment begins (`Lead-up Duration`) and how long it lasts (`Key moment Duration_LF`).\n",
    "- These tables let us align short-form clips with the corresponding segment inside the long-form presentation for downstream comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f5419",
   "metadata": {},
   "source": [
    "## Stage 1: Demographics\n",
    "We extract respondent-level identifiers and timing information from the metadata embedded in each sensor export to seed the unified view (UV). This pass scans every sensor CSV, captures study name, respondent attributes, and recording timestamps, and prepares the foundation for later feature merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8bff719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03\n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05\n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40\n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42\n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22\n",
       "..          ...   ...        ...         ...        ...\n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03\n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06\n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14\n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41\n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00\n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wbdlib import (\n",
    "    derive_respondent_identifier,\n",
    "    first_segment,\n",
    "    infer_group_letter,\n",
    "    parse_recording_timestamp,\n",
    " )\n",
    "\n",
    "metadata_keys = [\n",
    "    \"Study name\",\n",
    "    \"Respondent Name\",\n",
    "    \"Respondent Group\",\n",
    "    \"Recording time\"\n",
    " ]\n",
    "\n",
    "sensor_file_paths = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    " )\n",
    "\n",
    "demographic_records = []\n",
    "\n",
    "for csv_path in sensor_file_paths:\n",
    "    meta = read_imotions_metadata(csv_path, metadata=metadata_keys)\n",
    "\n",
    "    study_clean = first_segment(meta.get(\"Study name\"))\n",
    "    respondent_group_clean = first_segment(meta.get(\"Respondent Group\"))\n",
    "    group_letter = infer_group_letter(study_clean, respondent_group_clean, csv_path)\n",
    "\n",
    "    respondent_raw = first_segment(meta.get(\"Respondent Name\"))\n",
    "    respondent_value = derive_respondent_identifier(respondent_raw, csv_path.stem)\n",
    "\n",
    "    date_study, time_study = parse_recording_timestamp(meta.get(\"Recording time\"))\n",
    "\n",
    "    demographic_records.append({\n",
    "        \"source_file\": csv_path.name,\n",
    "        \"group\": group_letter,\n",
    "        \"respondent\": respondent_value,\n",
    "        \"date_study\": date_study,\n",
    "        \"time_study\": time_study\n",
    "    })\n",
    "\n",
    "uv_stage1 = pd.DataFrame(demographic_records)\n",
    "\n",
    "if not uv_stage1.empty:\n",
    "    uv_stage1 = uv_stage1.sort_values([\"group\", \"respondent\"]).reset_index(drop=True)\n",
    "    uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8da774bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>44-59</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-27</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>Male</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  age  gender age_group                      ethnicity  \\\n",
       "0        104   59    Male     44-59                          White   \n",
       "1        106   30    Male     28-43                          White   \n",
       "2        116   19    Male     18-27                          White   \n",
       "3         14   33    Male     28-43  Hispanic/Latino/Latina/Latinx   \n",
       "4          3   34  Female     28-43                          White   \n",
       "\n",
       "                 income_group          content_consumption  \\\n",
       "0    $60,000 or more per year  More than 24 hours per week   \n",
       "1    $60,000 or more per year       3 to 12 hours per week   \n",
       "2  $35,000  $60,000 per year       3 to 12 hours per week   \n",
       "3    $60,000 or more per year  More than 24 hours per week   \n",
       "4    $60,000 or more per year      12 to 24 hours per week   \n",
       "\n",
       "   content_consumption_movies  content_consumption_series  \\\n",
       "0                          10                          90   \n",
       "1                          25                          50   \n",
       "2                          25                          50   \n",
       "3                          20                          40   \n",
       "4                          10                          70   \n",
       "\n",
       "   content_consumption_short grid_comments  \n",
       "0                          0           NaN  \n",
       "1                         25           NaN  \n",
       "2                         25           NaN  \n",
       "3                         40       No EEG.  \n",
       "4                         20       No EEG.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach supplemental demographics from grid.csv\n",
    "grid_path = project_root / \"data\" / \"grid.csv\"\n",
    "grid_rename_map = {\n",
    "    \"QB2. Age\": \"age\",\n",
    "    \"QB2. Age.1\": \"age_group\",\n",
    "    \"QA2. Gender\": \"gender\",\n",
    "    \"QC. Ethnicity\": \"ethnicity\",\n",
    "    \"QD. Income\": \"income_group\",\n",
    "    \"Q1. Content Hours Per Week\": \"content_consumption\",\n",
    "    \"Q2. Program Type %- Movies\": \"content_consumption_movies\",\n",
    "    \"Q2. Program Type %- Series\": \"content_consumption_series\",\n",
    "    \"Q2. Program Type %- Short\": \"content_consumption_short\",\n",
    "    \"Comments\": \"grid_comments\",\n",
    "}\n",
    "\n",
    "grid_raw_full = pd.read_csv(grid_path, encoding=\"latin1\")\n",
    "grid_raw_full.columns = [col.strip() for col in grid_raw_full.columns]\n",
    "\n",
    "id_column = next((col for col in [\"respondent\", \"Respondent\", \"No.\", \"No\", \"Participant\", \"Participant #\"] if col in grid_raw_full.columns), None)\n",
    "if id_column is None:\n",
    "    raise ValueError(\"Unable to locate a respondent identifier column in grid.csv\")\n",
    "\n",
    "grid_raw_full = grid_raw_full.rename(columns={id_column: \"respondent\"})\n",
    "\n",
    "available_rename_map = {orig: dest for orig, dest in grid_rename_map.items() if orig in grid_raw_full.columns}\n",
    "missing_columns = sorted(set(grid_rename_map.keys()) - set(available_rename_map.keys()))\n",
    "if missing_columns:\n",
    "    print(f\"Warning: The following columns were not found in grid.csv and will be skipped: {missing_columns}\")\n",
    "\n",
    "grid_columns = [\"respondent\", *available_rename_map.keys()]\n",
    "grid_raw = grid_raw_full.loc[:, grid_columns].copy()\n",
    "\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "\n",
    "grid_raw[\"respondent\"] = pd.to_numeric(grid_raw[\"respondent\"], errors=\"coerce\")\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "grid_raw[\"respondent\"] = grid_raw[\"respondent\"].astype(int).astype(str)\n",
    "\n",
    "grid_subset = grid_raw.rename(columns=available_rename_map)\n",
    "\n",
    "text_cols = [\"age_group\", \"gender\", \"ethnicity\", \"income_group\", \"content_consumption\", \"grid_comments\"]\n",
    "for col in text_cols:\n",
    "    if col in grid_subset.columns:\n",
    "        grid_subset[col] = grid_subset[col].apply(lambda value: value.strip() if isinstance(value, str) else value)\n",
    "\n",
    "numeric_cols = [\"content_consumption_movies\", \"content_consumption_series\", \"content_consumption_short\", \"age\"]\n",
    "for col in numeric_cols:\n",
    "    if col in grid_subset.columns:\n",
    "        grid_subset[col] = pd.to_numeric(grid_subset[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "grid_subset = grid_subset.drop_duplicates(subset=\"respondent\", keep=\"first\")\n",
    "\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1 = uv_stage1.merge(grid_subset, on=\"respondent\", how=\"left\", validate=\"many_to_one\")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "display_columns = [\n",
    "    \"respondent\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"age_group\",\n",
    "    \"ethnicity\",\n",
    "    \"income_group\",\n",
    "    \"content_consumption\",\n",
    "    \"content_consumption_movies\",\n",
    "    \"content_consumption_series\",\n",
    "    \"content_consumption_short\",\n",
    "    \"grid_comments\",\n",
    "]\n",
    "existing_display_columns = [col for col in display_columns if col in uv_stage1.columns]\n",
    "uv_stage1.loc[:, existing_display_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a43b0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group respondent Short Form Long Form\n",
       "0     A        104    Mad Max  The Town\n",
       "1     A        106    Mad Max  The Town\n",
       "2     A        116    Mad Max  The Town\n",
       "3     A         14    Mad Max  The Town\n",
       "4     A          3    Mad Max  The Town"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wbdlib import build_group_short_long_map\n",
    "\n",
    "group_short_long_map = build_group_short_long_map()\n",
    "\n",
    "uv_stage1 = uv_stage1.merge(\n",
    "    group_short_long_map,\n",
    "    on=\"group\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1.loc[:, [\"group\", \"respondent\", \"Short Form\", \"Long Form\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb769cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate respondents detected.\n"
     ]
    }
   ],
   "source": [
    "duplicate_respondents = uv_stage1[uv_stage1.duplicated(subset=\"respondent\", keep=False)]\n",
    "if duplicate_respondents.empty:\n",
    "    print(\"No duplicate respondents detected.\")\n",
    "else:\n",
    "    duplicate_respondents.sort_values(\"respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92662888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>66</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>61</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03   59     44-59   \n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05   30     28-43   \n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40   19     18-27   \n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42   33     28-43   \n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22   34     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03   63     60-69   \n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06   66     60-69   \n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14   61     60-69   \n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41   34     28-43   \n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "\n",
       "    gender                      ethnicity                income_group  \\\n",
       "0     Male                          White    $60,000 or more per year   \n",
       "1     Male                          White    $60,000 or more per year   \n",
       "2     Male                          White  $35,000  $60,000 per year   \n",
       "3     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year   \n",
       "4   Female                          White    $60,000 or more per year   \n",
       "..     ...                            ...                         ...   \n",
       "78    Male         Black/African American    $60,000 or more per year   \n",
       "79    Male                          White  $35,000  $60,000 per year   \n",
       "80  Female         Black/African American  $35,000  $60,000 per year   \n",
       "81  Female                          White    $60,000 or more per year   \n",
       "82  Female                          White    $60,000 or more per year   \n",
       "\n",
       "            content_consumption  content_consumption_movies  \\\n",
       "0   More than 24 hours per week                          10   \n",
       "1        3 to 12 hours per week                          25   \n",
       "2        3 to 12 hours per week                          25   \n",
       "3   More than 24 hours per week                          20   \n",
       "4       12 to 24 hours per week                          10   \n",
       "..                          ...                         ...   \n",
       "78      12 to 24 hours per week                          70   \n",
       "79       3 to 12 hours per week                          10   \n",
       "80      12 to 24 hours per week                          30   \n",
       "81      12 to 24 hours per week                          40   \n",
       "82      12 to 24 hours per week                          20   \n",
       "\n",
       "    content_consumption_series  content_consumption_short grid_comments  \\\n",
       "0                           90                          0           NaN   \n",
       "1                           50                         25           NaN   \n",
       "2                           50                         25           NaN   \n",
       "3                           40                         40       No EEG.   \n",
       "4                           70                         20       No EEG.   \n",
       "..                         ...                        ...           ...   \n",
       "78                          20                         10           NaN   \n",
       "79                          80                         10           NaN   \n",
       "80                          30                         40           NaN   \n",
       "81                          60                          0           NaN   \n",
       "82                          60                         20           NaN   \n",
       "\n",
       "   Short Form         Long Form  \n",
       "0     Mad Max          The Town  \n",
       "1     Mad Max          The Town  \n",
       "2     Mad Max          The Town  \n",
       "3     Mad Max          The Town  \n",
       "4     Mad Max          The Town  \n",
       "..        ...               ...  \n",
       "78    Mad Max  Abbot Elementary  \n",
       "79    Mad Max  Abbot Elementary  \n",
       "80    Mad Max  Abbot Elementary  \n",
       "81    Mad Max  Abbot Elementary  \n",
       "82    Mad Max  Abbot Elementary  \n",
       "\n",
       "[83 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv_stage1.to_csv(project_root / \"results\" / \"uv_stage1.csv\", index=False)\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe8719",
   "metadata": {},
   "source": [
    "## Stage 2: Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b2525",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Stage 2 ingests the exposure-day survey responses while treating the Stage 1 export as the immutable base. Respondent metadata (age, gender, group, and the long/short assignment) is always reloaded from `results/uv_stage1.csv` before any survey joins. The section engineers Likert scores, familiarity composites, and open-ended extracts so downstream modeling can combine perceptual metrics with the demographic scaffold without relying on sensor features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd25fa2",
   "metadata": {},
   "source": [
    "### Workflow Summary\n",
    "1. Reload `results/uv_stage1.csv` to obtain the canonical respondent roster and long/short stimulus assignment.\n",
    "2. Build the survey metadata lookup (question map, polarity, subscales) to drive scoring logic.\n",
    "3. Ingest every group-level `MERGED_SURVEY_RESPONSE_MATRIX` export, harmonize respondent IDs, and apply the rename map.\n",
    "4. Score Likert, familiarity, and recency responses; compute enjoyment composites; and archive open-ended text separately.\n",
    "5. Merge the numeric survey features onto the Stage 1 base, logging missing or duplicate respondents to `uv_stage2_issues.csv`.\n",
    "6. Write Stage 2 deliverables: `uv_stage2_features.csv`, `uv_stage2_open_ended.csv`, and the combined `uv_stage2.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c179d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_file group respondent  date_study time_study  age age_group  gender  \\\n",
       "0  003_104.csv     A        104  10/16/2025   18:09:03   59     44-59    Male   \n",
       "1  002_106.csv     A        106  10/16/2025   19:35:05   30     28-43    Male   \n",
       "2  001_116.csv     A        116  10/18/2025   12:37:40   19     18-27    Male   \n",
       "3   006_14.csv     A         14  10/11/2025   09:32:42   33     28-43    Male   \n",
       "4    007_3.csv     A          3  10/10/2025   09:19:22   34     28-43  Female   \n",
       "\n",
       "                       ethnicity                income_group  \\\n",
       "0                          White    $60,000 or more per year   \n",
       "1                          White    $60,000 or more per year   \n",
       "2                          White  $35,000  $60,000 per year   \n",
       "3  Hispanic/Latino/Latina/Latinx    $60,000 or more per year   \n",
       "4                          White    $60,000 or more per year   \n",
       "\n",
       "           content_consumption  content_consumption_movies  \\\n",
       "0  More than 24 hours per week                          10   \n",
       "1       3 to 12 hours per week                          25   \n",
       "2       3 to 12 hours per week                          25   \n",
       "3  More than 24 hours per week                          20   \n",
       "4      12 to 24 hours per week                          10   \n",
       "\n",
       "   content_consumption_series  content_consumption_short grid_comments  \\\n",
       "0                          90                          0           NaN   \n",
       "1                          50                         25           NaN   \n",
       "2                          50                         25           NaN   \n",
       "3                          40                         40       No EEG.   \n",
       "4                          70                         20       No EEG.   \n",
       "\n",
       "  Short Form Long Form  \n",
       "0    Mad Max  The Town  \n",
       "1    Mad Max  The Town  \n",
       "2    Mad Max  The Town  \n",
       "3    Mad Max  The Town  \n",
       "4    Mad Max  The Town  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1 = uv_stage1_base.copy()\n",
    "uv_stage1_base.shape\n",
    "uv_stage1_base.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dbea40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "survey_rename_map = pd.read_csv(project_root / \"data\" / \"survey_column_rename_stage3.csv\")\n",
    "survey_questions = pd.read_csv(project_root / \"data\" / \"survey_questions.csv\")\n",
    "\n",
    "survey_questions[\"question_code\"] = survey_questions[\"question_code\"].astype(str).str.strip()\n",
    "survey_questions[\"question_type\"] = survey_questions[\"question_type\"].str.lower()\n",
    "survey_questions[\"subscale\"] = survey_questions[\"subscale\"].fillna(\"\")\n",
    "survey_questions[\"polarity\"] = survey_questions[\"polarity\"].fillna(\"\")\n",
    "\n",
    "survey_metadata = (\n",
    "    survey_rename_map\n",
    "    .merge(survey_questions, on=\"question_code\", how=\"left\", suffixes=(\"\", \"_details\"))\n",
    ")\n",
    "\n",
    "survey_metadata[\"question_type\"] = survey_metadata[\"question_type\"].fillna(\"likert\")\n",
    "survey_metadata[\"subscale\"] = survey_metadata[\"subscale\"].fillna(\"\")\n",
    "survey_metadata[\"polarity\"] = survey_metadata[\"polarity\"].fillna(\"\")\n",
    "survey_metadata_lookup = (\n",
    "    survey_metadata\n",
    "    .drop_duplicates(subset=[\"target_column\"])\n",
    "    .set_index(\"target_column\")\n",
    ")\n",
    "\n",
    "survey_files = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Survey/MERGED_SURVEY_RESPONSE_MATRIX-*.txt\")\n",
    ")\n",
    "\n",
    "if not survey_files:\n",
    "    raise FileNotFoundError(\"No survey response text files detected under data/Export/*/Survey/.\")\n",
    "\n",
    "len(survey_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04435f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wbdlib import (\n",
    "    clean_response,\n",
    "    clip_zero_to_four,\n",
    "    parse_likert_value,\n",
    "    reverse_likert,\n",
    "    safe_write_csv,\n",
    "    score_familiarity,\n",
    "    score_last_watched,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7119b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\2064908789.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E21</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E22</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>3 = Neither Agree nor Disagree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>1 = Strongly Disagree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E2 Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                 5 = Strongly Agree                         4 = Agree   \n",
       "1                          4 = Agree                         4 = Agree   \n",
       "2                          4 = Agree    3 = Neither Agree nor Disagree   \n",
       "3                 5 = Strongly Agree                         4 = Agree   \n",
       "4                 5 = Strongly Agree             1 = Strongly Disagree   \n",
       "..                               ...                               ...   \n",
       "77                               NaN                               NaN   \n",
       "78                               NaN                               NaN   \n",
       "79                               NaN                               NaN   \n",
       "80                               NaN                               NaN   \n",
       "81                               NaN                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                 5 = Strongly Agree  ...   \n",
       "1                 5 = Strongly Agree  ...   \n",
       "2                          4 = Agree  ...   \n",
       "3                          4 = Agree  ...   \n",
       "4                 5 = Strongly Agree  ...   \n",
       "..                               ...  ...   \n",
       "77                               NaN  ...   \n",
       "78                               NaN  ...   \n",
       "79                               NaN  ...   \n",
       "80                               NaN  ...   \n",
       "81                               NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E21  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "81                                         NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E22  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "81                                         NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "81                                          NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wbdlib import (\n",
    "    extract_group_letter_from_path,\n",
    "    load_survey_file,\n",
    "    rename_survey_columns,\n",
    " )\n",
    "\n",
    "\n",
    "survey_frames = []\n",
    "\n",
    "for survey_path in survey_files:\n",
    "    group_letter = extract_group_letter_from_path(survey_path)\n",
    "    df = load_survey_file(survey_path)\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    df = df.replace({\"EMPTY FIELD\": np.nan})\n",
    "    df[\"RESPONDENT\"] = df[\"RESPONDENT\"].astype(str).str.strip()\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"RESPONDENT\": \"respondent\",\n",
    "            \"GROUP\": \"survey_group\",\n",
    "            \"STUDY\": \"survey_study\",\n",
    "            \"GENDER\": \"survey_gender\",\n",
    "            \"AGE\": \"survey_age\",\n",
    "        }\n",
    "    )\n",
    "    if \"survey_group\" not in df.columns:\n",
    "        df[\"survey_group\"] = group_letter\n",
    "    df[\"survey_group\"] = df[\"survey_group\"].fillna(group_letter).astype(str).str.strip()\n",
    "    if \"survey_study\" in df.columns:\n",
    "        df[\"survey_study\"] = df[\"survey_study\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_study\"] = np.nan\n",
    "    if \"survey_gender\" in df.columns:\n",
    "        df[\"survey_gender\"] = df[\"survey_gender\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_gender\"] = np.nan\n",
    "    df[\"survey_file\"] = survey_path.name\n",
    "    if \"survey_age\" in df.columns:\n",
    "        df[\"survey_age\"] = pd.to_numeric(df[\"survey_age\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"survey_age\"] = np.nan\n",
    "    base_columns = df[\n",
    "        [\n",
    "            \"respondent\",\n",
    "            \"survey_group\",\n",
    "            \"survey_study\",\n",
    "            \"survey_gender\",\n",
    "            \"survey_age\",\n",
    "            \"survey_file\",\n",
    "        ]\n",
    "    ].copy()\n",
    "    renamed_columns = rename_survey_columns(df, survey_metadata, group_letter)\n",
    "    df_prepared = pd.concat([base_columns, renamed_columns], axis=1)\n",
    "    survey_frames.append(df_prepared)\n",
    "\n",
    "survey_responses = pd.concat(survey_frames, ignore_index=True)\n",
    "survey_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d5a7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E21</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E22</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>3 = Neither Agree nor Disagree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>1 = Strongly Disagree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E2 Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                 5 = Strongly Agree                         4 = Agree   \n",
       "1                          4 = Agree                         4 = Agree   \n",
       "2                          4 = Agree    3 = Neither Agree nor Disagree   \n",
       "3                 5 = Strongly Agree                         4 = Agree   \n",
       "4                 5 = Strongly Agree             1 = Strongly Disagree   \n",
       "..                               ...                               ...   \n",
       "77                               NaN                               NaN   \n",
       "78                               NaN                               NaN   \n",
       "79                               NaN                               NaN   \n",
       "80                               NaN                               NaN   \n",
       "81                               NaN                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                 5 = Strongly Agree  ...   \n",
       "1                 5 = Strongly Agree  ...   \n",
       "2                          4 = Agree  ...   \n",
       "3                          4 = Agree  ...   \n",
       "4                 5 = Strongly Agree  ...   \n",
       "..                               ...  ...   \n",
       "77                               NaN  ...   \n",
       "78                               NaN  ...   \n",
       "79                               NaN  ...   \n",
       "80                               NaN  ...   \n",
       "81                               NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E21  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "81                                         NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E22  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "81                                         NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "81                                          NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_responses[\"respondent\"] = survey_responses[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "duplicate_ids = sorted(survey_responses.loc[survey_responses[\"respondent\"].duplicated(), \"respondent\"].unique())\n",
    "if duplicate_ids:\n",
    "    print(f\"Warning: duplicate survey rows detected for respondents: {duplicate_ids}\")\n",
    "\n",
    "survey_numeric = survey_responses.drop_duplicates(subset=[\"respondent\"], keep=\"first\").copy()\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2e556ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E21</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E22</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E2  Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                                 5.0                                4.0   \n",
       "1                                 4.0                                4.0   \n",
       "2                                 4.0                                3.0   \n",
       "3                                 5.0                                4.0   \n",
       "4                                 5.0                                1.0   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                                 5.0  ...   \n",
       "1                                 5.0  ...   \n",
       "2                                 4.0  ...   \n",
       "3                                 4.0  ...   \n",
       "4                                 5.0  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_E21  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_E22  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "81                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                            NaN  \n",
       "1                                            NaN  \n",
       "2                                            NaN  \n",
       "3                                            NaN  \n",
       "4                                            NaN  \n",
       "..                                           ...  \n",
       "77                                           NaN  \n",
       "78                                           NaN  \n",
       "79                                           NaN  \n",
       "80                                           NaN  \n",
       "81                                           NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "\n",
    "    question_code = (meta.get(\"question_code\") or \"\").strip().upper()\n",
    "    question_type = (meta.get(\"question_type\") or \"\").strip().lower()\n",
    "    polarity = (meta.get(\"polarity\") or \"\").strip().lower()\n",
    "\n",
    "    if question_code in {\"F1\", \"F3\"} or column.endswith(\"_Survey_Familiarity_F1\") or column.endswith(\"_Survey_Familiarity_F3\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(score_familiarity)\n",
    "    elif question_code == \"F2\" or column.endswith(\"_Survey_Familiarity_F2\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(score_last_watched)\n",
    "    elif question_type == \"likert\":\n",
    "        survey_numeric[column] = survey_numeric[column].apply(parse_likert_value)\n",
    "        if polarity == \"negative\":\n",
    "            survey_numeric[column] = survey_numeric[column].apply(reverse_likert)\n",
    "\n",
    "familiarity_columns = [\n",
    "    column\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F2\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F3\")\n",
    "]\n",
    "\n",
    "for column in familiarity_columns:\n",
    "    survey_numeric[column] = survey_numeric[column].apply(clip_zero_to_four)\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6336772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\865435719.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E2  Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                                 5.0                                4.0   \n",
       "1                                 4.0                                4.0   \n",
       "2                                 4.0                                3.0   \n",
       "3                                 5.0                                4.0   \n",
       "4                                 5.0                                1.0   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                                 5.0  ...   \n",
       "1                                 5.0  ...   \n",
       "2                                 4.0  ...   \n",
       "3                                 4.0  ...   \n",
       "4                                 5.0  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN           \n",
       "1                                                 NaN           \n",
       "2                                                 NaN           \n",
       "3                                                 NaN           \n",
       "4                                                 NaN           \n",
       "..                                                ...           \n",
       "77                                               43.0           \n",
       "78                                               59.0           \n",
       "79                                               53.0           \n",
       "80                                               47.0           \n",
       "81                                               45.0           \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "77                                           3.583333      \n",
       "78                                           4.916667      \n",
       "79                                           4.416667      \n",
       "80                                           3.916667      \n",
       "81                                           3.750000      \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "77                                           0.687500            \n",
       "78                                           0.895833            \n",
       "79                                           0.770833            \n",
       "80                                           0.645833            \n",
       "81                                           0.645833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "77                                           0.645833                     \n",
       "78                                           0.979167                     \n",
       "79                                           0.854167                     \n",
       "80                                           0.729167                     \n",
       "81                                           0.687500                     \n",
       "\n",
       "   Short_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "0                                                   0       \n",
       "1                                                   0       \n",
       "2                                                   0       \n",
       "3                                                   0       \n",
       "4                                                   0       \n",
       "..                                                ...       \n",
       "77                                                  0       \n",
       "78                                                  0       \n",
       "79                                                  0       \n",
       "80                                                  0       \n",
       "81                                                  0       \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "77                                                NaN      \n",
       "78                                                NaN      \n",
       "79                                                NaN      \n",
       "80                                                NaN      \n",
       "81                                                NaN      \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "77                                                NaN            \n",
       "78                                                NaN            \n",
       "79                                                NaN            \n",
       "80                                                NaN            \n",
       "81                                                NaN            \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN       \n",
       "1                                                 NaN       \n",
       "2                                                 NaN       \n",
       "3                                                 NaN       \n",
       "4                                                 NaN       \n",
       "..                                                ...       \n",
       "77                                                NaN       \n",
       "78                                                NaN       \n",
       "79                                                NaN       \n",
       "80                                                NaN       \n",
       "81                                                NaN       \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN             \n",
       "1                                                 NaN             \n",
       "2                                                 NaN             \n",
       "3                                                 NaN             \n",
       "4                                                 NaN             \n",
       "..                                                ...             \n",
       "77                                                NaN             \n",
       "78                                                NaN             \n",
       "79                                                NaN             \n",
       "80                                                NaN             \n",
       "81                                                NaN             \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "77                                                NaN                     \n",
       "78                                                NaN                     \n",
       "79                                                NaN                     \n",
       "80                                                NaN                     \n",
       "81                                                NaN                     \n",
       "\n",
       "[82 rows x 336 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscale_columns = defaultdict(list)\n",
    "overall_enjoyment_columns = defaultdict(list)\n",
    "\n",
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "    if meta.get(\"topic\") != \"enjoyment\":\n",
    "        continue\n",
    "    if (meta.get(\"question_type\") or \"\").strip().lower() != \"likert\":\n",
    "        continue\n",
    "    prefix = column.split(\"_Survey_\")[0]\n",
    "    subscale = (meta.get(\"subscale\") or \"\").strip()\n",
    "    overall_enjoyment_columns[prefix].append(column)\n",
    "    if subscale:\n",
    "        subscale_columns[(prefix, subscale)].append(column)\n",
    "\n",
    "for (prefix, subscale), cols in subscale_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_{subscale}_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_{subscale}_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_{subscale}_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_{subscale}_Normalized\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "    sum_values = values.sum(axis=1, min_count=1)\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = sum_values\n",
    "    survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "for prefix, cols in overall_enjoyment_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_EnjoymentComposite_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_EnjoymentComposite_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_EnjoymentComposite_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_EnjoymentComposite_Normalized\"\n",
    "    norm_corrected_col = f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\"\n",
    "    corrected_col = f\"{prefix}_Survey_EnjoymentComposite_Corrected\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "\n",
    "    raw_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "    corrected_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "\n",
    "    for column in cols:\n",
    "        polarity_meta = \"\"\n",
    "        if column in survey_metadata_lookup.index:\n",
    "            polarity_meta = (survey_metadata_lookup.loc[column].get(\"polarity\") or \"\").strip().lower()\n",
    "        if column in survey_responses.columns:\n",
    "            raw_series = survey_responses.loc[values.index, column]\n",
    "            parsed_series = raw_series.apply(parse_likert_value)\n",
    "        else:\n",
    "            fallback_series = pd.to_numeric(survey_numeric.loc[values.index, column], errors=\"coerce\")\n",
    "            if polarity_meta == \"negative\":\n",
    "                parsed_series = fallback_series.apply(reverse_likert)\n",
    "            else:\n",
    "                parsed_series = fallback_series\n",
    "        raw_components[column] = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        corrected_series = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        if polarity_meta == \"negative\":\n",
    "            corrected_series = corrected_series.apply(reverse_likert)\n",
    "        corrected_components[column] = corrected_series\n",
    "\n",
    "    enjoyment_questions = ['E1','E2','E3','E4','E5','E6','E7','E8','E9','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22','E23','E24','E25']\n",
    "    keep_cols = [c for c in raw_components.columns if any(eq in c for eq in enjoyment_questions)]\n",
    "    raw_components = raw_components[keep_cols]\n",
    "    corrected_components = corrected_components[keep_cols]\n",
    "    raw_sum_values = raw_components.sum(axis=1, min_count=1)\n",
    "    corrected_sum = corrected_components.sum(axis=1, min_count=1)\n",
    "    raw_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((raw_sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((corrected_sum - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_mean = np.where(count_values > 0, corrected_sum / count_values, np.nan)\n",
    "\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = raw_sum_values\n",
    "    survey_numeric[corrected_col] = corrected_sum\n",
    "    survey_numeric[mean_col] = corrected_mean\n",
    "    survey_numeric[norm_col] = raw_normalized\n",
    "    survey_numeric[norm_corrected_col] = corrected_normalized\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c762d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_36856\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E2  Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                                 5.0                                4.0   \n",
       "1                                 4.0                                4.0   \n",
       "2                                 4.0                                3.0   \n",
       "3                                 5.0                                4.0   \n",
       "4                                 5.0                                1.0   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                                 5.0  ...   \n",
       "1                                 5.0  ...   \n",
       "2                                 4.0  ...   \n",
       "3                                 4.0  ...   \n",
       "4                                 5.0  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "    Long_The Town_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                            0.000   \n",
       "1                                            0.750   \n",
       "2                                            0.000   \n",
       "3                                            0.000   \n",
       "4                                            0.125   \n",
       "..                                             ...   \n",
       "77                                             NaN   \n",
       "78                                             NaN   \n",
       "79                                             NaN   \n",
       "80                                             NaN   \n",
       "81                                             NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "..                                           ...   \n",
       "77                                           0.0   \n",
       "78                                           0.0   \n",
       "79                                           0.0   \n",
       "80                                           0.0   \n",
       "81                                           0.0   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Count  \\\n",
       "0                                                   0    \n",
       "1                                                   0    \n",
       "2                                                   0    \n",
       "3                                                   0    \n",
       "4                                                   0    \n",
       "..                                                ...    \n",
       "77                                                  0    \n",
       "78                                                  0    \n",
       "79                                                  0    \n",
       "80                                                  0    \n",
       "81                                                  0    \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                                 NaN         \n",
       "1                                                 NaN         \n",
       "2                                                 NaN         \n",
       "3                                                 NaN         \n",
       "4                                                 NaN         \n",
       "..                                                ...         \n",
       "77                                                NaN         \n",
       "78                                                NaN         \n",
       "79                                                NaN         \n",
       "80                                                NaN         \n",
       "81                                                NaN         \n",
       "\n",
       "   Short_Mad Max_Survey_Familiarity_C1  \\\n",
       "0                                  0.0   \n",
       "1                                  4.0   \n",
       "2                                  1.0   \n",
       "3                                  0.0   \n",
       "4                                  3.0   \n",
       "..                                 ...   \n",
       "77                                 5.0   \n",
       "78                                 0.0   \n",
       "79                                 3.0   \n",
       "80                                 0.0   \n",
       "81                                 1.0   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Count  \\\n",
       "0                                           2   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "3                                           2   \n",
       "4                                           2   \n",
       "..                                        ...   \n",
       "77                                          2   \n",
       "78                                          2   \n",
       "79                                          2   \n",
       "80                                          2   \n",
       "81                                          2   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                            0.000   \n",
       "1                                            0.500   \n",
       "2                                            0.125   \n",
       "3                                            0.000   \n",
       "4                                            0.375   \n",
       "..                                             ...   \n",
       "77                                           0.625   \n",
       "78                                           0.000   \n",
       "79                                           0.375   \n",
       "80                                           0.000   \n",
       "81                                           0.125   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "..                                   ...   \n",
       "77                                   0.0   \n",
       "78                                   0.0   \n",
       "79                                   0.0   \n",
       "80                                   0.0   \n",
       "81                                   0.0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Count  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "77                                           0   \n",
       "78                                           0   \n",
       "79                                           0   \n",
       "80                                           0   \n",
       "81                                           0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Normalized  \n",
       "0                                               NaN  \n",
       "1                                               NaN  \n",
       "2                                               NaN  \n",
       "3                                               NaN  \n",
       "4                                               NaN  \n",
       "..                                              ...  \n",
       "77                                              NaN  \n",
       "78                                              NaN  \n",
       "79                                              NaN  \n",
       "80                                              NaN  \n",
       "81                                              NaN  \n",
       "\n",
       "[82 rows x 354 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "familiarity_prefixes = sorted({\n",
    "    column.split(\"_Survey_\")[0]\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "})\n",
    "\n",
    "for prefix in familiarity_prefixes:\n",
    "    f1_col = f\"{prefix}_Survey_Familiarity_F1\"\n",
    "    f2_col = f\"{prefix}_Survey_Familiarity_F2\"\n",
    "    if f1_col not in survey_numeric.columns or f2_col not in survey_numeric.columns:\n",
    "        continue\n",
    "    c1_col = f\"{prefix}_Survey_Familiarity_C1\"\n",
    "    count_col = f\"{prefix}_Survey_Familiarity_C1_Count\"\n",
    "    norm_col = f\"{prefix}_Survey_Familiarity_C1_Normalized\"\n",
    "    pair = survey_numeric[[f1_col, f2_col]]\n",
    "    sum_values = pair.fillna(0).sum(axis=1)\n",
    "    count_values = pair.notna().sum(axis=1)\n",
    "    survey_numeric[c1_col] = sum_values\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip(sum_values / (4 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a062347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E2</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E3</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E4</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E2  Long_The Town_Survey_Enjoyment_E3  \\\n",
       "0                                 5.0                                4.0   \n",
       "1                                 4.0                                4.0   \n",
       "2                                 4.0                                3.0   \n",
       "3                                 5.0                                4.0   \n",
       "4                                 5.0                                1.0   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E4  ...  \\\n",
       "0                                 5.0  ...   \n",
       "1                                 5.0  ...   \n",
       "2                                 4.0  ...   \n",
       "3                                 4.0  ...   \n",
       "4                                 5.0  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "    Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      2.0   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "77                                     NaN   \n",
       "78                                     NaN   \n",
       "79                                     NaN   \n",
       "80                                     NaN   \n",
       "81                                     NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               5.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               4.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               4.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               3.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               1.0   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                      1.0   \n",
       "1                                      4.0   \n",
       "2                                      4.0   \n",
       "3                                      2.0   \n",
       "4                                      4.0   \n",
       "..                                     ...   \n",
       "77                                     4.0   \n",
       "78                                     1.0   \n",
       "79                                     4.0   \n",
       "80                                     4.0   \n",
       "81                                     1.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                      1.0   \n",
       "1                                      3.0   \n",
       "2                                      3.0   \n",
       "3                                      2.0   \n",
       "4                                      3.0   \n",
       "..                                     ...   \n",
       "77                                     3.0   \n",
       "78                                     1.0   \n",
       "79                                     3.0   \n",
       "80                                     3.0   \n",
       "81                                     1.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "77                                     1.0   \n",
       "78                                     NaN   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     NaN   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      4.0   \n",
       "80                                      4.0   \n",
       "81                                      0.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      3.0   \n",
       "80                                      3.0   \n",
       "81                                      0.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F2  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                       NaN  \n",
       "..                                      ...  \n",
       "77                                      NaN  \n",
       "78                                      NaN  \n",
       "79                                      1.0  \n",
       "80                                      1.0  \n",
       "81                                      NaN  \n",
       "\n",
       "[82 rows x 366 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_ended_columns = [\n",
    "    column\n",
    "    for column, meta in survey_metadata_lookup.iterrows()\n",
    "    if column in survey_numeric.columns and (meta.get(\"question_type\") or \"\").strip().lower() == \"open ended\"\n",
    "]\n",
    "\n",
    "survey_open_ended = survey_responses[[\n",
    "    \"respondent\",\n",
    "    \"survey_group\",\n",
    "    \"survey_study\",\n",
    "    \"survey_gender\",\n",
    "    \"survey_age\",\n",
    "    \"survey_file\",\n",
    "    *open_ended_columns,\n",
    "]].copy()\n",
    "\n",
    "survey_features = survey_numeric.drop(columns=[col for col in open_ended_columns if col in survey_numeric.columns])\n",
    "\n",
    "# Integrate screening familiarity composites\n",
    "screening_path = project_root / \"results\" / \"individual_composite_scores.csv\"\n",
    "if screening_path.exists():\n",
    "    screening_raw = pd.read_csv(screening_path)\n",
    "    screening_raw[\"respondent\"] = screening_raw[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "    screening_value_columns = [\n",
    "        col\n",
    "        for col in screening_raw.columns\n",
    "        if col.endswith(\"_Survey_Familiarity_F1\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F2\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F3\")\n",
    "        or col.endswith(\"_Survey_Familiarity_C1\")\n",
    "    ]\n",
    "\n",
    "    respondent_groups = (\n",
    "        uv_stage1\n",
    "        .loc[:, [\"respondent\", \"group\"]]\n",
    "        .assign(group=lambda df: df[\"group\"].astype(str).str.strip().str.upper())\n",
    "        .set_index(\"respondent\")\n",
    "        .to_dict()\n",
    "    )[\"group\"]\n",
    "\n",
    "    title_normalization = {\n",
    "        \"mad max fury road\": \"Mad Max\",\n",
    "        \"mad max\": \"Mad Max\",\n",
    "        \"the town\": \"The Town\",\n",
    "        \"abbot elementary\": \"Abbot Elementary\",\n",
    "        \"abbott elementary\": \"Abbot Elementary\",\n",
    "    }\n",
    "\n",
    "    def canonicalize_title(raw_title: str) -> str:\n",
    "        cleaned = str(raw_title).strip()\n",
    "        return title_normalization.get(cleaned.lower(), cleaned)\n",
    "\n",
    "    stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "    stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"([A-F])\", expand=False)\n",
    "    stimulus_map[\"title_clean\"] = stimulus_map[\"title\"].astype(str).str.strip()\n",
    "\n",
    "    group_title_form_lookup = {}\n",
    "    default_form_per_title = {}\n",
    "\n",
    "    for row in stimulus_map.itertuples():\n",
    "        if pd.isna(row.group_letter) or pd.isna(row.title_clean) or pd.isna(row.form):\n",
    "            continue\n",
    "        canonical_title = canonicalize_title(row.title_clean)\n",
    "        form_value = str(row.form).title()\n",
    "        group_title_form_lookup[(row.group_letter, canonical_title)] = form_value\n",
    "        default_form_per_title.setdefault(canonical_title, form_value)\n",
    "\n",
    "    screening_records = []\n",
    "\n",
    "    for _, row in screening_raw.iterrows():\n",
    "        respondent_id = row.get(\"respondent\")\n",
    "        if respondent_id is None:\n",
    "            continue\n",
    "        respondent_id = str(respondent_id).strip()\n",
    "        group_letter = respondent_groups.get(respondent_id)\n",
    "        for column in screening_value_columns:\n",
    "            value = row.get(column)\n",
    "            if pd.isna(value) or value == \"\":\n",
    "                continue\n",
    "            base_part, _, suffix_part = column.partition(\"_Survey_Familiarity_\")\n",
    "            if not suffix_part:\n",
    "                continue\n",
    "            question_code = suffix_part.strip()\n",
    "            canonical_title = canonicalize_title(base_part.strip())\n",
    "            form_value = None\n",
    "            if group_letter:\n",
    "                form_value = group_title_form_lookup.get((group_letter, canonical_title))\n",
    "            if form_value is None:\n",
    "                form_value = default_form_per_title.get(canonical_title, \"Long\")\n",
    "            target_column = f\"{form_value}_{canonical_title}_Screening_Familiarity_{question_code}\"\n",
    "            screening_records.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"target_column\": target_column,\n",
    "                \"value\": pd.to_numeric(value, errors=\"coerce\")\n",
    "            })\n",
    "\n",
    "    if screening_records:\n",
    "        screening_features = (\n",
    "            pd.DataFrame(screening_records)\n",
    "            .pivot_table(index=\"respondent\", columns=\"target_column\", values=\"value\", aggfunc=\"first\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        screening_features.columns.name = None\n",
    "        survey_features = survey_features.merge(screening_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "survey_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef38cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 1 Stage 2 issues to uv_stage2_issues.csv.\n",
      "Stage 2 survey features saved to uv_stage2_features.csv with 82 respondents and 365 feature columns.\n",
      "Open-ended responses archived to uv_stage2_open_ended.csv.\n",
      "Unified view with Stage 2 survey data exported to uv_stage2.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>66</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>61</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0   003_104.csv     A        104  10/16/2025   18:09:03   59     44-59   \n",
       "1   002_106.csv     A        106  10/16/2025   19:35:05   30     28-43   \n",
       "2   001_116.csv     A        116  10/18/2025   12:37:40   19     18-27   \n",
       "3    006_14.csv     A         14  10/11/2025   09:32:42   33     28-43   \n",
       "4     007_3.csv     A          3  10/10/2025   09:19:22   34     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   005_50.csv     F         50  10/14/2025   09:54:03   63     60-69   \n",
       "79   004_60.csv     F         60  10/15/2025   09:34:06   66     60-69   \n",
       "80   003_70.csv     F         70  10/16/2025   09:49:14   61     60-69   \n",
       "81   002_85.csv     F         85  10/17/2025   14:37:41   34     28-43   \n",
       "82   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "\n",
       "    gender                      ethnicity                income_group  ...  \\\n",
       "0     Male                          White    $60,000 or more per year  ...   \n",
       "1     Male                          White    $60,000 or more per year  ...   \n",
       "2     Male                          White  $35,000  $60,000 per year  ...   \n",
       "3     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year  ...   \n",
       "4   Female                          White    $60,000 or more per year  ...   \n",
       "..     ...                            ...                         ...  ...   \n",
       "78    Male         Black/African American    $60,000 or more per year  ...   \n",
       "79    Male                          White  $35,000  $60,000 per year  ...   \n",
       "80  Female         Black/African American  $35,000  $60,000 per year  ...   \n",
       "81  Female                          White    $60,000 or more per year  ...   \n",
       "82  Female                          White    $60,000 or more per year  ...   \n",
       "\n",
       "   Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                     1.0   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     1.0   \n",
       "4                                     NaN   \n",
       "..                                    ...   \n",
       "78                                    NaN   \n",
       "79                                    NaN   \n",
       "80                                    NaN   \n",
       "81                                    NaN   \n",
       "82                                    NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               0.0   \n",
       "1                                               4.0   \n",
       "2                                               1.0   \n",
       "3                                               8.0   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               0.0   \n",
       "1                                               3.0   \n",
       "2                                               1.0   \n",
       "3                                               4.0   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               NaN   \n",
       "1                                               1.0   \n",
       "2                                               NaN   \n",
       "3                                               4.0   \n",
       "4                                               NaN   \n",
       "..                                              ...   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                     1.0   \n",
       "1                                     4.0   \n",
       "2                                     4.0   \n",
       "3                                     8.0   \n",
       "4                                     5.0   \n",
       "..                                    ...   \n",
       "78                                    6.0   \n",
       "79                                    1.0   \n",
       "80                                    1.0   \n",
       "81                                    4.0   \n",
       "82                                    4.0   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                     1.0   \n",
       "1                                     3.0   \n",
       "2                                     3.0   \n",
       "3                                     4.0   \n",
       "4                                     3.0   \n",
       "..                                    ...   \n",
       "78                                    4.0   \n",
       "79                                    1.0   \n",
       "80                                    1.0   \n",
       "81                                    3.0   \n",
       "82                                    3.0   \n",
       "\n",
       "   Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                     NaN   \n",
       "1                                     1.0   \n",
       "2                                     1.0   \n",
       "3                                     4.0   \n",
       "4                                     2.0   \n",
       "..                                    ...   \n",
       "78                                    2.0   \n",
       "79                                    NaN   \n",
       "80                                    NaN   \n",
       "81                                    1.0   \n",
       "82                                    1.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "..                                     ...   \n",
       "78                                     6.0   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     0.0   \n",
       "82                                     4.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "..                                     ...   \n",
       "78                                     3.0   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "81                                     0.0   \n",
       "82                                     3.0   \n",
       "\n",
       "   Short_The Town_Screening_Familiarity_F2  \n",
       "0                                      NaN  \n",
       "1                                      NaN  \n",
       "2                                      NaN  \n",
       "3                                      NaN  \n",
       "4                                      NaN  \n",
       "..                                     ...  \n",
       "78                                     3.0  \n",
       "79                                     NaN  \n",
       "80                                     NaN  \n",
       "81                                     NaN  \n",
       "82                                     1.0  \n",
       "\n",
       "[83 rows x 382 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stage1_path = results_dir / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "survey_features = survey_features.copy()\n",
    "survey_features[\"respondent\"] = survey_features[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage2 = uv_stage1_base.merge(survey_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "issues_records: list[dict] = []\n",
    "feature_ids = survey_features[\"respondent\"].dropna().astype(str).str.strip()\n",
    "expected_ids = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "missing_ids = sorted(set(expected_ids) - set(feature_ids))\n",
    "for respondent in missing_ids:\n",
    "    issues_records.append({\n",
    "        \"respondent\": respondent,\n",
    "        \"issue\": \"stage2_features_missing\",\n",
    "    })\n",
    "\n",
    "duplicate_mask = feature_ids.duplicated(keep=False)\n",
    "if duplicate_mask.any():\n",
    "    duplicates = feature_ids[duplicate_mask]\n",
    "    for respondent in sorted(duplicates.unique()):\n",
    "        count = int((feature_ids == respondent).sum())\n",
    "        issues_records.append({\n",
    "            \"respondent\": respondent,\n",
    "            \"issue\": f\"stage2_duplicate_records_count_{count}\",\n",
    "        })\n",
    "\n",
    "issues_df = pd.DataFrame(issues_records)\n",
    "\n",
    "features_path = safe_write_csv(survey_features, results_dir / \"uv_stage2_features.csv\")\n",
    "open_ended_path = safe_write_csv(survey_open_ended, results_dir / \"uv_stage2_open_ended.csv\")\n",
    "uv_path = safe_write_csv(uv_stage2, results_dir / \"uv_stage2.csv\")\n",
    "\n",
    "issues_path = results_dir / \"uv_stage2_issues.csv\"\n",
    "if issues_df.empty:\n",
    "    if issues_path.exists():\n",
    "        issues_path.unlink()\n",
    "    print(\"No Stage 2 survey issues detected.\")\n",
    "else:\n",
    "    safe_write_csv(issues_df, issues_path)\n",
    "    print(f\"Logged {issues_df.shape[0]} Stage 2 issues to {issues_path.name}.\")\n",
    "\n",
    "print(\n",
    "    f\"Stage 2 survey features saved to {features_path.name} with {survey_features.shape[0]} respondents and \"\n",
    "    f\"{survey_features.shape[1] - 1} feature columns.\"\n",
    ")\n",
    "print(f\"Open-ended responses archived to {open_ended_path.name}.\")\n",
    "print(f\"Unified view with Stage 2 survey data exported to {uv_path.name}.\")\n",
    "\n",
    "uv_stage2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf78dfd",
   "metadata": {},
   "source": [
    "### Stage 2 Long-Form Export\n",
    "Transforms the Stage 2 survey outputs into a respondent-by-question table with metadata joins and derived score rows so downstream work can consume a single tidy feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ce4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "DERIVED_PREFIX = \"DERIVED_STAGE2_\"\n",
    "feature_pattern = re.compile(r\"^(Long|Short)_(.*?)_(Survey|Screening)_(.*)$\")\n",
    "\n",
    "\n",
    "def load_or_read(frame_name: str, path: Path) -> pd.DataFrame:\n",
    "    existing = globals().get(frame_name)\n",
    "    if existing is not None:\n",
    "        return existing.copy()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Required artifact {path.name} not found at {path}.\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def standardize_respondent(series: pd.Series) -> pd.Series:\n",
    "    cleaned = series.astype(str).str.strip().replace({\"nan\": np.nan, \"\": np.nan})\n",
    "    cleaned = cleaned.str.replace(r\"\\\\.0$\", \"\", regex=True)\n",
    "    return cleaned.fillna(\"\").map(lambda value: value.zfill(3) if value else value)\n",
    "\n",
    "\n",
    "def canonicalize_title(value: str) -> str:\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    fixes = {\n",
    "        \"abbott elementary\": \"Abbot Elementary\",\n",
    "        \"abbot elementary\": \"Abbot Elementary\",\n",
    "        \"mad max fury road\": \"Mad Max\",\n",
    "        \"mad max\": \"Mad Max\",\n",
    "        \"the town\": \"The Town\",\n",
    "    }\n",
    "    return fixes.get(text.lower(), text)\n",
    "\n",
    "\n",
    "def build_stage2_metadata(rename_map_df: pd.DataFrame, questions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    base = rename_map_df.copy()\n",
    "    base[\"target_column\"] = base[\"target_column\"].astype(str).str.strip()\n",
    "    base = base.loc[base[\"target_column\"].ne(\"\")]\n",
    "    base[\"question_code\"] = base[\"question_code\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    questions = questions_df.copy()\n",
    "    questions[\"question_code\"] = questions[\"question_code\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    merged = base.merge(questions, on=\"question_code\", how=\"left\", suffixes=(\"\", \"_question\"))\n",
    "    merged[\"question_type\"] = merged[\"question_type\"].fillna(\"likert\").str.lower()\n",
    "    merged[\"subscale\"] = merged[\"subscale\"].fillna(\"\")\n",
    "    merged[\"polarity\"] = merged[\"polarity\"].fillna(\"\").str.lower()\n",
    "    merged[\"question_text\"] = merged[\"question_text\"].fillna(merged[\"suffix_text\"]).fillna(\"\")\n",
    "\n",
    "    extracted = merged[\"target_column\"].str.extract(r\"^(Long|Short)_(.*?)_Survey_(.*)$\")\n",
    "    merged[\"stimulus_form_token\"] = extracted[0].fillna(\"\")\n",
    "    merged[\"stimulus_title_raw\"] = extracted[1].fillna(\"\")\n",
    "    merged[\"stimulus_form\"] = merged[\"stimulus_form_token\"].map({\"Long\": \"Long Form\", \"Short\": \"Short Form\"}).fillna(\"\")\n",
    "    merged[\"stimulus_title\"] = merged[\"stimulus_title_raw\"].apply(canonicalize_title)\n",
    "    merged[\"category\"] = merged[\"topic\"].fillna(\"\").str.lower()\n",
    "    merged[\"questionnaire\"] = \"Stage 2 Survey\"\n",
    "    return merged\n",
    "\n",
    "\n",
    "def parse_feature_column(column: str) -> dict | None:\n",
    "    match = feature_pattern.match(column)\n",
    "    if not match:\n",
    "        return None\n",
    "    form_token, title_raw, instrument, tail = match.groups()\n",
    "    parts = tail.split(\"_\")\n",
    "    metric_base = parts[0]\n",
    "    stat = \"_\".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "    metric_label = metric_base if not stat else f\"{metric_base}_{stat}\"\n",
    "    question_code = f\"{DERIVED_PREFIX}{metric_label}\"\n",
    "\n",
    "    subscale = metric_base.replace(\"Composite\", \"\") or metric_base\n",
    "    if metric_base.lower().startswith(\"familiarity\"):\n",
    "        subscale = \"Familiarity\"\n",
    "    elif metric_base.lower() == \"nostalgia\":\n",
    "        subscale = \"Nostalgia\"\n",
    "\n",
    "    category = \"familiarity\" if \"familiarity\" in metric_base.lower() else \"enjoyment\"\n",
    "    if instrument == \"Screening\":\n",
    "        category = \"screening_familiarity\"\n",
    "\n",
    "    return {\n",
    "        \"stimulus_form\": \"Long Form\" if form_token == \"Long\" else \"Short Form\",\n",
    "        \"stimulus_title\": canonicalize_title(title_raw),\n",
    "        \"questionnaire\": f\"Stage 2 {instrument}\",\n",
    "        \"question_code\": question_code.upper(),\n",
    "        \"question_text\": f\"{instrument} {metric_base.replace('Composite', ' Composite')} {stat}\".strip(),\n",
    "        \"question_type\": \"derived_metric\",\n",
    "        \"subscale\": subscale.title(),\n",
    "        \"category\": category.replace(\"_\", \" \"),\n",
    "        \"score_stat\": stat,\n",
    "        \"metric_base\": metric_base,\n",
    "        \"instrument\": instrument,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4868678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata columns missing from Stage 2 wide export: 23 (e.g., General_Abbot Elementary (Abbot Elementary vs Mad Max)_Survey_Familiarity_F3, General_Abbot Elementary (Abbot Elementary vs Mad Max)_Survey_Familiarity_F4, General_Abbot Elementary (The Town vs Abbot Elementary)_Survey_Familiarity_F3, General_Abbot Elementary (The Town vs Abbot Elementary)_Survey_Familiarity_F4, General_Abbot Elementary vs Mad Max_Survey_Familiarity_F5)\n",
      "Skipped 8 Stage 2 feature column(s) that do not map to the long-form schema.\n",
      "Stage 2 long-form export contains 18,367 rows across 83 respondents and 69 unique question codes.\n",
      "Stage 2 issues propagated for 1 respondent(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submitted_timestamp</th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>stimulus_form</th>\n",
       "      <th>stimulus_title</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>...</th>\n",
       "      <th>response_raw</th>\n",
       "      <th>response_clean</th>\n",
       "      <th>response_numeric</th>\n",
       "      <th>score_value</th>\n",
       "      <th>score_method</th>\n",
       "      <th>score_stat</th>\n",
       "      <th>score_explanation</th>\n",
       "      <th>value_kind</th>\n",
       "      <th>data_quality_flag</th>\n",
       "      <th>issue_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>104</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>106</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaT</td>\n",
       "      <td>116</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "      <td>014</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "      <td>003</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaT</td>\n",
       "      <td>052</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaT</td>\n",
       "      <td>055</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaT</td>\n",
       "      <td>008</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaT</td>\n",
       "      <td>081</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaT</td>\n",
       "      <td>083</td>\n",
       "      <td>A</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Long Form</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage2_likert</td>\n",
       "      <td></td>\n",
       "      <td>Likert item scored on a 1-5 scale with polarit...</td>\n",
       "      <td>response</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  submitted_timestamp respondent group survey_group survey_study  \\\n",
       "0                 NaT        104     A      Default      Group A   \n",
       "1                 NaT        106     A      Default      Group A   \n",
       "2                 NaT        116     A      Default      Group A   \n",
       "3                 NaT        014     A      Default      Group A   \n",
       "4                 NaT        003     A      Default      Group A   \n",
       "5                 NaT        052     A      Default      Group A   \n",
       "6                 NaT        055     A      Default      Group A   \n",
       "7                 NaT        008     A      Default      Group A   \n",
       "8                 NaT        081     A      Default      Group A   \n",
       "9                 NaT        083     A      Default      Group A   \n",
       "\n",
       "  survey_gender  survey_age stimulus_form    stimulus_title Short Form  ...  \\\n",
       "0          MALE        59.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "1          MALE        30.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "2          MALE        19.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "3          MALE        33.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "4        FEMALE        34.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "5        FEMALE        50.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "6        FEMALE        37.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "7         OTHER        51.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "8        FEMALE        24.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "9          MALE        69.0     Long Form  Abbot Elementary    Mad Max  ...   \n",
       "\n",
       "  response_raw response_clean response_numeric score_value   score_method  \\\n",
       "0          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "1          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "2          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "3          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "4          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "5          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "6          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "7          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "8          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "9          NaN            NaN              NaN         NaN  stage2_likert   \n",
       "\n",
       "  score_stat                                  score_explanation value_kind  \\\n",
       "0             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "1             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "2             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "3             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "4             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "5             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "6             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "7             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "8             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "9             Likert item scored on a 1-5 scale with polarit...   response   \n",
       "\n",
       "  data_quality_flag issue_code  \n",
       "0             false        NaN  \n",
       "1             false        NaN  \n",
       "2             false        NaN  \n",
       "3             false        NaN  \n",
       "4             false        NaN  \n",
       "5             false        NaN  \n",
       "6             false        NaN  \n",
       "7             false        NaN  \n",
       "8             false        NaN  \n",
       "9             false        NaN  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = project_root / \"results\"\n",
    "\n",
    "stage2_wide_df = load_or_read(\"uv_stage2\", results_dir / \"uv_stage2.csv\")\n",
    "stage2_wide_df[\"respondent\"] = standardize_respondent(stage2_wide_df[\"respondent\"])\n",
    "if \"group\" in stage2_wide_df.columns:\n",
    "    stage2_wide_df[\"group\"] = stage2_wide_df[\"group\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "stage2_features_df = load_or_read(\"survey_features\", results_dir / \"uv_stage2_features.csv\")\n",
    "stage2_features_df[\"respondent\"] = standardize_respondent(stage2_features_df[\"respondent\"])\n",
    "\n",
    "open_ended_path = results_dir / \"uv_stage2_open_ended.csv\"\n",
    "if \"survey_open_ended\" in globals():\n",
    "    stage2_open_ended_df = load_or_read(\"survey_open_ended\", open_ended_path)\n",
    "elif open_ended_path.exists():\n",
    "    stage2_open_ended_df = pd.read_csv(open_ended_path)\n",
    "else:\n",
    "    stage2_open_ended_df = pd.DataFrame()\n",
    "if not stage2_open_ended_df.empty:\n",
    "    stage2_open_ended_df[\"respondent\"] = standardize_respondent(stage2_open_ended_df[\"respondent\"])\n",
    "\n",
    "stage2_issues_path = results_dir / \"uv_stage2_issues.csv\"\n",
    "stage2_issues_df = pd.read_csv(stage2_issues_path) if stage2_issues_path.exists() else pd.DataFrame()\n",
    "if not stage2_issues_df.empty:\n",
    "    stage2_issues_df[\"respondent\"] = standardize_respondent(stage2_issues_df[\"respondent\"])\n",
    "\n",
    "stage1_df = load_or_read(\"uv_stage1_base\", results_dir / \"uv_stage1.csv\")\n",
    "stage1_df[\"respondent\"] = standardize_respondent(stage1_df[\"respondent\"])\n",
    "stage1_df[\"group\"] = stage1_df[\"group\"].astype(str).str.strip().str.upper()\n",
    "stage1_lookup = stage1_df[[\"respondent\", \"group\", \"Short Form\", \"Long Form\"]].drop_duplicates(\"respondent\", keep=\"last\")\n",
    "\n",
    "rename_map_df = pd.read_csv(project_root / \"data\" / \"survey_column_rename.csv\")\n",
    "questions_df = pd.read_csv(project_root / \"data\" / \"survey_questions.csv\")\n",
    "metadata_df = build_stage2_metadata(rename_map_df, questions_df)\n",
    "\n",
    "response_columns = sorted(set(metadata_df[\"target_column\"]).intersection(stage2_wide_df.columns))\n",
    "missing_columns = sorted(set(metadata_df[\"target_column\"]) - set(response_columns))\n",
    "if missing_columns:\n",
    "    sample_missing = \", \".join(missing_columns[:5])\n",
    "    print(\n",
    "        f\"Metadata columns missing from Stage 2 wide export: {len(missing_columns)}\"\n",
    "        + (f\" (e.g., {sample_missing})\" if sample_missing else \"\")\n",
    "    )\n",
    "\n",
    "id_candidates = [\n",
    "    \"respondent\",\n",
    "    \"group\",\n",
    "    \"survey_group\",\n",
    "    \"survey_study\",\n",
    "    \"survey_gender\",\n",
    "    \"survey_age\",\n",
    "    \"survey_file\",\n",
    "    \"source_file\",\n",
    "    \"Short Form\",\n",
    "    \"Long Form\",\n",
    "]\n",
    "id_vars = [col for col in id_candidates if col in stage2_wide_df.columns]\n",
    "\n",
    "response_long = stage2_wide_df.melt(\n",
    "    id_vars=id_vars,\n",
    "    value_vars=response_columns,\n",
    "    var_name=\"target_column\",\n",
    "    value_name=\"response_raw\",\n",
    ")\n",
    "response_long[\"respondent\"] = standardize_respondent(response_long[\"respondent\"])\n",
    "response_long[\"response_clean\"] = response_long[\"response_raw\"].apply(\n",
    "    lambda value: clean_response(value) if pd.notna(value) else value\n",
    ")\n",
    "\n",
    "numeric_columns = [col for col in response_columns if col in stage2_features_df.columns]\n",
    "if numeric_columns:\n",
    "    numeric_long = stage2_features_df.melt(\n",
    "        id_vars=[\"respondent\"],\n",
    "        value_vars=numeric_columns,\n",
    "        var_name=\"target_column\",\n",
    "        value_name=\"response_numeric\",\n",
    "    )\n",
    "    numeric_long[\"respondent\"] = standardize_respondent(numeric_long[\"respondent\"])\n",
    "else:\n",
    "    numeric_long = pd.DataFrame(columns=[\"respondent\", \"target_column\", \"response_numeric\"])\n",
    "\n",
    "long_df = response_long.merge(numeric_long, on=[\"respondent\", \"target_column\"], how=\"left\")\n",
    "\n",
    "meta_cols = [\n",
    "    \"target_column\",\n",
    "    \"question_code\",\n",
    "    \"question_text\",\n",
    "    \"question_type\",\n",
    "    \"subscale\",\n",
    "    \"polarity\",\n",
    "    \"stimulus_form\",\n",
    "    \"stimulus_title\",\n",
    "    \"category\",\n",
    "    \"questionnaire\",\n",
    "]\n",
    "long_df = long_df.merge(metadata_df[meta_cols], on=\"target_column\", how=\"left\")\n",
    "\n",
    "long_df[\"stimulus_form\"] = long_df[\"stimulus_form\"].replace(\"\", np.nan)\n",
    "long_df[\"stimulus_form\"] = long_df[\"stimulus_form\"].fillna(\n",
    "    long_df[\"target_column\"].str.extract(r\"^(Long|Short)_\")[0].map({\"Long\": \"Long Form\", \"Short\": \"Short Form\"})\n",
    ")\n",
    "long_df[\"stimulus_title\"] = long_df[\"stimulus_title\"].replace(\"\", np.nan)\n",
    "long_df[\"stimulus_title\"] = long_df[\"stimulus_title\"].fillna(\n",
    "    long_df[\"target_column\"].str.extract(r\"^(?:Long|Short)_(.*?)_Survey_\")[0].apply(canonicalize_title)\n",
    ")\n",
    "\n",
    "long_df = long_df.merge(stage1_lookup, on=\"respondent\", how=\"left\", suffixes=(\"\", \"_stage1\"))\n",
    "for column in [\"group\", \"Short Form\", \"Long Form\"]:\n",
    "    stage1_col = f\"{column}_stage1\"\n",
    "    if stage1_col in long_df.columns:\n",
    "        long_df[column] = long_df[column].fillna(long_df[stage1_col])\n",
    "        long_df.drop(columns=[stage1_col], inplace=True)\n",
    "\n",
    "long_df[\"group\"] = long_df.get(\"group\", \"\").astype(str).str.strip().str.upper()\n",
    "long_df[\"question_code\"] = long_df[\"question_code\"].fillna(\"\").str.upper()\n",
    "missing_code_mask = long_df[\"question_code\"].eq(\"\")\n",
    "long_df.loc[missing_code_mask, \"question_code\"] = long_df.loc[missing_code_mask, \"target_column\"].apply(\n",
    "    lambda col: f\"UNKNOWN_{col}\"\n",
    ")\n",
    "long_df[\"question_type\"] = long_df[\"question_type\"].fillna(\"unknown\").str.lower()\n",
    "long_df[\"subscale\"] = long_df[\"subscale\"].fillna(\"\").str.strip()\n",
    "long_df[\"category\"] = long_df[\"category\"].fillna(\"\").str.replace(\"_\", \" \").str.strip()\n",
    "long_df[\"polarity\"] = long_df[\"polarity\"].fillna(\"\")\n",
    "long_df[\"questionnaire\"] = long_df[\"questionnaire\"].fillna(\"Stage 2 Survey\")\n",
    "long_df[\"source_path\"] = long_df.get(\"survey_file\")\n",
    "long_df[\"submitted_timestamp\"] = pd.NaT\n",
    "long_df[\"accuracy\"] = \"\"\n",
    "long_df[\"value_kind\"] = \"response\"\n",
    "long_df[\"score_stat\"] = \"\"\n",
    "\n",
    "\n",
    "def infer_score_method(question_type: str) -> str:\n",
    "    if question_type == \"likert\":\n",
    "        return \"stage2_likert\"\n",
    "    if question_type in {\"familiarity\", \"binary\"}:\n",
    "        return \"stage2_familiarity\"\n",
    "    if question_type == \"open ended\":\n",
    "        return \"stage2_open_ended\"\n",
    "    return \"stage2_response\"\n",
    "\n",
    "\n",
    "long_df[\"score_method\"] = long_df[\"question_type\"].apply(infer_score_method)\n",
    "long_df[\"score_value\"] = long_df[\"response_numeric\"]\n",
    "EXPLANATION_MAP = {\n",
    "    \"stage2_likert\": \"Likert item scored on a 1-5 scale with polarity applied.\",\n",
    "    \"stage2_familiarity\": \"Familiarity item normalized per Stage 2 scoring pipeline.\",\n",
    "    \"stage2_open_ended\": \"Open-ended text captured verbatim.\",\n",
    "    \"stage2_response\": \"Stage 2 response value without additional scoring.\",\n",
    "}\n",
    "long_df[\"score_explanation\"] = long_df[\"score_method\"].map(EXPLANATION_MAP)\n",
    "\n",
    "response_column_set = set(response_columns)\n",
    "derived_columns = [\n",
    "    col for col in stage2_features_df.columns if col not in response_column_set.union({\"respondent\"})\n",
    "]\n",
    "derived_records = []\n",
    "if derived_columns:\n",
    "    derived_long = stage2_features_df.melt(\n",
    "        id_vars=[\"respondent\"],\n",
    "        value_vars=derived_columns,\n",
    "        var_name=\"feature_name\",\n",
    "        value_name=\"score_value\",\n",
    "    ).dropna(subset=[\"score_value\"], how=\"all\")\n",
    "    if not derived_long.empty:\n",
    "        derived_long[\"respondent\"] = standardize_respondent(derived_long[\"respondent\"])\n",
    "        parsed_meta = derived_long[\"feature_name\"].apply(parse_feature_column)\n",
    "        valid_mask = parsed_meta.notna()\n",
    "        skipped_columns = sorted(set(derived_columns) - set(derived_long.loc[valid_mask, \"feature_name\"].unique()))\n",
    "        if skipped_columns:\n",
    "            print(\n",
    "                f\"Skipped {len(skipped_columns)} Stage 2 feature column(s) that do not map to the long-form schema.\"\n",
    "            )\n",
    "        if valid_mask.any():\n",
    "            derived_augmented = pd.concat(\n",
    "                [\n",
    "                    derived_long.loc[valid_mask].reset_index(drop=True),\n",
    "                    pd.DataFrame(parsed_meta[valid_mask].tolist()),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            derived_augmented = derived_augmented.merge(\n",
    "                stage1_lookup, on=\"respondent\", how=\"left\", suffixes=(\"\", \"_stage1\")\n",
    "            )\n",
    "            for column in [\"group\", \"Short Form\", \"Long Form\"]:\n",
    "                stage1_col = f\"{column}_stage1\"\n",
    "                if stage1_col in derived_augmented.columns:\n",
    "                    derived_augmented[column] = derived_augmented[column].fillna(derived_augmented[stage1_col])\n",
    "                    derived_augmented.drop(columns=[stage1_col], inplace=True)\n",
    "            derived_augmented[\"group\"] = derived_augmented.get(\"group\", \"\").astype(str).str.strip().str.upper()\n",
    "            derived_augmented[\"stimulus_title\"] = derived_augmented[\"stimulus_title\"].apply(canonicalize_title)\n",
    "            derived_augmented[\"question_text\"] = derived_augmented[\"question_text\"].str.strip()\n",
    "            derived_augmented[\"question_type\"] = derived_augmented[\"question_type\"].str.lower()\n",
    "            derived_augmented[\"subscale\"] = derived_augmented[\"subscale\"].str.strip()\n",
    "            derived_augmented[\"category\"] = derived_augmented[\"category\"].str.strip()\n",
    "            derived_augmented[\"score_method\"] = derived_augmented.apply(\n",
    "                lambda row: \"stage2_feature_\"\n",
    "                + row[\"instrument\"].lower()\n",
    "                + (f\"_{row['score_stat'].lower()}\" if row[\"score_stat\"] else \"\"),\n",
    "                axis=1,\n",
    "            )\n",
    "            derived_augmented[\"score_explanation\"] = derived_augmented.apply(\n",
    "                lambda row: f\"{row['instrument']} {row['metric_base']} metric\"\n",
    "                + (f\" ({row['score_stat']})\" if row['score_stat'] else \"\"),\n",
    "                axis=1,\n",
    "            )\n",
    "            derived_augmented[\"response_raw\"] = np.nan\n",
    "            derived_augmented[\"response_clean\"] = np.nan\n",
    "            derived_augmented[\"response_numeric\"] = derived_augmented[\"score_value\"]\n",
    "            derived_augmented[\"value_kind\"] = \"score\"\n",
    "            derived_augmented[\"submitted_timestamp\"] = pd.NaT\n",
    "            derived_augmented[\"accuracy\"] = \"\"\n",
    "            derived_augmented[\"source_path\"] = np.nan\n",
    "            derived_augmented[\"survey_group\"] = np.nan\n",
    "            derived_augmented[\"survey_study\"] = np.nan\n",
    "            derived_augmented[\"survey_gender\"] = np.nan\n",
    "            derived_augmented[\"survey_age\"] = np.nan\n",
    "            derived_augmented[\"target_column\"] = derived_augmented[\"feature_name\"]\n",
    "            derived_records.append(\n",
    "                derived_augmented.drop(columns=[\"feature_name\", \"metric_base\", \"instrument\"])\n",
    "            )\n",
    "\n",
    "if derived_records:\n",
    "    derived_df = pd.concat(derived_records, ignore_index=True)\n",
    "else:\n",
    "    derived_df = pd.DataFrame(columns=long_df.columns)\n",
    "\n",
    "stage2_long = pd.concat([long_df, derived_df], ignore_index=True, sort=False)\n",
    "\n",
    "if not stage2_issues_df.empty:\n",
    "    issue_map = (\n",
    "        stage2_issues_df.groupby(\"respondent\")[\"issue\"].apply(\n",
    "            lambda values: \"; \".join(sorted({str(v).strip() for v in values if pd.notna(v)}))\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"issue\": \"issue_code\"})\n",
    "    )\n",
    "    stage2_long = stage2_long.merge(issue_map, on=\"respondent\", how=\"left\")\n",
    "else:\n",
    "    stage2_long[\"issue_code\"] = np.nan\n",
    "stage2_long[\"data_quality_flag\"] = stage2_long[\"issue_code\"].notna().map({True: \"true\", False: \"false\"})\n",
    "\n",
    "stage2_long[\"stimulus_title\"] = stage2_long[\"stimulus_title\"].apply(canonicalize_title)\n",
    "stage2_long[\"respondent\"] = standardize_respondent(stage2_long[\"respondent\"])\n",
    "\n",
    "column_order = [\n",
    "    \"submitted_timestamp\",\n",
    "    \"respondent\",\n",
    "    \"group\",\n",
    "    \"survey_group\",\n",
    "    \"survey_study\",\n",
    "    \"survey_gender\",\n",
    "    \"survey_age\",\n",
    "    \"stimulus_form\",\n",
    "    \"stimulus_title\",\n",
    "    \"Short Form\",\n",
    "    \"Long Form\",\n",
    "    \"questionnaire\",\n",
    "    \"source_path\",\n",
    "    \"question_code\",\n",
    "    \"target_column\",\n",
    "    \"question_text\",\n",
    "    \"question_type\",\n",
    "    \"subscale\",\n",
    "    \"category\",\n",
    "    \"accuracy\",\n",
    "    \"response_raw\",\n",
    "    \"response_clean\",\n",
    "    \"response_numeric\",\n",
    "    \"score_value\",\n",
    "    \"score_method\",\n",
    "    \"score_stat\",\n",
    "    \"score_explanation\",\n",
    "    \"value_kind\",\n",
    "    \"data_quality_flag\",\n",
    "    \"issue_code\",\n",
    "]\n",
    "for column in column_order:\n",
    "    if column not in stage2_long.columns:\n",
    "        stage2_long[column] = np.nan\n",
    "stage2_long = stage2_long.loc[:, column_order]\n",
    "\n",
    "response_dupes = stage2_long.loc[\n",
    "    (stage2_long[\"value_kind\"] == \"response\")\n",
    "    & stage2_long.duplicated(subset=[\"respondent\", \"target_column\"], keep=False)\n",
    "]\n",
    "missing_fallback = stage2_long[\"question_code\"].str.startswith(\"UNKNOWN_\").sum()\n",
    "\n",
    "long_output_path = results_dir / \"stage2_enjoyment_responses_long.csv\"\n",
    "preview_path = (\n",
    "    project_root\n",
    "    / \"tasks\"\n",
    "    / \"13-11-25 Wrap up enjoyment data\"\n",
    "    / \"stage2_enjoyment_responses_long.csv\"\n",
    ")\n",
    "preview_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "long_output_path = safe_write_csv(stage2_long, long_output_path)\n",
    "preview_path = safe_write_csv(stage2_long, preview_path)\n",
    "\n",
    "print(\n",
    "    f\"Stage 2 long-form export contains {stage2_long.shape[0]:,} rows across \"\n",
    "    f\"{stage2_long['respondent'].nunique()} respondents and \"\n",
    "    f\"{stage2_long['question_code'].nunique()} unique question codes.\"\n",
    ")\n",
    "if not response_dupes.empty:\n",
    "    print(\n",
    "        f\"Warning: {response_dupes.shape[0]} response row(s) share the same respondent/target_column combination.\"\n",
    "    )\n",
    "if missing_fallback:\n",
    "    print(f\"Warning: {missing_fallback} row(s) used fallback question codes.\")\n",
    "if not stage2_issues_df.empty:\n",
    "    print(\n",
    "        f\"Stage 2 issues propagated for {stage2_issues_df['respondent'].nunique()} respondent(s).\"\n",
    "    )\n",
    "\n",
    "stage2_long.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac5765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>respondents_in_long</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respondents_in_wide</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respondent_sets_match</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response_rows</th>\n",
       "      <td>7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expected_response_slots</th>\n",
       "      <td>7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duplicate_response_pairs</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_null_responses</th>\n",
       "      <td>2478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>empty_responses</th>\n",
       "      <td>5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_rows</th>\n",
       "      <td>10814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_column_coverage</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issues_flagged</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          value\n",
       "respondents_in_long          83\n",
       "respondents_in_wide          83\n",
       "respondent_sets_match      True\n",
       "response_rows              7553\n",
       "expected_response_slots    7553\n",
       "duplicate_response_pairs      0\n",
       "non_null_responses         2478\n",
       "empty_responses            5075\n",
       "score_rows                10814\n",
       "target_column_coverage     True\n",
       "issues_flagged               91"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation summary written to tasks\\13-11-25 Wrap up enjoyment data\\stage2_validation_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Stage 2 validation summary to confirm export integrity.\n",
    "stage2_long_responses = stage2_long[stage2_long[\"value_kind\"] == \"response\"].copy()\n",
    "stage2_long_scores = stage2_long[stage2_long[\"value_kind\"] == \"score\"].copy()\n",
    "\n",
    "expected_response_rows = len(stage2_wide_df) * len(response_columns)\n",
    "actual_response_rows = stage2_long_responses.shape[0]\n",
    "non_null_responses = stage2_long_responses[\"response_clean\"].notna().sum()\n",
    "empty_responses = actual_response_rows - non_null_responses\n",
    "duplicate_response_pairs = stage2_long_responses.duplicated([\"respondent\", \"target_column\"], keep=False).sum()\n",
    "\n",
    "respondent_sets_match = set(stage2_long_responses[\"respondent\"].unique()) == set(stage2_wide_df[\"respondent\"].unique())\n",
    "target_column_coverage = set(stage2_long_responses[\"target_column\"].unique()) == set(response_columns)\n",
    "\n",
    "validation_summary = {\n",
    "    \"respondents_in_long\": int(stage2_long_responses[\"respondent\"].nunique()),\n",
    "    \"respondents_in_wide\": int(stage2_wide_df[\"respondent\"].nunique()),\n",
    "    \"respondent_sets_match\": respondent_sets_match,\n",
    "    \"response_rows\": int(actual_response_rows),\n",
    "    \"expected_response_slots\": int(expected_response_rows),\n",
    "    \"duplicate_response_pairs\": int(duplicate_response_pairs),\n",
    "    \"non_null_responses\": int(non_null_responses),\n",
    "    \"empty_responses\": int(empty_responses),\n",
    "    \"score_rows\": int(stage2_long_scores.shape[0]),\n",
    "    \"target_column_coverage\": target_column_coverage,\n",
    "    \"issues_flagged\": int(stage2_long[\"data_quality_flag\"].eq(\"true\").sum()),\n",
    "}\n",
    "\n",
    "if duplicate_response_pairs > 0:\n",
    "    print(f\"Warning: Found {duplicate_response_pairs} duplicate response pair(s).\")\n",
    "\n",
    "validation_frame = pd.DataFrame([validation_summary]).T\n",
    "validation_frame.columns = [\"value\"]\n",
    "display(validation_frame)\n",
    "\n",
    "validation_summary_path = (\n",
    "    project_root\n",
    "    / \"tasks\"\n",
    "    / \"13-11-25 Wrap up enjoyment data\"\n",
    "    / \"stage2_validation_summary.json\"\n",
    ")\n",
    "validation_summary_path.write_text(json.dumps(validation_summary, indent=2))\n",
    "\n",
    "print(f\"Validation summary written to {validation_summary_path.relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72330f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31    25.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Sum, dtype: float64\n",
      "31    2.0\n",
      "Name: Long_Abbot Elementary_Survey_Enjoyment_E18, dtype: float64\n",
      "31    23.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Sum'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_Enjoyment_E18'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15074b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>normalized_from_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "22                                               41.0     \n",
       "23                                               51.0     \n",
       "24                                               50.0     \n",
       "25                                               45.0     \n",
       "26                                               51.0     \n",
       "27                                               35.0     \n",
       "28                                               19.0     \n",
       "29                                               19.0     \n",
       "30                                               55.0     \n",
       "31                                               25.0     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "22                                               41.0           \n",
       "23                                               53.0           \n",
       "24                                               54.0           \n",
       "25                                               49.0           \n",
       "26                                               55.0           \n",
       "27                                               37.0           \n",
       "28                                               15.0           \n",
       "29                                               15.0           \n",
       "30                                               55.0           \n",
       "31                                               23.0           \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "22                                                 12       \n",
       "23                                                 12       \n",
       "24                                                 12       \n",
       "25                                                 12       \n",
       "26                                                 12       \n",
       "27                                                 12       \n",
       "28                                                 12       \n",
       "29                                                 12       \n",
       "30                                                 12       \n",
       "31                                                 12       \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "22                                           0.604167            \n",
       "23                                           0.812500            \n",
       "24                                           0.791667            \n",
       "25                                           0.687500            \n",
       "26                                           0.812500            \n",
       "27                                           0.479167            \n",
       "28                                           0.145833            \n",
       "29                                           0.145833            \n",
       "30                                           0.895833            \n",
       "31                                           0.270833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "22                                           0.604167                     \n",
       "23                                           0.854167                     \n",
       "24                                           0.875000                     \n",
       "25                                           0.770833                     \n",
       "26                                           0.895833                     \n",
       "27                                           0.520833                     \n",
       "28                                           0.062500                     \n",
       "29                                           0.062500                     \n",
       "30                                           0.895833                     \n",
       "31                                           0.229167                     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "22                                           3.416667      \n",
       "23                                           4.416667      \n",
       "24                                           4.500000      \n",
       "25                                           4.083333      \n",
       "26                                           4.583333      \n",
       "27                                           3.083333      \n",
       "28                                           1.250000      \n",
       "29                                           1.250000      \n",
       "30                                           4.583333      \n",
       "31                                           1.916667      \n",
       "\n",
       "    normalized_from_corrected  \n",
       "22                   0.604167  \n",
       "23                   0.854167  \n",
       "24                   0.875000  \n",
       "25                   0.770833  \n",
       "26                   0.895833  \n",
       "27                   0.520833  \n",
       "28                   0.062500  \n",
       "29                   0.062500  \n",
       "30                   0.895833  \n",
       "31                   0.229167  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Long_Abbot Elementary\"\n",
    "columns_to_show = [\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Sum\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Corrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Count\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Normalized\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Mean\",\n",
    "]\n",
    "comparison = survey_features.loc[\n",
    "    survey_features[f\"{prefix}_Survey_EnjoymentComposite_Count\"].gt(0),\n",
    "    columns_to_show\n",
    "].head(10).copy()\n",
    "comparison[\"normalized_from_corrected\"] = np.clip(\n",
    "    (comparison[f\"{prefix}_Survey_EnjoymentComposite_Corrected\"]\n",
    "     - comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"])\n",
    "    / (4.0 * comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"]),\n",
    "    0,\n",
    "    1,\n",
    " )\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b2306",
   "metadata": {},
   "source": [
    "## Stage 3: Post Questionnaire\n",
    "The seven-day follow-up instrument captures delayed recall, comprehension, and broader post-viewing perceptions. This section documents how the Post questionnaire exports extend the unified view (`uv`) with respondent-level memory, confidence, and engagement measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97e60a",
   "metadata": {},
   "source": [
    "### Planned Workflow\n",
    "- Inventory every group-level Post questionnaire export under `data/Recall/`, ensuring respondent IDs stay string-typed for clean joins.\n",
    "- Harmonize column names via `data/post_survey_map.csv` so variants or duplicate headers collapse to a single `question_code`.\n",
    "- Engineer response features (e.g., accuracy, confidence, free-text summaries) while following the `{form}_{title}_Post_{metric}_{method}` naming pattern.\n",
    "- Merge the Post feature frame onto the Stage 1 base (preserving respondent metadata) and log gaps to a dedicated issues table.\n",
    "- Validate the enriched unified view and emit Stage 3 outputs (`uv_stage3.csv`, `uv_stage3_issues.csv`) to `results/` with timestamped fallbacks when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcfd5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated post_survey_map.csv with question_code column; 2 rows are missing a parsed code.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_code</th>\n",
       "      <th>type</th>\n",
       "      <th>subscale</th>\n",
       "      <th>category</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Group A</th>\n",
       "      <th>Group B</th>\n",
       "      <th>Group C</th>\n",
       "      <th>Group D</th>\n",
       "      <th>Group E</th>\n",
       "      <th>Group F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>Wonder Woman</td>\n",
       "      <td>Wonder Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Schitts Creek</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>5.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>unseen</td>\n",
       "      <td>miss</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>6.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>I am Legend</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>seen</td>\n",
       "      <td>hit</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>fake</td>\n",
       "      <td>miss</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>seen</td>\n",
       "      <td>hit</td>\n",
       "      <td>The Town</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1. Did you see this scene in the videos you...</td>\n",
       "      <td>10.1</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>A Star Is Born</td>\n",
       "      <td>Ironman</td>\n",
       "      <td>Ironman</td>\n",
       "      <td>The Office</td>\n",
       "      <td>The Office</td>\n",
       "      <td>A Star Is Born</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question question_code    type  \\\n",
       "0  1.1. Did you see this scene in the videos you ...           1.1  binary   \n",
       "1  2.1. Did you see this scene in the videos you ...           2.1  binary   \n",
       "2  3.1. Did you see this scene in the videos you ...           3.1  binary   \n",
       "3  4.1. Did you see this scene in the videos you ...           4.1  binary   \n",
       "4  5.1. Did you see this scene in the videos you ...           5.1  binary   \n",
       "5  6.1. Did you see this scene in the videos you ...           6.1  binary   \n",
       "6  7.1. Did you see this scene in the videos you ...           7.1  binary   \n",
       "7  8.1. Did you see this scene in the videos you ...           8.1  binary   \n",
       "8  9.1. Did you see this scene in the videos you ...           9.1  binary   \n",
       "9  10.1. Did you see this scene in the videos you...          10.1  binary   \n",
       "\n",
       "      subscale    category accuracy          Group A          Group B  \\\n",
       "0  recognition         key      hit          Mad Max         The Town   \n",
       "1  recognition        fake     miss  The Dark Knight  The Dark Knight   \n",
       "2  recognition  distractor      hit          Titanic     The Notebook   \n",
       "3  recognition         key      hit         The Town          Mad Max   \n",
       "4  recognition      unseen     miss          Mad Max         The Town   \n",
       "5  recognition        fake     miss      I am Legend          Titanic   \n",
       "6  recognition        seen      hit         The Town          Mad Max   \n",
       "7  recognition        fake     miss          Friends          Friends   \n",
       "8  recognition        seen      hit         The Town         The Town   \n",
       "9  recognition  distractor      hit   A Star Is Born          Ironman   \n",
       "\n",
       "            Group C           Group D           Group E           Group F  \n",
       "0          The Town  Abbot Elementary           Mad Max           Mad Max  \n",
       "1        Goodfellas        Goodfellas      Wonder Woman      Wonder Woman  \n",
       "2      The Notebook     Schitts Creek     Schitts Creek           Titanic  \n",
       "3  Abbot Elementary          The Town  Abbot Elementary  Abbot Elementary  \n",
       "4          The Town  Abbot Elementary  Abbot Elementary           Mad Max  \n",
       "5          Seinfeld          Seinfeld           Friends           Friends  \n",
       "6  Abbot Elementary          The Town           Mad Max  Abbot Elementary  \n",
       "7           Friends           Friends          Seinfeld          Seinfeld  \n",
       "8  Abbot Elementary  Abbot Elementary  Abbot Elementary           Mad Max  \n",
       "9           Ironman        The Office        The Office    A Star Is Born  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "post_map_path = project_root / \"data\" / \"post_survey_map.csv\"\n",
    "post_map_df = pd.read_csv(post_map_path)\n",
    "\n",
    "code_pattern = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
    "\n",
    "def extract_question_code(text: str):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = code_pattern.match(str(text))\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "if \"question_code\" not in post_map_df.columns:\n",
    "    post_map_df.insert(1, \"question_code\", post_map_df[\"question\"].apply(extract_question_code))\n",
    "else:\n",
    "    post_map_df[\"question_code\"] = post_map_df[\"question\"].apply(extract_question_code)\n",
    "\n",
    "post_map_df.to_csv(post_map_path, index=False)\n",
    "missing_codes = post_map_df[\"question_code\"].isna().sum()\n",
    "print(\n",
    "    f\"Updated {post_map_path.name} with question_code column; \"\n",
    "    f\"{missing_codes} rows are missing a parsed code.\"\n",
    ")\n",
    "post_map_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eddef5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 3 respondent(s) due to post group mismatch: 6, 116, 117.\n",
      "Parsed 2880 recognition feature rows across 39 composite columns and 110 raw response columns for 80 respondents.\n",
      "Sample feature columns:\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q07\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q04</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q07</th>\n",
       "      <th>Long_Abbot Elementary_Post_RecognitionComposite_Q09</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q4-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q4-2</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q7-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q7-2</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q9-1</th>\n",
       "      <th>Long_Abbot Elementary_Post_Recognition_Q9-2</th>\n",
       "      <th>...</th>\n",
       "      <th>distractor_Post_Recognition_Sum</th>\n",
       "      <th>fake_Post_Recognition_Count</th>\n",
       "      <th>fake_Post_Recognition_Mean</th>\n",
       "      <th>fake_Post_Recognition_NormalizedMean</th>\n",
       "      <th>fake_Post_Recognition_Sum</th>\n",
       "      <th>unseen_Post_Recognition_Count</th>\n",
       "      <th>unseen_Post_Recognition_Mean</th>\n",
       "      <th>unseen_Post_Recognition_NormalizedMean</th>\n",
       "      <th>unseen_Post_Recognition_Sum</th>\n",
       "      <th>post_survey_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>data/Post/Group B_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group C_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  Long_Abbot Elementary_Post_RecognitionComposite_Q04  \\\n",
       "0          1                                                1.0     \n",
       "1         10                                                NaN     \n",
       "2        100                                                NaN     \n",
       "3        101                                                1.0     \n",
       "4        102                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_RecognitionComposite_Q07  \\\n",
       "0                                               0.75     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                               1.00     \n",
       "4                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_RecognitionComposite_Q09  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                                1.0     \n",
       "4                                                NaN     \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q4-1  \\\n",
       "0                                          1.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q4-2  \\\n",
       "0                                          4.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          4.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q7-1  \\\n",
       "0                                          1.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q7-2  \\\n",
       "0                                          3.0   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          4.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q9-1  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          1.0   \n",
       "4                                          NaN   \n",
       "\n",
       "   Long_Abbot Elementary_Post_Recognition_Q9-2  ...  \\\n",
       "0                                          NaN  ...   \n",
       "1                                          NaN  ...   \n",
       "2                                          NaN  ...   \n",
       "3                                          4.0  ...   \n",
       "4                                          NaN  ...   \n",
       "\n",
       "   distractor_Post_Recognition_Sum  fake_Post_Recognition_Count  \\\n",
       "0                             0.00                          2.0   \n",
       "1                             1.75                          4.0   \n",
       "2                             2.00                          4.0   \n",
       "3                             1.00                          4.0   \n",
       "4                             2.00                          4.0   \n",
       "\n",
       "   fake_Post_Recognition_Mean  fake_Post_Recognition_NormalizedMean  \\\n",
       "0                      0.3750                                0.3750   \n",
       "1                      0.8125                                0.8125   \n",
       "2                      1.0000                                1.0000   \n",
       "3                      0.6250                                0.6250   \n",
       "4                      1.0000                                1.0000   \n",
       "\n",
       "   fake_Post_Recognition_Sum  unseen_Post_Recognition_Count  \\\n",
       "0                       0.75                            1.0   \n",
       "1                       3.25                            1.0   \n",
       "2                       4.00                            1.0   \n",
       "3                       2.50                            1.0   \n",
       "4                       4.00                            1.0   \n",
       "\n",
       "   unseen_Post_Recognition_Mean  unseen_Post_Recognition_NormalizedMean  \\\n",
       "0                          0.00                                    0.00   \n",
       "1                          0.75                                    0.75   \n",
       "2                          1.00                                    1.00   \n",
       "3                          0.50                                    0.50   \n",
       "4                          0.00                                    0.00   \n",
       "\n",
       "   unseen_Post_Recognition_Sum  \\\n",
       "0                         0.00   \n",
       "1                         0.75   \n",
       "2                         1.00   \n",
       "3                         0.50   \n",
       "4                         0.00   \n",
       "\n",
       "                             post_survey_source_path  \n",
       "0  data/Post/Group C_Post Viewing Questionnaire P...  \n",
       "1  data/Post/Group B_ Post Viewing Questionnaire ...  \n",
       "2  data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "3  data/Post/Group C_ Post Viewing Questionnaire ...  \n",
       "4  data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "post_data_dir = project_root / \"data\" / \"Post\"\n",
    "if not post_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Post questionnaire directory not found at {post_data_dir}.\")\n",
    "post_files = sorted(\n",
    "    path\n",
    "    for path in post_data_dir.rglob(\"*.csv\")\n",
    "    if path.is_file() and not path.name.startswith(\"~$\")\n",
    ")\n",
    "if not post_files:\n",
    "    raise FileNotFoundError(f\"No post questionnaire CSV files found under {post_data_dir}.\")\n",
    "\n",
    "def merge_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _sanitize(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, str):\n",
    "            cleaned = value.strip()\n",
    "            return cleaned if cleaned else np.nan\n",
    "        return value\n",
    "    merged = {}\n",
    "    column_order = []\n",
    "    for column in df.columns:\n",
    "        normalized = re.sub(r\"\\s+\", \" \", str(column)).strip()\n",
    "        series = df[column].astype(\"object\").map(_sanitize)\n",
    "        if normalized not in merged:\n",
    "            merged[normalized] = series\n",
    "            column_order.append(normalized)\n",
    "        else:\n",
    "            merged[normalized] = merged[normalized].combine_first(series)\n",
    "    return pd.DataFrame({name: merged[name] for name in column_order})\n",
    "\n",
    "post_map_full = pd.read_csv(project_root / \"data\" / \"post_survey_map.csv\")\n",
    "if \"question_code\" not in post_map_full.columns:\n",
    "    post_map_full.insert(1, \"question_code\", post_map_full[\"question\"].apply(extract_question_code))\n",
    "else:\n",
    "    post_map_full[\"question_code\"] = post_map_full[\"question\"].apply(extract_question_code)\n",
    "post_map_full[\"subscale\"] = post_map_full[\"subscale\"].astype(str)\n",
    "\n",
    "CATEGORY_RENAME = {\n",
    "    \"key\": \"wb-key\",\n",
    "    \"seen\": \"wb-notKeySeen\",\n",
    "    \"unseen\": \"wb-notKeyUnseen\",\n",
    "    \"fake\": \"distractor\",\n",
    "    \"distractor\": \"comp-key\",\n",
    "    \"distractor2\": \"comp-notKeySeen\",\n",
    "}\n",
    "\n",
    "STAT_LABELS = {\n",
    "    \"count\": \"Count\",\n",
    "    \"sum\": \"Sum\",\n",
    "    \"mean\": \"Mean\",\n",
    "    \"normalized_mean\": \"NormalizedMean\",\n",
    "}\n",
    "RECOGNITION_BINARY_MAX = 2.0\n",
    "RECOGNITION_CONFIDENCE_MAX = 4.0\n",
    "RECOGNITION_COMPOSITE_MAX = RECOGNITION_BINARY_MAX * RECOGNITION_CONFIDENCE_MAX\n",
    "FORM_CATEGORY_KEYS = {\"key\", \"seen\"}\n",
    "NON_FORM_CATEGORY_KEYS = {\"unseen\", \"fake\", \"distractor\", \"distractor2\"}\n",
    "\n",
    "uv_stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not uv_stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 dataset not found at {uv_stage1_path}.\")\n",
    "uv_stage1 = pd.read_csv(uv_stage1_path)\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1[\"group\"] = uv_stage1[\"group\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "uv_stage2_path = project_root / \"results\" / \"uv_stage2.csv\"\n",
    "uv_stage2_cached = None\n",
    "if uv_stage2_path.exists():\n",
    "    uv_stage2_cached = pd.read_csv(uv_stage2_path)\n",
    "    uv_stage2_cached[\"respondent\"] = uv_stage2_cached[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage1_lookup_df = (\n",
    "    uv_stage1\n",
    "    .loc[:, [\"respondent\", \"group\", \"Short Form\", \"Long Form\"]]\n",
    "    .dropna(subset=[\"respondent\"])\n",
    "    .assign(respondent=lambda df: df[\"respondent\"].astype(str).str.strip())\n",
    "    .drop_duplicates(\"respondent\", keep=\"last\")\n",
    ")\n",
    "uv_stage1_lookup = uv_stage1_lookup_df.set_index(\"respondent\").to_dict(\"index\")\n",
    "\n",
    "recognition_map = post_map_full.loc[\n",
    "    post_map_full[\"subscale\"].str.lower() == \"recognition\",\n",
    "    :,\n",
    "]\n",
    "recognition_map = recognition_map.loc[recognition_map[\"question_code\"].notna()].copy()\n",
    "recognition_map[\"question_code\"] = recognition_map[\"question_code\"].astype(str).str.strip()\n",
    "recognition_map = recognition_map.loc[recognition_map[\"question_code\"].str.match(r\"^\\d+\\.[12]$\")]\n",
    "\n",
    "valid_question_codes = set(recognition_map[\"question_code\"].unique())\n",
    "\n",
    "group_columns = [col for col in recognition_map.columns if col.startswith(\"Group \")]\n",
    "\n",
    "TITLE_NORMALIZATION = {\n",
    "    \"abbott elementary\": \"Abbot Elementary\",\n",
    "    \"abbot elementary\": \"Abbot Elementary\",\n",
    "    \"schitts creek\": \"Schittss Creek\",\n",
    "    \"schittss creek\": \"Schittss Creek\",\n",
    "    \"mad max fury road\": \"Mad Max\",\n",
    "    \"mad max\": \"Mad Max\",\n",
    "}\n",
    "\n",
    "def canonicalize_title(raw_title: str) -> str:\n",
    "    if pd.isna(raw_title):\n",
    "        return \"\"\n",
    "    cleaned = str(raw_title).strip()\n",
    "    if not cleaned:\n",
    "        return \"\"\n",
    "    lookup_key = cleaned.lower()\n",
    "    return TITLE_NORMALIZATION.get(lookup_key, cleaned)\n",
    "\n",
    "respondent_exposures: dict[str, dict[str, set[str]]] = {}\n",
    "for respondent_id, info in uv_stage1_lookup.items():\n",
    "    exposures: dict[str, set[str]] = {}\n",
    "    long_title = canonicalize_title(info.get(\"Long Form\", \"\"))\n",
    "    if long_title:\n",
    "        exposures.setdefault(\"Long\", set()).add(long_title)\n",
    "    short_title = canonicalize_title(info.get(\"Short Form\", \"\"))\n",
    "    if short_title:\n",
    "        exposures.setdefault(\"Short\", set()).add(short_title)\n",
    "    if exposures:\n",
    "        respondent_exposures[str(respondent_id)] = exposures\n",
    "\n",
    "meta_lookup = {}\n",
    "for _, row in recognition_map.iterrows():\n",
    "    q_code = row[\"question_code\"]\n",
    "    q_root, q_suffix = q_code.split(\".\")\n",
    "    category = str(row.get(\"category\", \"\")).strip()\n",
    "    accuracy = str(row.get(\"accuracy\", \"\")).strip().lower()\n",
    "    for group_col in group_columns:\n",
    "        group_letter = group_col.replace(\"Group \", \"\").strip().upper()\n",
    "        title_value = canonicalize_title(row.get(group_col, \"\"))\n",
    "        meta_lookup[(group_letter, q_root, q_suffix)] = {\n",
    "            \"title\": title_value,\n",
    "            \"category\": category,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"([A-F])\", expand=False)\n",
    "stimulus_map[\"title_clean\"] = stimulus_map[\"title\"].map(canonicalize_title)\n",
    "stimulus_map[\"form_clean\"] = stimulus_map[\"form\"].astype(str).str.title()\n",
    "\n",
    "group_title_form_lookup = {\n",
    "    (row.group_letter, row.title_clean): row.form_clean\n",
    "    for row in stimulus_map.itertuples()\n",
    "    if isinstance(row.group_letter, str) and isinstance(row.title_clean, str) and row.title_clean\n",
    "}\n",
    "\n",
    "if uv_stage2_cached is not None:\n",
    "    uv_columns = pd.Index(uv_stage2_cached.columns)\n",
    "else:\n",
    "    uv_columns = pd.Index(uv_stage1.columns)\n",
    "\n",
    "yes_values = {\"yes\", \"y\", \"true\", \"1\"}\n",
    "no_values = {\"no\", \"n\", \"false\", \"0\"}\n",
    "\n",
    "confidence_pattern = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\")\n",
    "\n",
    "def extract_scalar(value):\n",
    "    if isinstance(value, pd.Series):\n",
    "        non_null = value.dropna()\n",
    "        if non_null.empty:\n",
    "            return np.nan\n",
    "        return non_null.iloc[0]\n",
    "    return value\n",
    "\n",
    "def parse_yes_no(value) -> float:\n",
    "    value = extract_scalar(value)\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip().lower()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    if text in yes_values:\n",
    "        return 1.0\n",
    "    if text in no_values:\n",
    "        return 0.0\n",
    "    return np.nan\n",
    "\n",
    "def parse_confidence(value) -> float:\n",
    "    value = extract_scalar(value)\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    match = confidence_pattern.match(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def resolve_form(respondent_id: str, group_letter: str, title: str) -> tuple[str, list[str]]:\n",
    "    notes: list[str] = []\n",
    "    exposures = uv_stage1_lookup.get(respondent_id, {})\n",
    "    long_title = canonicalize_title(exposures.get(\"Long Form\", \"\")) if exposures else \"\"\n",
    "    short_title = canonicalize_title(exposures.get(\"Short Form\", \"\")) if exposures else \"\"\n",
    "    canonical = canonicalize_title(title)\n",
    "    if canonical and canonical == long_title:\n",
    "        return \"Long\", notes\n",
    "    if canonical and canonical == short_title:\n",
    "        return \"Short\", notes\n",
    "    group_clean = (group_letter or \"\").strip().upper()\n",
    "    if canonical:\n",
    "        mapped = group_title_form_lookup.get((group_clean, canonical))\n",
    "        if mapped:\n",
    "            notes.append(\"form_from_group_stimulus_map\")\n",
    "            return mapped, notes\n",
    "        has_short = any(col.startswith(f\"Short_{canonical}\") for col in uv_columns)\n",
    "        has_long = any(col.startswith(f\"Long_{canonical}\") for col in uv_columns)\n",
    "        if has_short and not has_long:\n",
    "            notes.append(\"form_inferred_short_from_uv\")\n",
    "            return \"Short\", notes\n",
    "        if has_long and not has_short:\n",
    "            notes.append(\"form_inferred_long_from_uv\")\n",
    "            return \"Long\", notes\n",
    "        if has_short and has_long:\n",
    "            notes.append(\"form_ambiguous_default_short\")\n",
    "            return \"Short\", notes\n",
    "    notes.append(\"form_unresolved_default_short\")\n",
    "    return \"Short\", notes\n",
    "\n",
    "recognition_records: list[dict] = []\n",
    "issue_records: list[dict] = []\n",
    "respondent_post_paths: dict[str, str] = {}\n",
    "excluded_group_mismatch = set()\n",
    "\n",
    "for csv_path in post_files:\n",
    "    file_group_match = re.search(r\"Group\\s+([A-F])\", csv_path.stem, re.IGNORECASE)\n",
    "    fallback_group_letter = file_group_match.group(1).upper() if file_group_match else \"\"\n",
    "    df_raw = pd.read_csv(csv_path, dtype=str)\n",
    "    df_merged = merge_duplicate_columns(df_raw)\n",
    "    rename_map = {}\n",
    "    for column in df_merged.columns:\n",
    "        q_code = extract_question_code(column)\n",
    "        if q_code and q_code in valid_question_codes:\n",
    "            rename_map[column] = q_code\n",
    "    df_standard = df_merged.rename(columns=rename_map)\n",
    "    respondent_col = next(((\n",
    "        col for col in df_standard.columns\n",
    "        if \"participant number\" in col.lower()\n",
    "    )), None)\n",
    "    if respondent_col is None:\n",
    "        raise KeyError(f\"Participant identifier column not found in {csv_path.name}\")\n",
    "    df_standard[\"respondent\"] = df_standard[respondent_col].astype(str).str.strip()\n",
    "    df_standard[\"respondent\"] = df_standard[\"respondent\"].replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    df_standard[\"respondent\"] = df_standard[\"respondent\"].str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if \"Timestamp\" in df_standard.columns:\n",
    "        df_standard[\"timestamp_iso\"] = pd.to_datetime(df_standard[\"Timestamp\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df_standard[\"timestamp_iso\"] = pd.NaT\n",
    "\n",
    "    for _, row in df_standard.iterrows():\n",
    "        respondent_id = row.get(\"respondent\")\n",
    "        if pd.isna(respondent_id):\n",
    "            continue\n",
    "        respondent_id = str(respondent_id).strip()\n",
    "        if not respondent_id:\n",
    "            continue\n",
    "        respondent_info = uv_stage1_lookup.get(respondent_id, {})\n",
    "        stage1_group = str(respondent_info.get(\"group\", \"\")).strip().upper()\n",
    "        post_group_letter = fallback_group_letter\n",
    "        if stage1_group and post_group_letter and stage1_group != post_group_letter:\n",
    "            if respondent_id not in excluded_group_mismatch:\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": stage1_group,\n",
    "                    \"question_number\": np.nan,\n",
    "                    \"title\": \"\",\n",
    "                    \"issue\": f\"post_group_mismatch_uv_{stage1_group}_file_{post_group_letter}\",\n",
    "                    \"source_file\": csv_path.name,\n",
    "                })\n",
    "            excluded_group_mismatch.add(respondent_id)\n",
    "            continue\n",
    "        group_letter = stage1_group or post_group_letter\n",
    "        if not group_letter:\n",
    "            issue_records.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"group\": \"\",\n",
    "                \"question_number\": np.nan,\n",
    "                \"title\": \"\",\n",
    "                \"issue\": \"group_missing_in_stage1_and_filename\",\n",
    "                \"source_file\": csv_path.name,\n",
    "            })\n",
    "            continue\n",
    "        if respondent_id not in respondent_post_paths:\n",
    "            try:\n",
    "                relative_path = csv_path.relative_to(project_root)\n",
    "                respondent_post_paths[respondent_id] = relative_path.as_posix()\n",
    "            except ValueError:\n",
    "                respondent_post_paths[respondent_id] = csv_path.as_posix()\n",
    "        for q_num in map(str, range(1, 13)):\n",
    "            base_meta = meta_lookup.get((group_letter, q_num, \"1\"))\n",
    "            conf_meta = meta_lookup.get((group_letter, q_num, \"2\"))\n",
    "            if base_meta is None or conf_meta is None:\n",
    "                continue\n",
    "            binary_value = row.get(f\"{q_num}.1\")\n",
    "            confidence_value = row.get(f\"{q_num}.2\")\n",
    "            yes_no = parse_yes_no(binary_value)\n",
    "            confidence = parse_confidence(confidence_value)\n",
    "            issues_here: list[str] = []\n",
    "            if np.isnan(yes_no):\n",
    "                issues_here.append(\"binary_response_missing_or_unrecognized\")\n",
    "            accuracy = base_meta.get(\"accuracy\", \"\")\n",
    "            expected_yes = accuracy == \"hit\"\n",
    "            base_score_raw = np.nan\n",
    "            base_score = np.nan\n",
    "            if not np.isnan(yes_no):\n",
    "                answered_yes = bool(yes_no)\n",
    "                base_score_raw = RECOGNITION_BINARY_MAX if answered_yes == expected_yes else 0.0\n",
    "                base_score = base_score_raw / RECOGNITION_BINARY_MAX\n",
    "            if np.isnan(confidence):\n",
    "                issues_here.append(\"confidence_missing_or_unrecognized\")\n",
    "            composite_raw = np.nan\n",
    "            composite = np.nan\n",
    "            if not np.isnan(base_score_raw) and not np.isnan(confidence):\n",
    "                composite_raw = base_score_raw * confidence\n",
    "                composite = composite_raw / RECOGNITION_COMPOSITE_MAX\n",
    "            form_value, form_notes = resolve_form(respondent_id, group_letter, base_meta.get(\"title\", \"\"))\n",
    "            issues_here.extend(form_notes)\n",
    "            question_int = int(float(q_num))\n",
    "            title_segment = base_meta.get(\"title\", \"\").strip() or f\"Question {question_int}\"\n",
    "            composite_name = f\"{form_value}_{title_segment}_Post_RecognitionComposite_Q{question_int:02d}\"\n",
    "            base_name = f\"{form_value}_{title_segment}_Post_Recognition_Q{question_int}-1\"\n",
    "            confidence_name = f\"{form_value}_{title_segment}_Post_Recognition_Q{question_int}-2\"\n",
    "            records_to_add = [\n",
    "                (\"composite\", composite_name, composite),\n",
    "                (\"binary\", base_name, base_score),\n",
    "                (\"confidence\", confidence_name, confidence),\n",
    "            ]\n",
    "            for metric_label, feature_name, feature_value in records_to_add:\n",
    "                recognition_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": group_letter,\n",
    "                    \"question_number\": question_int,\n",
    "                    \"title\": base_meta.get(\"title\", \"\"),\n",
    "                    \"category\": base_meta.get(\"category\", \"\"),\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"form\": form_value,\n",
    "                    \"metric\": metric_label,\n",
    "                    \"column_name\": feature_name,\n",
    "                    \"value\": feature_value,\n",
    "                    \"timestamp\": row.get(\"timestamp_iso\", pd.NaT),\n",
    "                })\n",
    "            for issue in issues_here:\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"group\": group_letter,\n",
    "                    \"question_number\": question_int,\n",
    "                    \"title\": base_meta.get(\"title\", \"\"),\n",
    "                    \"issue\": issue,\n",
    "                    \"source_file\": csv_path.name,\n",
    "                })\n",
    "\n",
    "recognition_df = pd.DataFrame(recognition_records)\n",
    "if recognition_df.empty:\n",
    "    raise ValueError(\"No recognition responses were parsed from post questionnaire files.\")\n",
    "recognition_df = recognition_df.sort_values([\"respondent\", \"column_name\", \"timestamp\"])\n",
    "recognition_df = recognition_df.drop_duplicates([\"respondent\", \"column_name\"], keep=\"last\")\n",
    "\n",
    "recognition_features = (\n",
    "    recognition_df\n",
    "    .pivot(index=\"respondent\", columns=\"column_name\", values=\"value\")\n",
    "    .sort_index(axis=1)\n",
    "    .reset_index()\n",
    ")\n",
    "recognition_features.columns.name = None\n",
    "\n",
    "composite_subset = recognition_df.loc[recognition_df[\"metric\"] == \"composite\"].copy()\n",
    "if not composite_subset.empty:\n",
    "    composite_subset[\"category_lower\"] = (\n",
    "        composite_subset[\"category\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace(\"\", np.nan)\n",
    "    )\n",
    "    composite_subset = composite_subset.loc[composite_subset[\"category_lower\"].notna()]\n",
    "    composite_subset[\"title_clean\"] = composite_subset[\"title\"].map(canonicalize_title)\n",
    "    composite_subset[\"respondent\"] = composite_subset[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "    form_subset = composite_subset.loc[composite_subset[\"category_lower\"].isin(FORM_CATEGORY_KEYS)].copy()\n",
    "    if not form_subset.empty:\n",
    "        def _match_form(row) -> str | None:\n",
    "            exposures = respondent_exposures.get(row[\"respondent\"], {})\n",
    "            for form_label, titles in exposures.items():\n",
    "                if row[\"title_clean\"] in titles:\n",
    "                    return form_label\n",
    "            return None\n",
    "\n",
    "        form_subset[\"aligned_form\"] = form_subset.apply(_match_form, axis=1)\n",
    "        unmatched_mask = form_subset[\"aligned_form\"].isna()\n",
    "        if unmatched_mask.any():\n",
    "            for row in form_subset.loc[unmatched_mask, [\"respondent\", \"question_number\", \"title\", \"category_lower\"]].itertuples(index=False):\n",
    "                respondent_key = str(row.respondent)\n",
    "                issue_records.append({\n",
    "                    \"respondent\": respondent_key,\n",
    "                    \"group\": uv_stage1_lookup.get(respondent_key, {}).get(\"group\", \"\"),\n",
    "                    \"question_number\": int(row.question_number) if not pd.isna(row.question_number) else np.nan,\n",
    "                    \"title\": row.title,\n",
    "                    \"issue\": \"post_form_title_not_in_stage1_exposures\",\n",
    "                    \"source_file\": respondent_post_paths.get(respondent_key, \"\"),\n",
    "                })\n",
    "            form_subset = form_subset.loc[~unmatched_mask]\n",
    "        if not form_subset.empty:\n",
    "            form_agg = (\n",
    "                form_subset\n",
    "                .groupby([\"respondent\", \"aligned_form\", \"category_lower\"])[\"value\"]\n",
    "                .agg(count=\"count\", sum=\"sum\", mean=\"mean\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            if not form_agg.empty:\n",
    "                form_agg[\"normalized_mean\"] = form_agg[\"mean\"].astype(float).clip(0, 1)\n",
    "                form_long = form_agg.melt(\n",
    "                    id_vars=[\"respondent\", \"aligned_form\", \"category_lower\"],\n",
    "                    value_vars=[\"count\", \"sum\", \"mean\", \"normalized_mean\"],\n",
    "                    var_name=\"statistic\",\n",
    "                    value_name=\"stat_value\",\n",
    "                )\n",
    "                form_long[\"column_name\"] = form_long.apply(\n",
    "                    lambda r: f\"{r['aligned_form']}_{r['category_lower']}_Post_Recognition_{STAT_LABELS[r['statistic']]}\"\n",
    "                    , axis=1,\n",
    "                )\n",
    "                form_wide = form_long.pivot(index=\"respondent\", columns=\"column_name\", values=\"stat_value\").reset_index()\n",
    "                form_wide.columns.name = None\n",
    "                recognition_features = recognition_features.merge(form_wide, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    non_form_subset = composite_subset.loc[composite_subset[\"category_lower\"].isin(NON_FORM_CATEGORY_KEYS)].copy()\n",
    "    if not non_form_subset.empty:\n",
    "        non_form_agg = (\n",
    "            non_form_subset\n",
    "            .groupby([\"respondent\", \"category_lower\"])[\"value\"]\n",
    "            .agg(count=\"count\", sum=\"sum\", mean=\"mean\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        if not non_form_agg.empty:\n",
    "            non_form_agg[\"normalized_mean\"] = non_form_agg[\"mean\"].astype(float).clip(0, 1)\n",
    "            non_form_long = non_form_agg.melt(\n",
    "                id_vars=[\"respondent\", \"category_lower\"],\n",
    "                value_vars=[\"count\", \"sum\", \"mean\", \"normalized_mean\"],\n",
    "                var_name=\"statistic\",\n",
    "                value_name=\"stat_value\",\n",
    "            )\n",
    "            non_form_long[\"column_name\"] = non_form_long.apply(\n",
    "                lambda r: f\"{r['category_lower']}_Post_Recognition_{STAT_LABELS[r['statistic']]}\"\n",
    "                , axis=1,\n",
    "            )\n",
    "            non_form_wide = non_form_long.pivot(index=\"respondent\", columns=\"column_name\", values=\"stat_value\").reset_index()\n",
    "            non_form_wide.columns.name = None\n",
    "            recognition_features = recognition_features.merge(non_form_wide, on=\"respondent\", how=\"left\")\n",
    "\n",
    "if respondent_post_paths:\n",
    "    post_path_df = (\n",
    "        pd.Series(respondent_post_paths, name=\"post_survey_source_path\")\n",
    "        .to_frame()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"respondent\"})\n",
    "    )\n",
    "    recognition_features = recognition_features.merge(post_path_df, on=\"respondent\", how=\"left\")\n",
    "\n",
    "feature_columns = [col for col in recognition_features.columns if col != \"respondent\"]\n",
    "composite_columns = [col for col in feature_columns if \"_Post_RecognitionComposite_\" in col]\n",
    "raw_columns = [\n",
    "    col\n",
    "    for col in feature_columns\n",
    "    if \"_Post_Recognition_\" in col and \"Composite\" not in col and not col.endswith(\"source_path\")\n",
    "]\n",
    "\n",
    "issues_df = pd.DataFrame(issue_records)\n",
    "if not issues_df.empty:\n",
    "    issues_df = issues_df.sort_values([\"respondent\", \"question_number\", \"issue\"])\n",
    "\n",
    "if excluded_group_mismatch:\n",
    "    def _mismatch_sort_key(value: str):\n",
    "        text = str(value)\n",
    "        if text.isdigit():\n",
    "            return (0, int(text))\n",
    "        return (1, text)\n",
    "    excluded_display = \", \".join(sorted(excluded_group_mismatch, key=_mismatch_sort_key))\n",
    "    print(\n",
    "        f\"Skipped {len(excluded_group_mismatch)} respondent(s) due to post group mismatch: {excluded_display}.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Parsed {recognition_df.shape[0]} recognition feature rows across \"\n",
    "    f\"{len(composite_columns)} composite columns and {len(raw_columns)} raw response columns \"\n",
    "    f\"for {recognition_features.shape[0]} respondents.\",\n",
    ")\n",
    "sample_columns = composite_columns[:3] + raw_columns[:3]\n",
    "if sample_columns:\n",
    "    print(\"Sample feature columns:\")\n",
    "    for name in sample_columns:\n",
    "        print(f\"  {name}\")\n",
    "recognition_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df111fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recognition feature columns: 150\n",
      "Composite columns (39 total):\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q07\n",
      "  Long_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q01\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q04\n",
      "  Long_Mad Max_Post_RecognitionComposite_Q07\n",
      "  Long_The Town_Post_RecognitionComposite_Q04\n",
      "  Long_The Town_Post_RecognitionComposite_Q07\n",
      "  Long_The Town_Post_RecognitionComposite_Q09\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q10\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q11\n",
      "  Short_A Star Is Born_Post_RecognitionComposite_Q12\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q01\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q04\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q05\n",
      "  Short_Abbot Elementary_Post_RecognitionComposite_Q09\n",
      "  Short_Friends_Post_RecognitionComposite_Q06\n",
      "  Short_Friends_Post_RecognitionComposite_Q08\n",
      "  Short_Goodfellas_Post_RecognitionComposite_Q02\n",
      "  Short_I am Legend_Post_RecognitionComposite_Q06\n",
      "  ...\n",
      "Raw recognition columns (110 total):\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q4-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q7-2\n",
      "  Long_Abbot Elementary_Post_Recognition_Q9-1\n",
      "  Long_Abbot Elementary_Post_Recognition_Q9-2\n",
      "  Long_Mad Max_Post_Recognition_Q1-1\n",
      "  Long_Mad Max_Post_Recognition_Q1-2\n",
      "  Long_Mad Max_Post_Recognition_Q4-1\n",
      "  Long_Mad Max_Post_Recognition_Q4-2\n",
      "  Long_Mad Max_Post_Recognition_Q7-1\n",
      "  Long_Mad Max_Post_Recognition_Q7-2\n",
      "  Long_The Town_Post_Recognition_Q4-1\n",
      "  Long_The Town_Post_Recognition_Q4-2\n",
      "  Long_The Town_Post_Recognition_Q7-1\n",
      "  Long_The Town_Post_Recognition_Q7-2\n",
      "  Long_The Town_Post_Recognition_Q9-1\n",
      "  Long_The Town_Post_Recognition_Q9-2\n",
      "  Short_A Star Is Born_Post_Recognition_Q10-1\n",
      "  Short_A Star Is Born_Post_Recognition_Q10-2\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [col for col in recognition_features.columns if col != \"respondent\"]\n",
    "composite_columns = [col for col in feature_columns if \"_Post_RecognitionComposite_\" in col]\n",
    "raw_columns = [col for col in feature_columns if \"_Post_Recognition_\" in col and \"Composite\" not in col]\n",
    "\n",
    "print(f\"Total recognition feature columns: {len(feature_columns)}\")\n",
    "print(f\"Composite columns ({len(composite_columns)} total):\")\n",
    "for name in composite_columns[:20]:\n",
    "    print(f\"  {name}\")\n",
    "if len(composite_columns) > 20:\n",
    "    print(\"  ...\")\n",
    "print(f\"Raw recognition columns ({len(raw_columns)} total):\")\n",
    "for name in raw_columns[:20]:\n",
    "    print(f\"  {name}\")\n",
    "if len(raw_columns) > 20:\n",
    "    print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ae4c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 UV shape: (83, 167); saved to uv_stage3.csv.\n",
      "Logged 599 recognition parsing issues to uv_stage3_issues.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>distractor_Post_Recognition_Sum</th>\n",
       "      <th>fake_Post_Recognition_Count</th>\n",
       "      <th>fake_Post_Recognition_Mean</th>\n",
       "      <th>fake_Post_Recognition_NormalizedMean</th>\n",
       "      <th>fake_Post_Recognition_Sum</th>\n",
       "      <th>unseen_Post_Recognition_Count</th>\n",
       "      <th>unseen_Post_Recognition_Mean</th>\n",
       "      <th>unseen_Post_Recognition_NormalizedMean</th>\n",
       "      <th>unseen_Post_Recognition_Sum</th>\n",
       "      <th>post_survey_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008_1.csv</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>10/07/2025</td>\n",
       "      <td>10:32:16</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005_10.csv</td>\n",
       "      <td>B</td>\n",
       "      <td>10</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>13:17:14</td>\n",
       "      <td>65</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>data/Post/Group B_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005_100.csv</td>\n",
       "      <td>E</td>\n",
       "      <td>100</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>19:14:06</td>\n",
       "      <td>25</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003_101.csv</td>\n",
       "      <td>C</td>\n",
       "      <td>101</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>14:58:27</td>\n",
       "      <td>67</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group C_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001_102.csv</td>\n",
       "      <td>D</td>\n",
       "      <td>102</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>16:35:40</td>\n",
       "      <td>37</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>004_94.csv</td>\n",
       "      <td>D</td>\n",
       "      <td>94</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>11:33:19</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group D_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group F_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>004_97.csv</td>\n",
       "      <td>E</td>\n",
       "      <td>97</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>17:41:01</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>004_98.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>98</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>17:41:49</td>\n",
       "      <td>32</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>003_99.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>19:09:48</td>\n",
       "      <td>25</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>data/Post/Group A_ Post Viewing Questionnaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  date_study time_study  age age_group  \\\n",
       "0     008_1.csv     C          1  10/07/2025   10:32:16   63     60-69   \n",
       "1    005_10.csv     B         10  10/10/2025   13:17:14   65     60-69   \n",
       "2   005_100.csv     E        100  10/15/2025   19:14:06   25     18-27   \n",
       "3   003_101.csv     C        101  10/16/2025   14:58:27   67     60-69   \n",
       "4   001_102.csv     D        102  10/16/2025   16:35:40   37     28-43   \n",
       "..          ...   ...        ...         ...        ...  ...       ...   \n",
       "78   004_94.csv     D         94  10/15/2025   11:33:19   32     28-43   \n",
       "79   004_96.csv     F         96  10/15/2025   14:32:00   29     28-43   \n",
       "80   004_97.csv     E         97  10/15/2025   17:41:01   32     28-43   \n",
       "81   004_98.csv     A         98  10/15/2025   17:41:49   32     28-43   \n",
       "82   003_99.csv     A         99  10/15/2025   19:09:48   25     18-27   \n",
       "\n",
       "    gender                      ethnicity                income_group  ...  \\\n",
       "0     Male  Hispanic/Latino/Latina/Latinx    $60,000 or more per year  ...   \n",
       "1     Male                          White  $35,000  $60,000 per year  ...   \n",
       "2   Female                          White  $35,000  $60,000 per year  ...   \n",
       "3     Male         Black/African American    $60,000 or more per year  ...   \n",
       "4     Male         Black/African American  $35,000  $60,000 per year  ...   \n",
       "..     ...                            ...                         ...  ...   \n",
       "78    Male                          White  $35,000  $60,000 per year  ...   \n",
       "79  Female                          White    $60,000 or more per year  ...   \n",
       "80  Female                          White    $60,000 or more per year  ...   \n",
       "81    Male                          White  $35,000  $60,000 per year  ...   \n",
       "82  Female                          White    $60,000 or more per year  ...   \n",
       "\n",
       "   distractor_Post_Recognition_Sum  fake_Post_Recognition_Count  \\\n",
       "0                             0.00                          2.0   \n",
       "1                             1.75                          4.0   \n",
       "2                             2.00                          4.0   \n",
       "3                             1.00                          4.0   \n",
       "4                             2.00                          4.0   \n",
       "..                             ...                          ...   \n",
       "78                            2.00                          4.0   \n",
       "79                            2.00                          4.0   \n",
       "80                            2.00                          4.0   \n",
       "81                            2.00                          4.0   \n",
       "82                            2.00                          4.0   \n",
       "\n",
       "    fake_Post_Recognition_Mean  fake_Post_Recognition_NormalizedMean  \\\n",
       "0                       0.3750                                0.3750   \n",
       "1                       0.8125                                0.8125   \n",
       "2                       1.0000                                1.0000   \n",
       "3                       0.6250                                0.6250   \n",
       "4                       1.0000                                1.0000   \n",
       "..                         ...                                   ...   \n",
       "78                      1.0000                                1.0000   \n",
       "79                      1.0000                                1.0000   \n",
       "80                      1.0000                                1.0000   \n",
       "81                      0.8750                                0.8750   \n",
       "82                      1.0000                                1.0000   \n",
       "\n",
       "   fake_Post_Recognition_Sum unseen_Post_Recognition_Count  \\\n",
       "0                       0.75                           1.0   \n",
       "1                       3.25                           1.0   \n",
       "2                       4.00                           1.0   \n",
       "3                       2.50                           1.0   \n",
       "4                       4.00                           1.0   \n",
       "..                       ...                           ...   \n",
       "78                      4.00                           1.0   \n",
       "79                      4.00                           1.0   \n",
       "80                      4.00                           1.0   \n",
       "81                      3.50                           1.0   \n",
       "82                      4.00                           1.0   \n",
       "\n",
       "   unseen_Post_Recognition_Mean  unseen_Post_Recognition_NormalizedMean  \\\n",
       "0                          0.00                                    0.00   \n",
       "1                          0.75                                    0.75   \n",
       "2                          1.00                                    1.00   \n",
       "3                          0.50                                    0.50   \n",
       "4                          0.00                                    0.00   \n",
       "..                          ...                                     ...   \n",
       "78                         0.00                                    0.00   \n",
       "79                         1.00                                    1.00   \n",
       "80                         0.00                                    0.00   \n",
       "81                         0.50                                    0.50   \n",
       "82                         1.00                                    1.00   \n",
       "\n",
       "    unseen_Post_Recognition_Sum  \\\n",
       "0                          0.00   \n",
       "1                          0.75   \n",
       "2                          1.00   \n",
       "3                          0.50   \n",
       "4                          0.00   \n",
       "..                          ...   \n",
       "78                         0.00   \n",
       "79                         1.00   \n",
       "80                         0.00   \n",
       "81                         0.50   \n",
       "82                         1.00   \n",
       "\n",
       "                              post_survey_source_path  \n",
       "0   data/Post/Group C_Post Viewing Questionnaire P...  \n",
       "1   data/Post/Group B_ Post Viewing Questionnaire ...  \n",
       "2   data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "3   data/Post/Group C_ Post Viewing Questionnaire ...  \n",
       "4   data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "..                                                ...  \n",
       "78  data/Post/Group D_ Post Viewing Questionnaire ...  \n",
       "79  data/Post/Group F_ Post Viewing Questionnaire ...  \n",
       "80  data/Post/Group E_ Post Viewing Questionnaire ...  \n",
       "81  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "82  data/Post/Group A_ Post Viewing Questionnaire ...  \n",
       "\n",
       "[83 rows x 167 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 output not found at {stage1_path}.\")\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage1_base[\"respondent\"] = uv_stage1_base[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "form_columns = [col for col in (\"Short Form\", \"Long Form\") if col in uv_stage1_base.columns]\n",
    "forms_lookup = uv_stage1_base[[\"respondent\", *form_columns]].copy() if form_columns else None\n",
    "if forms_lookup is not None:\n",
    "    forms_lookup[\"respondent\"] = forms_lookup[\"respondent\"].astype(str).str.strip()\n",
    "    if forms_lookup[\"respondent\"].duplicated().any():\n",
    "        dup_count = forms_lookup[\"respondent\"].duplicated(keep=False).sum()\n",
    "        print(f\"Warning: {dup_count} duplicate form records detected; using the last occurrence per respondent.\")\n",
    "        forms_lookup = forms_lookup.drop_duplicates(subset=\"respondent\", keep=\"last\")\n",
    "\n",
    "recognition_features = recognition_features.copy()\n",
    "recognition_features[\"respondent\"] = recognition_features[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage3 = uv_stage1_base.copy()\n",
    "if forms_lookup is not None:\n",
    "    uv_stage3 = uv_stage3.drop(columns=form_columns, errors=\"ignore\")\n",
    "    uv_stage3 = uv_stage3.merge(forms_lookup, on=\"respondent\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "uv_stage3 = uv_stage3.merge(recognition_features, on=\"respondent\", how=\"left\")\n",
    "uv_stage3 = uv_stage3.sort_values(\"respondent\").reset_index(drop=True)\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "stage3_path = results_dir / \"uv_stage3.csv\"\n",
    "uv_stage3.to_csv(stage3_path, index=False)\n",
    "print(f\"Stage 3 UV shape: {uv_stage3.shape}; saved to {stage3_path.name}.\")\n",
    "\n",
    "issues_path = results_dir / \"uv_stage3_issues.csv\"\n",
    "if issues_df.empty:\n",
    "    if issues_path.exists():\n",
    "        issues_path.unlink()\n",
    "    print(\"No Stage 3 recognition parsing issues detected.\")\n",
    "else:\n",
    "    issues_df.to_csv(issues_path, index=False)\n",
    "    print(f\"Logged {issues_df.shape[0]} recognition parsing issues to {issues_path.name}.\")\n",
    "\n",
    "uv_stage3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68169d9",
   "metadata": {},
   "source": [
    "## Stage 4: Open-Ended Integration\n",
    "- Merge Stage 2 and Stage 3 open-ended responses onto the Stage 1 baseline to maintain the respondent-level schema.\n",
    "- Filter prompts via `data/survey_questions.csv` and `data/post_survey_map.csv` so the export captures the open-ended set plus required follow-ups (`E21`, `E22`, `WBD1`, `WBD2`, `Q15`, `Q18`).\n",
    "- Normalize legacy `question1` markers to `E22` before applying the `{form}_{title}_{Survey/Post}_{subscale}_{question_code}` naming pattern.\n",
    "- Persist the enriched table to `results/uv_open_ended.csv` for downstream qualitative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e185134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote results\\uv_open_ended.csv with shape (83, 59).\n",
      "Stage 4 open-ended shape: (83, 59); saved to uv_open_ended.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Mad Max_Survey_Enjoyment_E22</th>\n",
       "      <th>Short_The Town_Post_ComprehensionHelp_Q15-1</th>\n",
       "      <th>Short_The Town_Post_ComprehensionImprove_Q15-1</th>\n",
       "      <th>Short_The Town_Post_Comprehension_Q15</th>\n",
       "      <th>Short_The Town_Post_Recall_Q13</th>\n",
       "      <th>Short_The Town_Survey_Enjoyment_E16</th>\n",
       "      <th>Short_The Town_Survey_Enjoyment_E21</th>\n",
       "      <th>Short_The Town_Survey_Enjoyment_E22</th>\n",
       "      <th>post_open_source_path</th>\n",
       "      <th>survey_open_source_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>She was able to share her gift with people who...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>data\\Post\\Group A_ Post Viewing Questionnaire ...</td>\n",
       "      <td>data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>I'm not usually a fan of horror. This scene br...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>data\\Post\\Group A_ Post Viewing Questionnaire ...</td>\n",
       "      <td>data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>It was from one of my favorite horror movies t...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>data\\Post\\Group F_ Post Viewing Questionnaire ...</td>\n",
       "      <td>data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_file group  respondent  date_study time_study  age age_group gender  \\\n",
       "0  003_104.csv     A         104  10/16/2025   18:09:03   59     44-59   Male   \n",
       "1  002_106.csv     A         106  10/16/2025   19:35:05   30     28-43   Male   \n",
       "2  001_116.csv     A         116  10/18/2025   12:37:40   19     18-27   Male   \n",
       "\n",
       "  ethnicity                income_group  ...  \\\n",
       "0     White    $60,000 or more per year  ...   \n",
       "1     White    $60,000 or more per year  ...   \n",
       "2     White  $35,000  $60,000 per year  ...   \n",
       "\n",
       "                  Short_Mad Max_Survey_Enjoyment_E22  \\\n",
       "0  She was able to share her gift with people who...   \n",
       "1  I'm not usually a fan of horror. This scene br...   \n",
       "2  It was from one of my favorite horror movies t...   \n",
       "\n",
       "   Short_The Town_Post_ComprehensionHelp_Q15-1  \\\n",
       "0                                         None   \n",
       "1                                         None   \n",
       "2                                         None   \n",
       "\n",
       "   Short_The Town_Post_ComprehensionImprove_Q15-1  \\\n",
       "0                                            None   \n",
       "1                                            None   \n",
       "2                                            None   \n",
       "\n",
       "   Short_The Town_Post_Comprehension_Q15 Short_The Town_Post_Recall_Q13  \\\n",
       "0                                   None                           None   \n",
       "1                                   None                           None   \n",
       "2                                   None                           None   \n",
       "\n",
       "  Short_The Town_Survey_Enjoyment_E16 Short_The Town_Survey_Enjoyment_E21  \\\n",
       "0                                 NaN                                 NaN   \n",
       "1                                 NaN                                 NaN   \n",
       "2                                 NaN                                 NaN   \n",
       "\n",
       "  Short_The Town_Survey_Enjoyment_E22  \\\n",
       "0                                <NA>   \n",
       "1                                <NA>   \n",
       "2                                <NA>   \n",
       "\n",
       "                               post_open_source_path  \\\n",
       "0  data\\Post\\Group A_ Post Viewing Questionnaire ...   \n",
       "1  data\\Post\\Group A_ Post Viewing Questionnaire ...   \n",
       "2  data\\Post\\Group F_ Post Viewing Questionnaire ...   \n",
       "\n",
       "                             survey_open_source_path  \n",
       "0  data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...  \n",
       "1  data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...  \n",
       "2  data\\Export\\Group A\\Analyses\\Group A-2\\Survey\\...  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 4: compile open-ended survey and post responses within the notebook\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Set, cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "project_root = globals().get(\"project_root\", Path.cwd().parent)\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "BASE_DIR = project_root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "POST_DIR = DATA_DIR / \"Post\"\n",
    "STAGE1_PATH = RESULTS_DIR / \"uv_stage1.csv\"\n",
    "STAGE2_PATH = RESULTS_DIR / \"uv_stage2.csv\"\n",
    "SURVEY_QUESTIONS_PATH = DATA_DIR / \"survey_questions.csv\"\n",
    "SURVEY_RENAME_MAP_PATH = DATA_DIR / \"survey_column_rename_stage3.csv\"\n",
    "POST_MAP_PATH = DATA_DIR / \"post_survey_map.csv\"\n",
    "OUTPUT_PATH = RESULTS_DIR / \"uv_open_ended.csv\"\n",
    "\n",
    "SURVEY_EXTRA_CODES = {\"E21\"}\n",
    "POST_EXTRA_CODES = {\"15\", \"18\"}\n",
    "\n",
    "def normalize_text(value: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(value).strip()).lower()\n",
    "\n",
    "def extract_column_code(column_name: str) -> str:\n",
    "    if \"_\" not in column_name:\n",
    "        return column_name.upper()\n",
    "    return column_name.rsplit(\"_\", 1)[-1].upper()\n",
    "\n",
    "def load_survey_target_metadata(open_code_set: Set[str]) -> pd.DataFrame:\n",
    "    rename_map = pd.read_csv(SURVEY_RENAME_MAP_PATH)\n",
    "    rename_map = rename_map.dropna(\n",
    "        subset=[\"group\", \"raw_column\", \"target_column\", \"question_code\"]\n",
    "    )\n",
    "    rename_map[\"normalized_code\"] = (\n",
    "        rename_map[\"question_code\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"question1\", \"E22\", case=False)\n",
    "        .str.upper()\n",
    "    )\n",
    "    metadata = rename_map.loc[\n",
    "        rename_map[\"normalized_code\"].isin(open_code_set),\n",
    "        [\"group\", \"raw_column\", \"target_column\"],\n",
    "    ].copy()\n",
    "    metadata[\"raw_column\"] = metadata[\"raw_column\"].str.strip()\n",
    "    metadata[\"target_column\"] = metadata[\"target_column\"].str.strip()\n",
    "    metadata = metadata.dropna(subset=[\"raw_column\", \"target_column\"])\n",
    "    metadata = metadata.drop_duplicates()\n",
    "    return metadata\n",
    "\n",
    "def first_non_empty(series: pd.Series) -> object:\n",
    "    for value in series:\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        if isinstance(value, str):\n",
    "            stripped = value.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            return stripped\n",
    "        return value\n",
    "    return pd.NA\n",
    "\n",
    "def extract_stage2_raw_responses(target_metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    if target_metadata.empty:\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for group, group_rows in target_metadata.groupby(\"group\"):\n",
    "        survey_dir = (\n",
    "            DATA_DIR\n",
    "            / \"Export\"\n",
    "            / f\"Group {group}\"\n",
    "            / \"Analyses\"\n",
    "            / f\"Group {group}-2\"\n",
    "            / \"Survey\"\n",
    "        )\n",
    "        if not survey_dir.exists():\n",
    "            continue\n",
    "        group_rows = group_rows.copy()\n",
    "        raw_columns = group_rows[\"raw_column\"].tolist()\n",
    "        rename_lookup = dict(zip(group_rows[\"raw_column\"], group_rows[\"target_column\"]))\n",
    "\n",
    "        for path in sorted(survey_dir.glob(\"MERGED_SURVEY_RESPONSE_MATRIX-*.txt\")):\n",
    "            df_raw = pd.read_csv(\n",
    "                path,\n",
    "                sep=\"\\t\",\n",
    "                engine=\"python\",\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "                dtype=str,\n",
    "            )\n",
    "            respondent_col = None\n",
    "            for candidate in df_raw.columns:\n",
    "                if normalize_text(candidate) in {\n",
    "                    \"respondent\",\n",
    "                    \"respondent id\",\n",
    "                    \"respondentid\",\n",
    "                }:\n",
    "                    respondent_col = candidate\n",
    "                    break\n",
    "            if respondent_col is None:\n",
    "                continue\n",
    "\n",
    "            available_raw = [col for col in raw_columns if col in df_raw.columns]\n",
    "            if not available_raw:\n",
    "                continue\n",
    "\n",
    "            subset_df = df_raw[[respondent_col, *available_raw]].copy()\n",
    "            rename_subset = {raw: rename_lookup[raw] for raw in available_raw}\n",
    "            subset_df = subset_df.rename(columns=rename_subset)\n",
    "            subset_df = subset_df.rename(columns={respondent_col: \"respondent\"})\n",
    "            subset_df[\"survey_open_source_path\"] = str(path.relative_to(BASE_DIR))\n",
    "            subset_df[\"respondent\"] = pd.to_numeric(subset_df[\"respondent\"], errors=\"coerce\")\n",
    "            subset_df = subset_df.dropna(subset=[\"respondent\"])\n",
    "            subset_df[\"respondent\"] = subset_df[\"respondent\"].astype(int)\n",
    "            frames.append(subset_df)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    value_columns = [col for col in combined.columns if col != \"respondent\"]\n",
    "    if not value_columns:\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    aggregated = (\n",
    "        combined.groupby(\"respondent\")[value_columns]\n",
    "        .agg(first_non_empty)\n",
    "        .reset_index()\n",
    "        .sort_values(\"respondent\")\n",
    "    )\n",
    "    return aggregated\n",
    "\n",
    "def format_post_code(raw_code: float | str) -> str:\n",
    "    if pd.isna(raw_code):\n",
    "        raise ValueError(\"Open-ended question code is missing\")\n",
    "    if isinstance(raw_code, str):\n",
    "        raw_code = raw_code.strip()\n",
    "        if raw_code:\n",
    "            try:\n",
    "                numeric = float(raw_code)\n",
    "                return format_post_code(numeric)\n",
    "            except ValueError:\n",
    "                return raw_code\n",
    "        raise ValueError(\"Encountered blank question code\")\n",
    "    whole = int(raw_code)\n",
    "    frac = raw_code - whole\n",
    "    if abs(frac) < 1e-9:\n",
    "        return f\"Q{whole:02d}\"\n",
    "    sub = int(round(frac * 10))\n",
    "    return f\"Q{whole:02d}-{sub}\"\n",
    "\n",
    "def format_post_subscale(subscale: str | float, question: str) -> str:\n",
    "    text = question.lower()\n",
    "    if \"what helped\" in text:\n",
    "        return \"ComprehensionHelp\"\n",
    "    if \"what would have improved\" in text:\n",
    "        return \"ComprehensionImprove\"\n",
    "    if \"what happened\" in text:\n",
    "        return \"Recall\"\n",
    "    if isinstance(subscale, str) and subscale.strip():\n",
    "        return subscale.strip().title().replace(\" \", \"\")\n",
    "    return \"OpenEnded\"\n",
    "\n",
    "def build_stage2_open_ended(open_codes: Iterable[str]) -> pd.DataFrame:\n",
    "    open_code_set = {code.upper() for code in open_codes}\n",
    "    target_metadata = load_survey_target_metadata(open_code_set)\n",
    "\n",
    "    open_file = pd.read_csv(RESULTS_DIR / \"uv_stage2_open_ended.csv\")\n",
    "    source_col = \"survey_file\"\n",
    "    metadata_cols = [\n",
    "        col\n",
    "        for col in open_file.columns\n",
    "        if col.startswith(\"survey_\") and col != source_col\n",
    "    ]\n",
    "    stage2_open = open_file.drop(columns=metadata_cols)\n",
    "    if source_col in stage2_open.columns:\n",
    "        stage2_open = stage2_open.rename(columns={source_col: \"survey_open_source_path\"})\n",
    "        stage2_open[\"survey_open_source_path\"] = stage2_open[\"survey_open_source_path\"].astype(str).str.strip()\n",
    "        stage2_open.loc[stage2_open[\"survey_open_source_path\"] == \"\", \"survey_open_source_path\"] = pd.NA\n",
    "    stage2_open[\"respondent\"] = stage2_open[\"respondent\"].astype(int)\n",
    "\n",
    "    present_codes = {\n",
    "        col.split(\"_\")[-1].upper()\n",
    "        for col in stage2_open.columns\n",
    "        if col != \"respondent\"\n",
    "    }\n",
    "    missing_codes = open_code_set - present_codes\n",
    "\n",
    "    if missing_codes:\n",
    "        stage2 = pd.read_csv(STAGE2_PATH)\n",
    "        extra_cols = [\n",
    "            col\n",
    "            for col in stage2.columns\n",
    "            if (col != \"respondent\" and col.split(\"_\")[-1].upper() in missing_codes)\n",
    "        ]\n",
    "        if extra_cols:\n",
    "            extras = stage2[[\"respondent\", *sorted(extra_cols)]]\n",
    "            stage2_open = stage2_open.merge(extras, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    targets_needed = set(target_metadata[\"target_column\"])\n",
    "    existing_targets = {col for col in stage2_open.columns if col != \"respondent\"}\n",
    "    null_targets = {\n",
    "        col\n",
    "        for col in targets_needed & existing_targets\n",
    "        if stage2_open[col].notna().sum() == 0\n",
    "    }\n",
    "    missing_targets = targets_needed - existing_targets\n",
    "    targets_to_backfill = missing_targets | null_targets\n",
    "\n",
    "    if targets_to_backfill:\n",
    "        raw_metadata = target_metadata[target_metadata[\"target_column\"].isin(targets_to_backfill)]\n",
    "        raw_df = extract_stage2_raw_responses(raw_metadata)\n",
    "        if not raw_df.empty:\n",
    "            stage2_open = stage2_open.set_index(\"respondent\")\n",
    "            raw_df = raw_df.set_index(\"respondent\")\n",
    "            stage2_open = stage2_open.reindex(stage2_open.index.union(raw_df.index))\n",
    "            aligned_raw = raw_df.reindex(stage2_open.index)\n",
    "            for target in targets_to_backfill:\n",
    "                if target not in aligned_raw.columns:\n",
    "                    continue\n",
    "                values = aligned_raw[target]\n",
    "                if target in stage2_open.columns:\n",
    "                    stage2_open[target] = stage2_open[target].where(stage2_open[target].notna(), values)\n",
    "                else:\n",
    "                    stage2_open[target] = values\n",
    "            if \"survey_open_source_path\" in aligned_raw.columns:\n",
    "                path_values = aligned_raw[\"survey_open_source_path\"]\n",
    "                if \"survey_open_source_path\" in stage2_open.columns:\n",
    "                    available = path_values.notna()\n",
    "                    stage2_open.loc[available, \"survey_open_source_path\"] = path_values.loc[available]\n",
    "                else:\n",
    "                    stage2_open[\"survey_open_source_path\"] = path_values\n",
    "            stage2_open = stage2_open.reset_index()\n",
    "            stage2_open[\"respondent\"] = stage2_open[\"respondent\"].astype(int)\n",
    "\n",
    "    allowed_codes = {code.upper() for code in open_code_set}\n",
    "    ordered_cols = [\"respondent\"]\n",
    "    if \"survey_open_source_path\" in stage2_open.columns:\n",
    "        ordered_cols.append(\"survey_open_source_path\")\n",
    "    ordered_cols.extend(\n",
    "        col\n",
    "        for col in stage2_open.columns\n",
    "        if col not in ordered_cols and extract_column_code(col) in allowed_codes\n",
    "    )\n",
    "    stage2_open = stage2_open.loc[:, ordered_cols]\n",
    "\n",
    "    return stage2_open\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PostQuestion:\n",
    "    group: str\n",
    "    question: str\n",
    "    title: str\n",
    "    question_code: str\n",
    "    subscale: str\n",
    "\n",
    "def _normalize_question_code(value: float | str) -> str | None:\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        cleaned = value.strip()\n",
    "        if cleaned.endswith(\".0\"):\n",
    "            cleaned = cleaned[:-2]\n",
    "        return cleaned\n",
    "    if isinstance(value, (int, float)):\n",
    "        if float(value).is_integer():\n",
    "            return str(int(value))\n",
    "        return str(value)\n",
    "    return str(value)\n",
    "\n",
    "def collect_allowed_post_codes(post_map: pd.DataFrame) -> Set[str]:\n",
    "    allowed: Set[str] = set()\n",
    "    for _, row in post_map.iterrows():\n",
    "        raw_code = row.get(\"question_code\")\n",
    "        if pd.isna(raw_code):\n",
    "            continue\n",
    "        normalized = _normalize_question_code(raw_code)\n",
    "        if normalized is None:\n",
    "            continue\n",
    "        is_open = str(row.get(\"type\", \"\")).strip().lower() == \"open-ended\"\n",
    "        is_extra = normalized in POST_EXTRA_CODES\n",
    "        if not (is_open or is_extra):\n",
    "            continue\n",
    "        try:\n",
    "            numeric = float(normalized)\n",
    "            formatted = format_post_code(numeric)\n",
    "        except ValueError:\n",
    "            formatted = format_post_code(normalized)\n",
    "        allowed.add(formatted.upper())\n",
    "    return allowed\n",
    "\n",
    "def explode_post_questions() -> List[PostQuestion]:\n",
    "    post_map = pd.read_csv(POST_MAP_PATH)\n",
    "    extra_codes = {code for code in POST_EXTRA_CODES if code}\n",
    "    mask_open = post_map[\"type\"].astype(str).str.strip().str.lower() == \"open-ended\"\n",
    "    mask_extra = post_map[\"question_code\"].apply(\n",
    "        lambda value: _normalize_question_code(value) in extra_codes\n",
    "    )\n",
    "    open_ended = post_map.loc[mask_open | mask_extra].copy()\n",
    "    questions: List[PostQuestion] = []\n",
    "    for _, row in open_ended.iterrows():\n",
    "        if pd.isna(row[\"question_code\"]):\n",
    "            continue\n",
    "        code = format_post_code(row[\"question_code\"])\n",
    "        subscale = format_post_subscale(row.get(\"subscale\", \"\"), row[\"question\"])\n",
    "        for group in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
    "            title = row.get(f\"Group {group}\")\n",
    "            if isinstance(title, str) and title.strip():\n",
    "                questions.append(\n",
    "                    PostQuestion(\n",
    "                        group=group,\n",
    "                        question=row[\"question\"],\n",
    "                        title=title.strip(),\n",
    "                        question_code=code,\n",
    "                        subscale=subscale,\n",
    "                    )\n",
    "                )\n",
    "    return questions\n",
    "\n",
    "def load_post_open_ended(forms_map: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    questions = explode_post_questions()\n",
    "    grouped: Dict[str, List[PostQuestion]] = {}\n",
    "    for q in questions:\n",
    "        grouped.setdefault(q.group, []).append(q)\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for path in sorted(POST_DIR.glob(\"Group *_*.csv\")):\n",
    "        match = re.search(r\"Group ([A-F])\", path.name)\n",
    "        if not match:\n",
    "            continue\n",
    "        group = match.group(1)\n",
    "        if group not in grouped:\n",
    "            continue\n",
    "        df_raw = pd.read_csv(path)\n",
    "        df_raw.columns = [col.strip() for col in df_raw.columns]\n",
    "        normalized_lookup = {normalize_text(col): col for col in df_raw.columns}\n",
    "        respondent_col = None\n",
    "        for key, col in normalized_lookup.items():\n",
    "            if \"participant number\" in key:\n",
    "                respondent_col = col\n",
    "                break\n",
    "        if respondent_col is None:\n",
    "            raise KeyError(f\"Participant number column missing in {path}\")\n",
    "        respondent_series = df_raw[respondent_col].astype(str).str.extract(r\"(\\d+)\")[0]\n",
    "        df = pd.DataFrame({\"respondent\": pd.to_numeric(respondent_series, errors=\"coerce\")})\n",
    "\n",
    "        long_title = forms_map[group][\"Long Form\"]\n",
    "        short_title = forms_map[group][\"Short Form\"]\n",
    "\n",
    "        for question in grouped[group]:\n",
    "            norm = normalize_text(question.question)\n",
    "            raw_col = normalized_lookup.get(norm)\n",
    "            if raw_col is None:\n",
    "                continue\n",
    "            if question.title == long_title:\n",
    "                form = \"Long\"\n",
    "            elif question.title == short_title:\n",
    "                form = \"Short\"\n",
    "            else:\n",
    "                form = \"All\"\n",
    "            column_name = f\"{form}_{question.title}_Post_{question.subscale}_{question.question_code}\"\n",
    "            df[column_name] = df_raw[raw_col]\n",
    "\n",
    "        df[\"post_open_source_path\"] = str(path.relative_to(BASE_DIR))\n",
    "\n",
    "        df = df.dropna(subset=[\"respondent\"])\n",
    "        df[\"respondent\"] = df[\"respondent\"].astype(int)\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    merged = merged.groupby(\"respondent\", as_index=False).first()\n",
    "    merged = merged.sort_values(\"respondent\")\n",
    "    return merged\n",
    "\n",
    "def build_open_ended_table() -> pd.DataFrame:\n",
    "    stage1 = pd.read_csv(STAGE1_PATH)\n",
    "    stage1_forms = cast(\n",
    "        Dict[str, Dict[str, str]],\n",
    "        stage1[[\"group\", \"Short Form\", \"Long Form\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"group\")\n",
    "        .to_dict(orient=\"index\")\n",
    "    )\n",
    "\n",
    "    survey_questions = pd.read_csv(SURVEY_QUESTIONS_PATH)\n",
    "    open_codes = (\n",
    "        survey_questions.loc[\n",
    "            survey_questions[\"question_type\"].astype(str).str.strip().str.lower() == \"open ended\",\n",
    "            \"question_code\",\n",
    "        ]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"question1\", \"E22\", case=False)\n",
    "    )\n",
    "    open_codes = open_codes.tolist() + list(SURVEY_EXTRA_CODES)\n",
    "\n",
    "    stage2_open = build_stage2_open_ended(open_codes)\n",
    "    post_open = load_post_open_ended(stage1_forms)\n",
    "\n",
    "    combined = stage1.merge(stage2_open, on=\"respondent\", how=\"left\")\n",
    "    combined = combined.merge(post_open, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    allowed_survey_codes = (\n",
    "        survey_questions.loc[\n",
    "            survey_questions[\"question_type\"].astype(str).str.strip().str.lower() == \"open ended\",\n",
    "            \"question_code\",\n",
    "        ]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"question1\", \"E22\", case=False)\n",
    "        .str.upper()\n",
    "    )\n",
    "    allowed_survey_codes = set(allowed_survey_codes).union({code.upper() for code in SURVEY_EXTRA_CODES})\n",
    "\n",
    "    post_map = pd.read_csv(POST_MAP_PATH)\n",
    "    allowed_post_codes = collect_allowed_post_codes(post_map)\n",
    "\n",
    "    allowed_codes_total = allowed_survey_codes.union(allowed_post_codes)\n",
    "\n",
    "    ordered_cols = list(stage1.columns)\n",
    "    optional_cols: List[str] = []\n",
    "    for col in combined.columns:\n",
    "        if col in ordered_cols:\n",
    "            continue\n",
    "        if col in {\"survey_open_source_path\", \"post_open_source_path\"}:\n",
    "            optional_cols.append(col)\n",
    "            continue\n",
    "        code = extract_column_code(col)\n",
    "        if code in allowed_codes_total:\n",
    "            optional_cols.append(col)\n",
    "\n",
    "    combined = combined.loc[:, ordered_cols + sorted(optional_cols)]\n",
    "\n",
    "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if OUTPUT_PATH.exists():\n",
    "        try:\n",
    "            OUTPUT_PATH.unlink()\n",
    "        except PermissionError:\n",
    "            temp_path = OUTPUT_PATH.with_suffix(\".tmp\")\n",
    "            combined.to_csv(temp_path, index=False)\n",
    "            try:\n",
    "                temp_path.replace(OUTPUT_PATH)\n",
    "                rel_path = OUTPUT_PATH.relative_to(BASE_DIR)\n",
    "                print(\n",
    "                    \"Notice: replaced locked open-ended export via temporary file; \"\n",
    "                    f\"wrote {rel_path}.\"\n",
    "                )\n",
    "                return combined\n",
    "            except PermissionError:\n",
    "                rel_temp = temp_path.relative_to(BASE_DIR)\n",
    "                print(\n",
    "                    \"Warning: existing open-ended export is locked; \"\n",
    "                    f\"wrote temporary file to {rel_temp} instead.\"\n",
    "                )\n",
    "                return combined\n",
    "\n",
    "    combined.to_csv(OUTPUT_PATH, index=False)\n",
    "    rel_path = OUTPUT_PATH.relative_to(BASE_DIR)\n",
    "    print(f\"Wrote {rel_path} with shape {combined.shape}.\")\n",
    "    return combined\n",
    "\n",
    "uv_open = build_open_ended_table()\n",
    "uv_open_path = OUTPUT_PATH\n",
    "print(f\"Stage 4 open-ended shape: {uv_open.shape}; saved to {uv_open_path.name}.\")\n",
    "uv_open.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c17a5576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote results\\uv_open_ended_long.csv with 737 rows and 8 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question</th>\n",
       "      <th>format</th>\n",
       "      <th>title</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Seeing the actors talk about eating pizza and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The facial expressions, the emotions were writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>That agree was avoiding answering what his fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>It was just a clear and funny scene. They were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>It was reaction when he looked at the pizza an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The dialogue between characters and my own abi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Would helped my understanding is that I got in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The scene was well written, and even though I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Because Greg was shown to be uncomfortable wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q15-1</td>\n",
       "      <td>ComprehensionHelp</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Facial clues and body movements help me with m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent group questionnaire question_code           question format  \\\n",
       "0         116     A          Post         Q15-1  ComprehensionHelp   Long   \n",
       "1         107     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "2         109     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "3          11     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "4         113     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "5          17     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "6          41     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "7          42     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "8          44     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "9          50     F          Post         Q15-1  ComprehensionHelp   Long   \n",
       "\n",
       "              title                                           response  \n",
       "0  Abbot Elementary  Seeing the actors talk about eating pizza and ...  \n",
       "1  Abbot Elementary  The facial expressions, the emotions were writ...  \n",
       "2  Abbot Elementary  That agree was avoiding answering what his fav...  \n",
       "3  Abbot Elementary  It was just a clear and funny scene. They were...  \n",
       "4  Abbot Elementary  It was reaction when he looked at the pizza an...  \n",
       "5  Abbot Elementary  The dialogue between characters and my own abi...  \n",
       "6  Abbot Elementary  Would helped my understanding is that I got in...  \n",
       "7  Abbot Elementary  The scene was well written, and even though I ...  \n",
       "8  Abbot Elementary  Because Greg was shown to be uncomfortable wit...  \n",
       "9  Abbot Elementary  Facial clues and body movements help me with m...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 4 export → long format\n",
    "from pathlib import Path\n",
    "\n",
    "if \"uv_open\" not in globals():\n",
    "    raise RuntimeError(\"Stage 4 table not found. Run the previous cell first.\")\n",
    "if \"collect_allowed_post_codes\" not in globals():\n",
    "    raise RuntimeError(\"Stage 4 helper functions missing. Re-run the Stage 4 cell.\")\n",
    "\n",
    "survey_questions_fresh = pd.read_csv(SURVEY_QUESTIONS_PATH)\n",
    "allowed_survey_codes = (\n",
    "    survey_questions_fresh.loc[\n",
    "        survey_questions_fresh[\"question_type\"].astype(str).str.strip().str.lower() == \"open ended\",\n",
    "        \"question_code\",\n",
    "    ]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(\"question1\", \"E22\", case=False)\n",
    "    .str.upper()\n",
    ")\n",
    "allowed_survey_codes = set(allowed_survey_codes).union({code.upper() for code in SURVEY_EXTRA_CODES})\n",
    "\n",
    "post_map_fresh = pd.read_csv(POST_MAP_PATH)\n",
    "allowed_post_codes = collect_allowed_post_codes(post_map_fresh)\n",
    "allowed_codes_total = allowed_survey_codes.union(allowed_post_codes)\n",
    "\n",
    "value_columns = [\n",
    "    col for col in uv_open.columns\n",
    "    if ('_Survey_' in col or '_Post_' in col) and not col.lower().endswith('source_path')\n",
    "]\n",
    "value_columns = [\n",
    "    col for col in value_columns if extract_column_code(col) in allowed_codes_total\n",
    "]\n",
    "\n",
    "if not value_columns:\n",
    "    raise ValueError(\"No eligible open-ended response columns detected after filtering.\")\n",
    "\n",
    "def _parse_open_column(name: str):\n",
    "    if '_Survey_' not in name and '_Post_' not in name:\n",
    "        return None\n",
    "    first_sep = name.find('_')\n",
    "    if first_sep == -1:\n",
    "        return None\n",
    "    format_part = name[:first_sep]\n",
    "    remainder = name[first_sep + 1 :]\n",
    "    second_sep = remainder.find('_')\n",
    "    if second_sep == -1:\n",
    "        return None\n",
    "    title_part = remainder[:second_sep]\n",
    "    remainder2 = remainder[second_sep + 1 :]\n",
    "    third_sep = remainder2.find('_')\n",
    "    if third_sep == -1:\n",
    "        return None\n",
    "    questionnaire_part = remainder2[:third_sep]\n",
    "    rest = remainder2[third_sep + 1 :]\n",
    "    last_sep = rest.rfind('_')\n",
    "    if last_sep == -1:\n",
    "        question_part = ''\n",
    "        question_code = rest\n",
    "    else:\n",
    "        question_part = rest[:last_sep]\n",
    "        question_code = rest[last_sep + 1 :]\n",
    "    return {\n",
    "        'format': format_part,\n",
    "        'title': title_part,\n",
    "        'questionnaire': questionnaire_part,\n",
    "        'question': question_part.replace('_', ' ').strip() or pd.NA,\n",
    "        'question_code': question_code,\n",
    "    }\n",
    "\n",
    "long_df = uv_open.melt(\n",
    "    id_vars=[\"respondent\", \"group\"],\n",
    "    value_vars=value_columns,\n",
    "    var_name=\"column_name\",\n",
    "    value_name=\"response\",\n",
    ")\n",
    "\n",
    "metadata_expanded = long_df[\"column_name\"].apply(_parse_open_column)\n",
    "metadata_df = pd.DataFrame(list(metadata_expanded))\n",
    "\n",
    "combined_long = pd.concat([long_df, metadata_df], axis=1)\n",
    "combined_long = combined_long.dropna(subset=[\"response\", \"question_code\"])\n",
    "combined_long[\"response\"] = (\n",
    "    combined_long[\"response\"].astype(str).str.strip().replace({\"\": pd.NA, \"nan\": pd.NA})\n",
    ")\n",
    "combined_long = combined_long.dropna(subset=[\"response\"])\n",
    "combined_long[\"questionnaire\"] = combined_long[\"questionnaire\"].str.title()\n",
    "combined_long[\"format\"] = combined_long[\"format\"].str.title()\n",
    "combined_long[\"title\"] = combined_long[\"title\"].str.strip()\n",
    "combined_long[\"question_code\"] = combined_long[\"question_code\"].str.strip()\n",
    "combined_long[\"question\"] = combined_long[\"question\"].fillna(\"\").str.replace(\"  \", \" \").str.strip()\n",
    "combined_long.loc[combined_long[\"question\"] == \"\", \"question\"] = pd.NA\n",
    "\n",
    "final_columns = [\n",
    "    \"respondent\",\n",
    "    \"group\",\n",
    "    \"questionnaire\",\n",
    "    \"question_code\",\n",
    "    \"question\",\n",
    "    \"format\",\n",
    "    \"title\",\n",
    "    \"response\",\n",
    "]\n",
    "open_long = combined_long[final_columns].reset_index(drop=True)\n",
    "\n",
    "long_output_path = RESULTS_DIR / \"uv_open_ended_long.csv\"\n",
    "open_long.to_csv(long_output_path, index=False)\n",
    "\n",
    "print(\n",
    "    \"Wrote\", long_output_path.relative_to(project_root),\n",
    "    \"with\", open_long.shape[0], \"rows and\", open_long.shape[1], \"columns.\"\n",
    ")\n",
    "open_long.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff04708",
   "metadata": {},
   "source": [
    "## Stage 5.1: LLM-Based Recall Scoring\n",
    "\n",
    "This stage applies automated qualitative scoring to open-ended recall responses using GPT-4.1. The workflow:\n",
    "- Loads model event sequences from `data/model_answers_events.md` for each title/format combination\n",
    "- Reads long-form recall responses from `results/uv_open_ended_long_recall.csv` (Stage 4 output)\n",
    "- Batches responses and calls the OpenAI Responses API to generate recall quality scores (0-100), confidence scores (0-100), and brief rationales\n",
    "- Exports scored responses to `results/recall_coded_responses_full.csv` for downstream merging into the UV\n",
    "\n",
    "**Prerequisites**: Valid `OPENAI_API_KEY` environment variable and `data/model_answers_events.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40764676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model events from: model_answers_events.md\n",
      "Loading recall responses from: uv_open_ended_long_recall.csv\n",
      "OpenAI client available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from io import StringIO\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    OpenAI = None\n",
    "\n",
    "# Configuration\n",
    "MODEL_EVENTS_PATH = project_root / \"data\" / \"model_answers_events.md\"\n",
    "OPEN_ENDED_RECALL_PATH = RESULTS_DIR / \"uv_open_ended_long_recall.csv\"\n",
    "RECALL_OUTPUT_PATH = RESULTS_DIR / \"recall_coded_responses_full.csv\"\n",
    "KEY_MOMENT_ERROR_LOG_PATH = RESULTS_DIR / \"recall_coded_responses_errors.csv\"\n",
    "MODEL_NAME = \"gpt-4.1\"\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "# Initialize OpenAI client\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY) if (OpenAI and OPENAI_API_KEY) else None\n",
    "\n",
    "if not MODEL_EVENTS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model events file not found: {MODEL_EVENTS_PATH}. \"\n",
    "        \"Please ensure data/model_answers_events.md exists.\"\n",
    "    )\n",
    "\n",
    "if not OPEN_ENDED_RECALL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Open-ended recall data not found: {OPEN_ENDED_RECALL_PATH}. \"\n",
    "        \"Stage 4 must complete before running recall scoring.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loading model events from: {MODEL_EVENTS_PATH.name}\")\n",
    "print(f\"Loading recall responses from: {OPEN_ENDED_RECALL_PATH.name}\")\n",
    "print(f\"OpenAI client available: {openai_client is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aa15334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 5 helper functions imported from wbdlib.\n"
     ]
    }
   ],
   "source": [
    "# Stage 5 helper functions shared by Stage 5.1 and Stage 5.2\n",
    "from wbdlib import (\n",
    "    SYSTEM_PROMPT_STAGE51,\n",
    "    build_batch_prompt,\n",
    "    call_llm_batch,\n",
    "    describe_event_source,\n",
    "    enrich_dataframe_with_scores,\n",
    "    normalise_form,\n",
    "    normalise_title,\n",
    "    parse_llm_json,\n",
    "    parse_model_events,\n",
    "    resolve_event_list,\n",
    " )\n",
    "\n",
    "print(\"Stage 5 helper functions imported from wbdlib.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79503eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded events for 6 title/format combinations\n",
      "Parsed combinations:\n",
      "  - Title: abbot elementary | Form: long\n",
      "  - Title: abbot elementary | Form: short\n",
      "  - Title: mad max | Form: long\n",
      "  - Title: mad max | Form: short\n",
      "  - Title: the town | Form: long\n",
      "  - Title: the town | Form: short\n",
      "Loaded 162 recall responses\n",
      "Unique titles: 3 | Forms: ['Long', 'Short']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question</th>\n",
       "      <th>form</th>\n",
       "      <th>title</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>A</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q13</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The black male actor lied about liking pizza a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q13</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The guy grabbing the pizza in reality doesn’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>F</td>\n",
       "      <td>Post</td>\n",
       "      <td>Q13</td>\n",
       "      <td>Recall</td>\n",
       "      <td>Long</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Greg, Jacob and staff were having lunch in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  respondent group questionnaire question_code question  form  \\\n",
       "0   0         116     A          Post           Q13   Recall  Long   \n",
       "1   1         107     F          Post           Q13   Recall  Long   \n",
       "2   2         109     F          Post           Q13   Recall  Long   \n",
       "\n",
       "              title                                           response  \n",
       "0  Abbot Elementary  The black male actor lied about liking pizza a...  \n",
       "1  Abbot Elementary  The guy grabbing the pizza in reality doesn’t ...  \n",
       "2  Abbot Elementary  Greg, Jacob and staff were having lunch in the...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and parse model events\n",
    "model_events_text = MODEL_EVENTS_PATH.read_text(encoding=\"utf-8\")\n",
    "model_events_lookup = parse_model_events(model_events_text)\n",
    "print(f\"Loaded events for {len(model_events_lookup)} title/format combinations\")\n",
    "print(\"Parsed combinations:\")\n",
    "for title_key, form_key in sorted(model_events_lookup.keys()):\n",
    "    print(f\"  - Title: {title_key} | Form: {form_key}\")\n",
    "\n",
    "# Load recall responses\n",
    "try:\n",
    "    recall_df = pd.read_csv(OPEN_ENDED_RECALL_PATH)\n",
    "except UnicodeDecodeError:\n",
    "    with OPEN_ENDED_RECALL_PATH.open(\"r\", encoding=\"cp1252\", errors=\"ignore\") as fh:\n",
    "        buffer = StringIO(fh.read())\n",
    "    recall_df = pd.read_csv(buffer)\n",
    "\n",
    "# Normalise column names for downstream logic\n",
    "recall_df.columns = [col.strip().lower() for col in recall_df.columns]\n",
    "column_aliases = {\n",
    "    \"format\": \"form\",\n",
    "    \"media_format\": \"form\",\n",
    "    \"media_form\": \"form\",\n",
    "    \"content_form\": \"form\",\n",
    "}\n",
    "for candidate, target in column_aliases.items():\n",
    "    if candidate in recall_df.columns and target not in recall_df.columns:\n",
    "        recall_df = recall_df.rename(columns={candidate: target})\n",
    "if \"form\" not in recall_df.columns:\n",
    "    available = \", \".join(sorted(recall_df.columns))\n",
    "    raise KeyError(f\"Expected a 'form' column in recall data; available columns: {available}\")\n",
    "\n",
    "if \"id\" not in recall_df.columns:\n",
    "    recall_df.insert(0, \"id\", range(len(recall_df)))\n",
    "\n",
    "print(f\"Loaded {len(recall_df)} recall responses\")\n",
    "print(f\"Unique titles: {recall_df['title'].nunique()} | Forms: {sorted(recall_df['form'].unique().tolist())}\")\n",
    "recall_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adffeef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_llm_json helper imported from wbdlib.\n"
     ]
    }
   ],
   "source": [
    "# parse_llm_json helper provided by wbdlib\n",
    "from wbdlib import parse_llm_json\n",
    "\n",
    "print(\"parse_llm_json helper imported from wbdlib.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e096fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall dataframe shape: (162, 9)\n",
      "Recall columns: ['form', 'group', 'id', 'question', 'question_code', 'questionnaire', 'respondent', 'response', 'title']\n",
      "Unique titles: ['Abbot Elementary', 'Mad Max', 'The Town']\n",
      "Forms present: ['Long', 'Short']\n",
      "Total model event combos: 6\n"
     ]
    }
   ],
   "source": [
    "# Stage 5.1 sanity summary (deduplicated)\n",
    "if 'recall_df' not in globals():\n",
    "    raise RuntimeError(\"recall_df is not defined. Run the earlier Stage 5.1 setup cells first.\")\n",
    "if 'model_events_lookup' not in globals():\n",
    "    raise RuntimeError(\"model_events_lookup is not defined. Run the event parsing cell first.\")\n",
    "\n",
    "print(f\"Recall dataframe shape: {recall_df.shape}\")\n",
    "print(f\"Recall columns: {sorted(recall_df.columns.tolist())}\")\n",
    "print(f\"Unique titles: {sorted(recall_df['title'].unique().tolist())}\")\n",
    "print(f\"Forms present: {sorted(recall_df['form'].unique().tolist())}\")\n",
    "print(f\"Total model event combos: {len(model_events_lookup)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a03f099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 162 recall responses in batches of 3 using gpt-4.1.\n",
      "  ✓ Batch 1 scored (3 rows)\n",
      "  ✓ Batch 2 scored (3 rows)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[32m     21\u001b[39m     missing_event_keys.update(missing_keys)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m raw_response = \u001b[43mcall_llm_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m batch_results = parse_llm_json(raw_response)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Normalise data types for downstream merging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\analysis\\wbdlib\\recall_scoring.py:255\u001b[39m, in \u001b[36mcall_llm_batch\u001b[39m\u001b[34m(prompt, client_obj, model, system_prompt, max_retries, sleep_seconds)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m         response = \u001b[43mclient_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.output_text\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:840\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    804\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    805\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    838\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    839\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute Method 2 recall scoring and persist outputs\n",
    "if \"recall_df\" not in globals():\n",
    "    raise RuntimeError(\"recall_df is not defined. Re-run the Stage 5.1 setup cells first.\")\n",
    "if \"model_events_lookup\" not in globals():\n",
    "    raise RuntimeError(\"model_events_lookup is not defined. Re-run the event parsing cell first.\")\n",
    "if openai_client is None:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is not configured. Set it before running Stage 5.1 recall scoring.\")\n",
    "\n",
    "if recall_df.empty:\n",
    "    raise ValueError(\"Recall dataframe is empty; nothing to score.\")\n",
    "\n",
    "print(f\"Scoring {len(recall_df)} recall responses in batches of {BATCH_SIZE} using {MODEL_NAME}.\")\n",
    "\n",
    "all_results: List[Dict[str, object]] = []\n",
    "missing_event_keys: set[Tuple[str, str]] = set()\n",
    "\n",
    "for batch_index, start in enumerate(range(0, len(recall_df), BATCH_SIZE), start=1):\n",
    "    batch_df = recall_df.iloc[start : start + BATCH_SIZE]\n",
    "    prompt_text, missing_keys = build_batch_prompt(batch_df, model_events_lookup)\n",
    "    if missing_keys:\n",
    "        missing_event_keys.update(missing_keys)\n",
    "\n",
    "    raw_response = call_llm_batch(\n",
    "        prompt_text,\n",
    "        client_obj=openai_client,\n",
    "        model=MODEL_NAME,\n",
    "    )\n",
    "    batch_results = parse_llm_json(raw_response)\n",
    "\n",
    "    # Normalise data types for downstream merging\n",
    "    for entry in batch_results:\n",
    "        entry[\"id\"] = int(entry[\"id\"])\n",
    "        entry[\"recall_score\"] = int(entry[\"recall_score\"])\n",
    "        entry[\"confidence_score\"] = int(entry[\"confidence_score\"])\n",
    "\n",
    "    all_results.extend(batch_results)\n",
    "    print(f\"  ✓ Batch {batch_index} scored ({len(batch_results)} rows)\")\n",
    "\n",
    "if missing_event_keys:\n",
    "    print(\"Warning: Missing model events for the following title/form combinations:\")\n",
    "    for title_key, form_key in sorted(missing_event_keys):\n",
    "        print(f\"  - Title: {title_key} | Form: {form_key}\")\n",
    "\n",
    "if not all_results:\n",
    "    raise RuntimeError(\"No scores were returned by the LLM. Aborting Stage 5.1 output.\")\n",
    "\n",
    "scored_recall_df = enrich_dataframe_with_scores(recall_df, all_results)\n",
    "scored_recall_df = scored_recall_df.sort_values([\"respondent\", \"title\", \"form\", \"id\"]).reset_index(drop=True)\n",
    "\n",
    "score_columns = [\"recall_score\", \"confidence_score\"]\n",
    "valid_scores = scored_recall_df.dropna(subset=score_columns)\n",
    "missing_count = len(scored_recall_df) - len(valid_scores)\n",
    "\n",
    "scored_recall_df.to_csv(RECALL_OUTPUT_PATH, index=False)\n",
    "print(\"\\nRecall scoring complete.\")\n",
    "print(f\"  Output saved to: {RECALL_OUTPUT_PATH.relative_to(project_root)}\")\n",
    "print(f\"  Total rows: {len(scored_recall_df)}\")\n",
    "print(f\"  Scored rows: {len(valid_scores)}\")\n",
    "print(f\"  Rows missing scores: {missing_count}\")\n",
    "\n",
    "summary = valid_scores[score_columns].describe()\n",
    "print(\"\\nScore distribution (valid rows):\")\n",
    "print(summary)\n",
    "\n",
    "scored_recall_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8f875",
   "metadata": {},
   "source": [
    "## Stage 5.2: Key-Moment Recall Scoring\n",
    "\n",
    "This stage re-scores the open-ended recall responses using the short-form key-moment event lists. Long-form respondents are evaluated against the short-form benchmark so that recall metrics are comparable across formats. The harness reuses the Stage 5 helper functions, prefers short-form events when available, and writes `results/recall_coded_responses_key_moment.csv`. Run these cells only after the Stage 5.2 prompt and pilot have been approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c143ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 5.2 configuration ready. Review SYSTEM_PROMPT_STAGE52 before running the scoring cell.\n"
     ]
    }
   ],
   "source": [
    "KEY_MOMENT_OUTPUT_PATH = RESULTS_DIR / \"recall_coded_responses_key_moment.csv\"\n",
    "KEY_MOMENT_ERROR_LOG_PATH = RESULTS_DIR / \"recall_coded_responses_key_moment_errors.csv\"\n",
    "SYSTEM_PROMPT_STAGE52 = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    You are an expert qualitative coder focusing on key-moment recall for media research.\n",
    "    Scoring guidelines:\n",
    "    - The MODEL EVENTS describe the short-form key moment. Apply them even when the participant viewed the long-form cut.\n",
    "    - Compare the participant response to the key-moment events and assess number of events recalled, accuracy, specificity, and ordering.\n",
    "    - If the participant explicitly says they do not remember, set recall_score to 0 and confidence_score to at least 90.\n",
    "    - Ignore long-form-only details that are not part of the key moment when assigning the score.\n",
    "    Return a valid JSON array containing one object per response with fields:\n",
    "      • \"id\" (integer row identifier)\n",
    "      • \"recall_score\" (integer 0-100)\n",
    "      • \"confidence_score\" (integer 0-100)\n",
    "      • \"rationale\" (1-3 sentences referencing the MODEL EVENTS)\n",
    "    Do not include any commentary outside of the JSON array.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "print(\"Stage 5.2 configuration ready. Review SYSTEM_PROMPT_STAGE52 and KEY_MOMENT_ERROR_LOG_PATH before running the scoring cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 162 recall responses in batches of 3 using gpt-4.1.\n",
      "Long-form respondents will be evaluated against short-form key-moment events.\n",
      "  ✓ Batch 1 completed (3 rows)\n",
      "  ✓ Batch 1 completed (3 rows)\n",
      "  ✓ Batch 2 completed (3 rows)\n",
      "  ✓ Batch 2 completed (3 rows)\n",
      "  ✓ Batch 3 completed (3 rows)\n",
      "  ✓ Batch 3 completed (3 rows)\n",
      "  ✓ Batch 4 completed (3 rows)\n",
      "  ✓ Batch 4 completed (3 rows)\n",
      "  ✓ Batch 5 completed (3 rows)\n",
      "  ✓ Batch 5 completed (3 rows)\n",
      "  ✓ Batch 6 completed (3 rows)\n",
      "  ✓ Batch 6 completed (3 rows)\n",
      "  ✓ Batch 7 completed (3 rows)\n",
      "  ✓ Batch 7 completed (3 rows)\n",
      "  ✓ Batch 8 completed (3 rows)\n",
      "  ✓ Batch 8 completed (3 rows)\n",
      "  ✓ Batch 9 completed (3 rows)\n",
      "  ✓ Batch 9 completed (3 rows)\n",
      "  ✓ Batch 10 completed (3 rows)\n",
      "  ✓ Batch 10 completed (3 rows)\n",
      "  ✓ Batch 11 completed (3 rows)\n",
      "  ✓ Batch 11 completed (3 rows)\n",
      "  ✓ Batch 12 completed (3 rows)\n",
      "  ✓ Batch 12 completed (3 rows)\n",
      "  ✓ Batch 13 completed (3 rows)\n",
      "  ✓ Batch 13 completed (3 rows)\n",
      "  ✓ Batch 14 completed (3 rows)\n",
      "  ✓ Batch 14 completed (3 rows)\n",
      "  ✓ Batch 15 completed (3 rows)\n",
      "  ✓ Batch 15 completed (3 rows)\n",
      "  ✓ Batch 16 completed (3 rows)\n",
      "  ✓ Batch 16 completed (3 rows)\n",
      "  ✓ Batch 17 completed (3 rows)\n",
      "  ✓ Batch 17 completed (3 rows)\n",
      "  ✓ Batch 18 completed (3 rows)\n",
      "  ✓ Batch 18 completed (3 rows)\n",
      "  ✓ Batch 19 completed (3 rows)\n",
      "  ✓ Batch 19 completed (3 rows)\n",
      "  ✓ Batch 20 completed (3 rows)\n",
      "  ✓ Batch 20 completed (3 rows)\n",
      "  ✓ Batch 21 completed (3 rows)\n",
      "  ✓ Batch 21 completed (3 rows)\n",
      "  ✓ Batch 22 completed (3 rows)\n",
      "  ✓ Batch 22 completed (3 rows)\n",
      "  ✓ Batch 23 completed (3 rows)\n",
      "  ✓ Batch 23 completed (3 rows)\n",
      "  ✓ Batch 24 completed (3 rows)\n",
      "  ✓ Batch 24 completed (3 rows)\n",
      "  ✓ Batch 25 completed (3 rows)\n",
      "  ✓ Batch 25 completed (3 rows)\n",
      "  ✓ Batch 26 completed (3 rows)\n",
      "  ✓ Batch 26 completed (3 rows)\n",
      "  ✓ Batch 27 completed (3 rows)\n",
      "  ✓ Batch 27 completed (3 rows)\n",
      "  ✓ Batch 28 completed (3 rows)\n",
      "  ✓ Batch 28 completed (3 rows)\n",
      "  ✓ Batch 29 completed (3 rows)\n",
      "  ✓ Batch 29 completed (3 rows)\n",
      "  ✓ Batch 30 completed (3 rows)\n",
      "  ✓ Batch 30 completed (3 rows)\n",
      "  ✓ Batch 31 completed (3 rows)\n",
      "  ✓ Batch 31 completed (3 rows)\n",
      "  ✓ Batch 32 completed (3 rows)\n",
      "  ✓ Batch 32 completed (3 rows)\n",
      "  ✓ Batch 33 completed (3 rows)\n",
      "  ✓ Batch 33 completed (3 rows)\n",
      "  ✓ Batch 34 completed (3 rows)\n",
      "  ✓ Batch 34 completed (3 rows)\n",
      "  ✓ Batch 35 completed (3 rows)\n",
      "  ✓ Batch 35 completed (3 rows)\n",
      "  ✓ Batch 36 completed (3 rows)\n",
      "  ✓ Batch 36 completed (3 rows)\n",
      "  ✓ Batch 37 completed (3 rows)\n",
      "  ✓ Batch 37 completed (3 rows)\n",
      "  ✓ Batch 38 completed (3 rows)\n",
      "  ✓ Batch 38 completed (3 rows)\n",
      "  ✓ Batch 39 completed (3 rows)\n",
      "  ✓ Batch 39 completed (3 rows)\n",
      "  ✓ Batch 40 completed (3 rows)\n",
      "  ✓ Batch 40 completed (3 rows)\n",
      "  ✓ Batch 41 completed (3 rows)\n",
      "  ✓ Batch 41 completed (3 rows)\n",
      "  ✓ Batch 42 completed (3 rows)\n",
      "  ✓ Batch 42 completed (3 rows)\n",
      "  ✓ Batch 43 completed (3 rows)\n",
      "  ✓ Batch 43 completed (3 rows)\n",
      "  ✓ Batch 44 completed (3 rows)\n",
      "  ✓ Batch 44 completed (3 rows)\n",
      "  ✓ Batch 45 completed (3 rows)\n",
      "  ✓ Batch 45 completed (3 rows)\n",
      "  ✓ Batch 46 completed (3 rows)\n",
      "  ✓ Batch 46 completed (3 rows)\n",
      "  ✓ Batch 47 completed (3 rows)\n",
      "  ✓ Batch 47 completed (3 rows)\n",
      "  ✓ Batch 48 completed (3 rows)\n",
      "  ✓ Batch 48 completed (3 rows)\n",
      "  ✓ Batch 49 completed (3 rows)\n",
      "  ✓ Batch 49 completed (3 rows)\n",
      "  ✓ Batch 50 completed (3 rows)\n",
      "  ✓ Batch 50 completed (3 rows)\n",
      "  ✓ Batch 51 completed (3 rows)\n",
      "  ✓ Batch 51 completed (3 rows)\n",
      "  ✓ Batch 52 completed (3 rows)\n",
      "  ✓ Batch 52 completed (3 rows)\n",
      "  ✓ Batch 53 completed (3 rows)\n",
      "  ✓ Batch 53 completed (3 rows)\n",
      "  ✓ Batch 54 completed (3 rows)\n",
      "\n",
      "Key-moment scoring complete.\n",
      "  Output saved to: results\\recall_coded_responses_key_moment.csv\n",
      "  Total rows: 162\n",
      "  Scored rows: 162\n",
      "  Rows missing scores: 0\n",
      "\n",
      "Event form usage summary:\n",
      "form_requested event_form_used  rerun  count\n",
      "          Long           short  False     81\n",
      "         Short           short  False     81\n",
      "  ✓ Batch 54 completed (3 rows)\n",
      "\n",
      "Key-moment scoring complete.\n",
      "  Output saved to: results\\recall_coded_responses_key_moment.csv\n",
      "  Total rows: 162\n",
      "  Scored rows: 162\n",
      "  Rows missing scores: 0\n",
      "\n",
      "Event form usage summary:\n",
      "form_requested event_form_used  rerun  count\n",
      "          Long           short  False     81\n",
      "         Short           short  False     81\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KEY_MOMENT_ERROR_LOG_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 261\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    255\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⚠ \u001b[39m\u001b[38;5;132;01m{count}\u001b[39;00m\u001b[33m responses could not be scored even after rerun. Details saved to \u001b[39m\u001b[38;5;132;01m{path}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    256\u001b[39m             count=\u001b[38;5;28mlen\u001b[39m(error_df),\n\u001b[32m    257\u001b[39m             path=KEY_MOMENT_ERROR_LOG_PATH.relative_to(project_root),\n\u001b[32m    258\u001b[39m         )\n\u001b[32m    259\u001b[39m     )\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mKEY_MOMENT_ERROR_LOG_PATH\u001b[49m.exists():\n\u001b[32m    262\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    263\u001b[39m             KEY_MOMENT_ERROR_LOG_PATH.unlink()\n",
      "\u001b[31mNameError\u001b[39m: name 'KEY_MOMENT_ERROR_LOG_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Stage 5.2 key-moment scoring (execute only after approval)\n",
    "if \"recall_df\" not in globals():\n",
    "    raise RuntimeError(\"recall_df is not defined. Run the Stage 5.1 setup cells first.\")\n",
    "if \"model_events_lookup\" not in globals():\n",
    "    raise RuntimeError(\"model_events_lookup is not defined. Run the Stage 5.1 event parsing cell first.\")\n",
    "if openai_client is None:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is not configured. Set it before running Stage 5.2 key-moment scoring.\")\n",
    "if recall_df.empty:\n",
    "    raise ValueError(\"Recall dataframe is empty; nothing to score.\")\n",
    "\n",
    "print(f\"Scoring {len(recall_df)} recall responses in batches of {BATCH_SIZE} using {MODEL_NAME}.\")\n",
    "print(\"Long-form respondents will be evaluated against short-form key-moment events.\")\n",
    "\n",
    "all_keymoment_results: List[Dict[str, object]] = []\n",
    "missing_keymoment_events: set[Tuple[str, str]] = set()\n",
    "key_moment_metadata_frames: List[pd.DataFrame] = []\n",
    "error_records: List[Dict[str, object]] = []\n",
    "\n",
    "for batch_index, start in enumerate(range(0, len(recall_df), BATCH_SIZE), start=1):\n",
    "    batch_df = recall_df.iloc[start : start + BATCH_SIZE]\n",
    "    expected_ids = [int(value) for value in batch_df[\"id\"].tolist()]\n",
    "\n",
    "    prompt_result = build_batch_prompt(\n",
    "        batch_df,\n",
    "        model_events_lookup,\n",
    "        prefer_short_for_long=True,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    if len(prompt_result) == 3:\n",
    "        prompt_text, missing_keys, metadata_df = prompt_result\n",
    "    else:\n",
    "        prompt_text, missing_keys = prompt_result\n",
    "        metadata_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"id\",\n",
    "                \"respondent\",\n",
    "                \"title\",\n",
    "                \"form_requested\",\n",
    "                \"event_form_used\",\n",
    "                \"event_count\",\n",
    "                \"event_label\",\n",
    "            ]\n",
    "        )\n",
    "    if missing_keys:\n",
    "        missing_keymoment_events.update(missing_keys)\n",
    "    if metadata_df is None or metadata_df.empty:\n",
    "        metadata_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"id\",\n",
    "                \"respondent\",\n",
    "                \"title\",\n",
    "                \"form_requested\",\n",
    "                \"event_form_used\",\n",
    "                \"event_count\",\n",
    "                \"event_label\",\n",
    "            ]\n",
    "        )\n",
    "    metadata_df = metadata_df.copy()\n",
    "    metadata_df[\"batch_index\"] = batch_index\n",
    "    metadata_df[\"rerun\"] = False\n",
    "    key_moment_metadata_frames.append(metadata_df)\n",
    "\n",
    "    raw_response = call_llm_batch(\n",
    "        prompt_text,\n",
    "        client_obj=openai_client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt=SYSTEM_PROMPT_STAGE52,\n",
    "    )\n",
    "    batch_results_raw = parse_llm_json(raw_response)\n",
    "\n",
    "    result_by_id: Dict[int, Dict[str, object]] = {}\n",
    "    invalid_ids: set[int] = set()\n",
    "\n",
    "    for entry in batch_results_raw:\n",
    "        rid_raw = entry.get(\"id\")\n",
    "        try:\n",
    "            rid = int(rid_raw)\n",
    "            recall_val = int(entry.get(\"recall_score\"))\n",
    "            conf_val = int(entry.get(\"confidence_score\"))\n",
    "        except (TypeError, ValueError):\n",
    "            try:\n",
    "                rid_context = int(rid_raw) if rid_raw is not None else None\n",
    "            except (TypeError, ValueError):\n",
    "                rid_context = None\n",
    "            if rid_context is not None:\n",
    "                invalid_ids.add(rid_context)\n",
    "            continue\n",
    "        entry[\"id\"] = rid\n",
    "        entry[\"recall_score\"] = recall_val\n",
    "        entry[\"confidence_score\"] = conf_val\n",
    "        result_by_id[rid] = entry\n",
    "\n",
    "    missing_ids: set[int] = {rid for rid in expected_ids if rid not in result_by_id}\n",
    "    missing_ids.update(invalid_ids & set(expected_ids))\n",
    "\n",
    "    for missing_id in sorted(missing_ids):\n",
    "        row_df = batch_df.loc[batch_df[\"id\"] == missing_id]\n",
    "        if row_df.empty:\n",
    "            continue\n",
    "        rerun_result = build_batch_prompt(\n",
    "            row_df,\n",
    "            model_events_lookup,\n",
    "            prefer_short_for_long=True,\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        if len(rerun_result) == 3:\n",
    "            single_prompt, single_missing_keys, single_metadata = rerun_result\n",
    "        else:\n",
    "            single_prompt, single_missing_keys = rerun_result\n",
    "            single_metadata = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"id\",\n",
    "                    \"respondent\",\n",
    "                    \"title\",\n",
    "                    \"form_requested\",\n",
    "                    \"event_form_used\",\n",
    "                    \"event_count\",\n",
    "                    \"event_label\",\n",
    "                ]\n",
    "            )\n",
    "        if single_missing_keys:\n",
    "            missing_keymoment_events.update(single_missing_keys)\n",
    "        if single_metadata is None or single_metadata.empty:\n",
    "            single_metadata = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"id\",\n",
    "                    \"respondent\",\n",
    "                    \"title\",\n",
    "                    \"form_requested\",\n",
    "                    \"event_form_used\",\n",
    "                    \"event_count\",\n",
    "                    \"event_label\",\n",
    "                ]\n",
    "            )\n",
    "        single_metadata = single_metadata.copy()\n",
    "        single_metadata[\"batch_index\"] = batch_index\n",
    "        single_metadata[\"rerun\"] = True\n",
    "        key_moment_metadata_frames.append(single_metadata)\n",
    "\n",
    "        rerun_success = False\n",
    "        rerun_reason: Optional[str] = None\n",
    "\n",
    "        try:\n",
    "            single_raw = call_llm_batch(\n",
    "                single_prompt,\n",
    "                client_obj=openai_client,\n",
    "                model=MODEL_NAME,\n",
    "                system_prompt=SYSTEM_PROMPT_STAGE52,\n",
    "            )\n",
    "            single_results = parse_llm_json(single_raw)\n",
    "            for entry in single_results:\n",
    "                rid_raw = entry.get(\"id\")\n",
    "                try:\n",
    "                    rid = int(rid_raw)\n",
    "                    recall_val = int(entry.get(\"recall_score\"))\n",
    "                    conf_val = int(entry.get(\"confidence_score\"))\n",
    "                except (TypeError, ValueError):\n",
    "                    continue\n",
    "                if rid == missing_id:\n",
    "                    entry[\"id\"] = rid\n",
    "                    entry[\"recall_score\"] = recall_val\n",
    "                    entry[\"confidence_score\"] = conf_val\n",
    "                    result_by_id[rid] = entry\n",
    "                    rerun_success = True\n",
    "                    break\n",
    "        except Exception as exc:\n",
    "            rerun_reason = str(exc)\n",
    "\n",
    "        if rerun_success:\n",
    "            print(f\"  ↻ Rerun succeeded for id {missing_id}\")\n",
    "            continue\n",
    "\n",
    "        if rerun_reason is None:\n",
    "            rerun_reason = \"LLM returned no valid result after rerun\"\n",
    "        row_info = row_df.iloc[0]\n",
    "        error_records.append(\n",
    "            {\n",
    "                \"id\": int(missing_id),\n",
    "                \"respondent\": row_info.get(\"respondent\", pd.NA),\n",
    "                \"title\": row_info.get(\"title\", pd.NA),\n",
    "                \"form\": row_info.get(\"form\", pd.NA),\n",
    "                \"batch_index\": batch_index,\n",
    "                \"rerun_attempted\": True,\n",
    "                \"reason\": rerun_reason,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  ⚠ Rerun failed for id {missing_id}: {rerun_reason}\")\n",
    "\n",
    "    ordered_results = [result_by_id[rid] for rid in expected_ids if rid in result_by_id]\n",
    "    all_keymoment_results.extend(ordered_results)\n",
    "\n",
    "    unresolved_count = len(expected_ids) - len(ordered_results)\n",
    "    print(f\"  ✓ Batch {batch_index} completed ({len(ordered_results)} rows)\")\n",
    "    if unresolved_count:\n",
    "        print(f\"    ⚠ {unresolved_count} rows remain unresolved after rerun (see error log)\")\n",
    "\n",
    "if missing_keymoment_events:\n",
    "    print(\"Warning: Missing model events for the following title/form combinations:\")\n",
    "    for title_key, form_key in sorted(missing_keymoment_events):\n",
    "        print(f\"  - Title: {title_key} | Form: {form_key}\")\n",
    "\n",
    "if not all_keymoment_results:\n",
    "    raise RuntimeError(\"No scores were returned by the LLM. Aborting Stage 5.2 output.\")\n",
    "\n",
    "scored_keymoment_df = enrich_dataframe_with_scores(recall_df, all_keymoment_results)\n",
    "scored_keymoment_df = scored_keymoment_df.sort_values([\"respondent\", \"title\", \"form\", \"id\"]).reset_index(drop=True)\n",
    "\n",
    "score_columns = [\"recall_score\", \"confidence_score\"]\n",
    "valid_keymoment_scores = scored_keymoment_df.dropna(subset=score_columns)\n",
    "missing_keymoment_count = len(scored_keymoment_df) - len(valid_keymoment_scores)\n",
    "\n",
    "scored_keymoment_df.to_csv(KEY_MOMENT_OUTPUT_PATH, index=False)\n",
    "print(\"\\nKey-moment scoring complete.\")\n",
    "print(f\"  Output saved to: {KEY_MOMENT_OUTPUT_PATH.relative_to(project_root)}\")\n",
    "print(f\"  Total rows: {len(scored_keymoment_df)}\")\n",
    "print(f\"  Scored rows: {len(valid_keymoment_scores)}\")\n",
    "print(f\"  Rows missing scores: {missing_keymoment_count}\")\n",
    "\n",
    "key_moment_event_metadata_columns = [\n",
    "    \"id\",\n",
    "    \"respondent\",\n",
    "    \"title\",\n",
    "    \"form_requested\",\n",
    "    \"event_form_used\",\n",
    "    \"event_count\",\n",
    "    \"event_label\",\n",
    "    \"batch_index\",\n",
    "    \"rerun\",\n",
    "]\n",
    "if key_moment_metadata_frames:\n",
    "    key_moment_event_metadata = pd.concat(key_moment_metadata_frames, ignore_index=True)\n",
    "    for col in key_moment_event_metadata_columns:\n",
    "        if col not in key_moment_event_metadata.columns:\n",
    "            key_moment_event_metadata[col] = pd.NA\n",
    "    key_moment_event_metadata = key_moment_event_metadata[key_moment_event_metadata_columns]\n",
    "else:\n",
    "    key_moment_event_metadata = pd.DataFrame(columns=key_moment_event_metadata_columns)\n",
    "\n",
    "if not key_moment_event_metadata.empty:\n",
    "    usage_summary = (\n",
    "        key_moment_event_metadata.groupby([\"form_requested\", \"event_form_used\", \"rerun\"])\n",
    "        .size()\n",
    "        .rename(\"count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"\\nEvent form usage summary:\")\n",
    "    print(usage_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo event metadata generated\")\n",
    "\n",
    "if error_records:\n",
    "    error_df = pd.DataFrame(error_records)\n",
    "    error_df.to_csv(KEY_MOMENT_ERROR_LOG_PATH, index=False)\n",
    "    print(\n",
    "        \"\\n⚠ {count} responses could not be scored even after rerun. Details saved to {path}\".format(\n",
    "            count=len(error_df),\n",
    "            path=KEY_MOMENT_ERROR_LOG_PATH.relative_to(project_root),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    if KEY_MOMENT_ERROR_LOG_PATH.exists():\n",
    "        try:\n",
    "            KEY_MOMENT_ERROR_LOG_PATH.unlink()\n",
    "        except OSError:\n",
    "            pass\n",
    "    print(\"\\nNo unresolved scoring errors encountered.\")\n",
    "\n",
    "summary_keymoment = valid_keymoment_scores[score_columns].describe()\n",
    "print(\"\\nKey-moment score distribution (valid rows):\")\n",
    "print(summary_keymoment)\n",
    "\n",
    "scored_keymoment_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a67da1",
   "metadata": {},
   "source": [
    "## UV Merge\n",
    "This consolidation step reloads the Stage 2 and Stage 3 outputs, checks their respondent-level metadata against the Stage 1 base, and publishes a merged UV file plus an issues log highlighting any discrepancies.\n",
    "\n",
    "**Stage 5.1 Integration**: If `recall_coded_responses_full.csv` exists (generated by Stage 5.1 LLM recall scoring), recall scores are pivoted and merged into the UV using respondent+title+form as keys, creating columns named `{Form}_{Title}_Post_Recall_OpenEndedSum`.\n",
    "**Stage 5.2 Integration**: If `recall_coded_responses_key_moment.csv` exists (generated by Stage 5.2 key-moment scoring), the same merge logic adds `{Form}_{Title}_Post_Recall_OpenEndedKMS` columns so analysts can compare like-for-like recall across formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e3e759",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage 5.1 scores from: recall_coded_responses_full.csv\n",
      "Loading Stage 5.2 scores from: recall_coded_responses_key_moment.csv\n",
      "Adding Stage 5.1 recall columns to UV:\n",
      "  - Long_Abbot Elementary_Post_Recall_OpenEndedSum\n",
      "  - Long_Mad Max_Post_Recall_OpenEndedSum\n",
      "  - Long_The Town_Post_Recall_OpenEndedSum\n",
      "  - Short_Abbot Elementary_Post_Recall_OpenEndedSum\n",
      "  - Short_Mad Max_Post_Recall_OpenEndedSum\n",
      "  - Short_The Town_Post_Recall_OpenEndedSum\n",
      "Adding Stage 5.2 key-moment recall columns to UV:\n",
      "  - Long_Abbot Elementary_Post_Recall_OpenEndedKMS\n",
      "  - Long_Mad Max_Post_Recall_OpenEndedKMS\n",
      "  - Long_The Town_Post_Recall_OpenEndedKMS\n",
      "  - Short_Abbot Elementary_Post_Recall_OpenEndedKMS\n",
      "  - Short_Mad Max_Post_Recall_OpenEndedKMS\n",
      "  - Short_The Town_Post_Recall_OpenEndedKMS\n",
      "Merged UV exported to results\\uv_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# UV Merge with recall-coded open-ended scores (Stage 5.1 + Stage 5.2)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "stage1_path = results_dir / \"uv_stage1.csv\"\n",
    "stage2_path = results_dir / \"uv_stage2.csv\"\n",
    "stage3_path = results_dir / \"uv_stage3.csv\"\n",
    "\n",
    "for required_path in [stage1_path, stage2_path, stage3_path]:\n",
    "    if not required_path.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {required_path}\")\n",
    "\n",
    "uv_stage1_base = pd.read_csv(stage1_path)\n",
    "uv_stage2 = pd.read_csv(stage2_path)\n",
    "uv_stage3 = pd.read_csv(stage3_path)\n",
    "\n",
    "for df in (uv_stage1_base, uv_stage2, uv_stage3):\n",
    "    df[\"respondent\"] = df[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "baseline_cols = list(uv_stage1_base.columns)\n",
    "baseline_set = set(baseline_cols)\n",
    "\n",
    "def _load_recall_matrix(\n",
    "    candidate_paths: list[Path],\n",
    "    *,\n",
    "    stage_label: str,\n",
    "    column_suffix: str,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Load a recall scoring file, pivot to wide format, and normalise respondent IDs.\"\"\"\n",
    "    path = next((p for p in candidate_paths if p.exists()), None)\n",
    "    if path is None:\n",
    "        print(f\"Notice: {stage_label} file not found; skipping {stage_label.lower()} merge.\")\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    suffix = path.suffix.lower()\n",
    "    print(f\"Loading {stage_label} scores from: {path.name}\")\n",
    "\n",
    "    if suffix == \".csv\":\n",
    "        coded = pd.read_csv(path)\n",
    "    elif suffix in {\".xlsx\", \".xls\"}:\n",
    "        coded = pd.read_excel(path)\n",
    "    elif suffix == \".parquet\":\n",
    "        coded = pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported {stage_label} extension: {path.suffix}\")\n",
    "\n",
    "    coded.columns = [str(col).strip().lower() for col in coded.columns]\n",
    "    required_columns = {\"respondent\", \"form\", \"title\", \"recall_score\"}\n",
    "    missing = required_columns - set(coded.columns)\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"{stage_label} is missing required columns: \" + \", \".join(sorted(missing))\n",
    "        )\n",
    "\n",
    "    coded = coded.copy()\n",
    "    coded[\"respondent\"] = pd.to_numeric(coded[\"respondent\"], errors=\"coerce\")\n",
    "    coded = coded.dropna(subset=[\"respondent\"])\n",
    "    coded[\"respondent\"] = coded[\"respondent\"].astype(int).astype(str)\n",
    "    coded[\"form\"] = coded[\"form\"].astype(str).str.title().str.strip()\n",
    "    coded[\"title\"] = coded[\"title\"].astype(str).str.strip()\n",
    "\n",
    "    records: list[dict[str, object]] = []\n",
    "    for row in coded.itertuples(index=False):\n",
    "        sum_name = f\"{row.form}_{row.title}_{column_suffix}\"\n",
    "        records.append({\n",
    "            \"respondent\": row.respondent,\n",
    "            \"column_name\": sum_name,\n",
    "            \"value\": getattr(row, \"recall_score\"),\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        print(f\"Notice: {stage_label} produced no rows after normalization; skipping merge.\")\n",
    "        return pd.DataFrame(columns=[\"respondent\"])\n",
    "\n",
    "    wide = (\n",
    "        pd.DataFrame(records)\n",
    "        .pivot(index=\"respondent\", columns=\"column_name\", values=\"value\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    wide.columns.name = None\n",
    "    wide[\"respondent\"] = wide[\"respondent\"].astype(int).astype(str)\n",
    "\n",
    "    return wide\n",
    "\n",
    "stage51_candidates = [\n",
    "    results_dir / \"recall_coded_responses_full.csv\",\n",
    "    results_dir / \"recall_coded_responses.csv\",\n",
    "    results_dir / \"coded_responses_full.csv\",\n",
    "    results_dir / \"coded_responses_full.parquet\",\n",
    "    project_root / \"data\" / \"coded_responses_full.csv\",\n",
    "    project_root / \"data\" / \"coded_responses_full.parquet\",\n",
    "    project_root / \"recall_openended\" / \"coded_responses_full.csv\",\n",
    "    project_root / \"recall_openended\" / \"coded_responses_full.parquet\",\n",
    "]\n",
    "stage51_wide = _load_recall_matrix(\n",
    "    stage51_candidates, stage_label=\"Stage 5.1\", column_suffix=\"Post_Recall_OpenEndedSum\"\n",
    " )\n",
    "\n",
    "stage52_candidates = [\n",
    "    results_dir / \"recall_coded_responses_key_moment.csv\",\n",
    "    results_dir / \"recall_coded_responses_key_moment.parquet\",\n",
    "    project_root / \"recall_openended\" / \"key moment\" / \"recall_coded_responses_key_moment.csv\",\n",
    "    project_root / \"recall_openended\" / \"key moment\" / \"recall_coded_responses_key_moment.parquet\",\n",
    "]\n",
    "stage52_wide = _load_recall_matrix(\n",
    "    stage52_candidates, stage_label=\"Stage 5.2\", column_suffix=\"Post_Recall_OpenEndedKMS\"\n",
    " )\n",
    "\n",
    "def _report_new_columns(wide: pd.DataFrame, label: str) -> set[str]:\n",
    "    if wide.empty:\n",
    "        return set()\n",
    "    candidate_cols = set(wide.columns) - {\"respondent\"}\n",
    "    new_cols = sorted(candidate_cols - baseline_set)\n",
    "    if new_cols:\n",
    "        print(f\"Adding {label} columns to UV:\")\n",
    "        for col in new_cols:\n",
    "            print(f\"  - {col}\")\n",
    "    baseline_set.update(candidate_cols)\n",
    "    return candidate_cols\n",
    "\n",
    "_ = _report_new_columns(stage51_wide, \"Stage 5.1 recall\")\n",
    "_ = _report_new_columns(stage52_wide, \"Stage 5.2 key-moment recall\")\n",
    "\n",
    "uv = uv_stage1_base.merge(uv_stage2, on=\"respondent\", how=\"left\")\n",
    "uv = uv.merge(uv_stage3, on=\"respondent\", how=\"left\")\n",
    "\n",
    "if not stage51_wide.empty:\n",
    "    uv = uv.merge(stage51_wide, on=\"respondent\", how=\"left\")\n",
    "if not stage52_wide.empty:\n",
    "    uv = uv.merge(stage52_wide, on=\"respondent\", how=\"left\")\n",
    "\n",
    "uv_output_path = results_dir / \"uv_merged.csv\"\n",
    "uv.to_csv(uv_output_path, index=False)\n",
    "print(f\"Merged UV exported to {uv_output_path.relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e342020",
   "metadata": {},
   "source": [
    "## Client Deliverables\n",
    "This section will curate the tables, exports, and visual summaries required for the client handoff. Populate placeholder cells below with the final deliverable generation workflows once the requirements are finalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9b9ea",
   "metadata": {},
   "source": [
    "### Client Deliverables extraction blueprint\n",
    "1. Load post-study raw CSVs (`data/Post/*`) with `load_post_survey_raw`, normalizing headers and capturing `source_path` plus submitted timestamps.\n",
    "2. Reshape the raw grid to long format via `reshape_post_survey_long`, generating one row per (`respondent`, `question_code`) while cleaning respondent IDs (string coercion, zero-padding, whitespace trim) and standardizing `questionnaire='Post'`.\n",
    "3. Join metadata using `attach_post_metadata`, merging `post_survey_map.csv` on `question_code` to append `question`, `type`, `subscale`, `category`, `accuracy`, and deriving `stimulus_form`/`stimulus_title` by mapping group-specific entries against Stage 1 exposures (`Short Form`, `Long Form`).\n",
    "4. Enrich with scores through `append_post_scores`:\n",
    "   - Melt Stage 3 recognition metrics to long form, parse column tokens to recover `question_code`, and attach binary/likert numeric scores plus confidence.\n",
    "   - Merge Stage 5 recall outputs (`results/recall_coded_responses_full.csv`, `..._key_moment.csv`) to reuse existing LLM `recall_score`, `confidence_score`, and `rationale` as `score_value`, `score_confidence`, and `score_explanation`.\n",
    "5. Finalize dataset by computing `response_clean`/`response_numeric` (numeric parsing for likert/binary, leave multiples as lists), tagging questions without scores, and persisting `results/post_study_survey_responses.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef18c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client Deliverables helpers\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "project_root = globals().get(\"project_root\", Path.cwd().parent)\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RESULTS_DIR = project_root / \"results\"\n",
    "POST_DIR = DATA_DIR / \"Post\"\n",
    "\n",
    "TITLE_NORMALIZATION = {\n",
    "    \"abbott elementary\": \"Abbot Elementary\",\n",
    "    \"abbot elementary\": \"Abbot Elementary\",\n",
    "    \"schitts creek\": \"Schittss Creek\",\n",
    "    \"schittss creek\": \"Schittss Creek\",\n",
    "    \"mad max fury road\": \"Mad Max\",\n",
    "    \"mad max\": \"Mad Max\",\n",
    "    \"the town (long)\": \"The Town\",\n",
    "}\n",
    "\n",
    "YES_VALUES = {\"yes\", \"y\", \"true\", \"1\"}\n",
    "NO_VALUES = {\"no\", \"n\", \"false\", \"0\"}\n",
    "LIKERT_PATTERN = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\")\n",
    "CODE_PATTERN = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
    "\n",
    "def canonicalize_title(raw_title: str | float | None) -> str:\n",
    "    if pd.isna(raw_title):\n",
    "        return \"\"\n",
    "    text = str(raw_title).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return TITLE_NORMALIZATION.get(text.lower(), text)\n",
    "\n",
    "def normalize_question_text(text: str | float | None) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    normalized = re.sub(r\"\\s+\", \" \", str(text)).strip().lower()\n",
    "    normalized = re.sub(r\"\\.(\\d+)$\", \"\", normalized)\n",
    "    return normalized\n",
    "\n",
    "def merge_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged: Dict[str, pd.Series] = {}\n",
    "    column_order: list[str] = []\n",
    "    for column in df.columns:\n",
    "        normalized = re.sub(r\"\\s+\", \" \", str(column)).strip()\n",
    "        normalized = re.sub(r\"\\.(\\d+)$\", \"\", normalized)\n",
    "        series = df[column].astype(\"object\")\n",
    "        series = series.where(series.notna(), np.nan)\n",
    "        series = series.map(lambda v: v.strip() if isinstance(v, str) else v)\n",
    "        if normalized not in merged:\n",
    "            merged[normalized] = series\n",
    "            column_order.append(normalized)\n",
    "        else:\n",
    "            merged[normalized] = merged[normalized].combine_first(series)\n",
    "    return pd.DataFrame({name: merged[name] for name in column_order})\n",
    "\n",
    "def extract_question_code(text: str | float | None) -> str | None:\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = CODE_PATTERN.match(str(text))\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def clean_response(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    if isinstance(value, str):\n",
    "        cleaned = value.strip()\n",
    "        return cleaned if cleaned else np.nan\n",
    "    return value\n",
    "\n",
    "def parse_binary(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip().lower()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    if text in YES_VALUES:\n",
    "        return 1.0\n",
    "    if text in NO_VALUES:\n",
    "        return 0.0\n",
    "    return np.nan\n",
    "\n",
    "def parse_likert(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        lowered = text.lower()\n",
    "        keyword_map = {\n",
    "            \"strongly disagree\": 1.0,\n",
    "            \"disagree\": 2.0,\n",
    "            \"neither agree nor disagree\": 3.0,\n",
    "            \"agree\": 4.0,\n",
    "            \"strongly agree\": 5.0,\n",
    "        }\n",
    "        for keyword, score in keyword_map.items():\n",
    "            if keyword in lowered:\n",
    "                return score\n",
    "    return np.nan\n",
    "\n",
    "def build_forms_lookup(stage1_forms: pd.DataFrame) -> Dict[str, Dict[str, str]]:\n",
    "    lookup: Dict[str, Dict[str, str]] = {}\n",
    "    for row in stage1_forms.itertuples(index=False):\n",
    "        respondent = str(getattr(row, \"respondent\", \"\")).strip()\n",
    "        if not respondent:\n",
    "            continue\n",
    "        short_form = canonicalize_title(getattr(row, \"Short Form\", \"\"))\n",
    "        long_form = canonicalize_title(getattr(row, \"Long Form\", \"\"))\n",
    "        record: Dict[str, str] = {}\n",
    "        if short_form:\n",
    "            record[\"Short\"] = short_form\n",
    "        if long_form:\n",
    "            record[\"Long\"] = long_form\n",
    "        if record:\n",
    "            lookup[respondent] = record\n",
    "    return lookup\n",
    "\n",
    "def build_group_title_form_lookup(stimulus_map: pd.DataFrame) -> Dict[tuple[str, str], str]:\n",
    "    lookup: Dict[tuple[str, str], str] = {}\n",
    "    for row in stimulus_map.itertuples(index=False):\n",
    "        group_letter = str(getattr(row, \"group\", \"\")).strip()\n",
    "        if not group_letter:\n",
    "            continue\n",
    "        if \" \" in group_letter:\n",
    "            group_letter = group_letter.split()[-1]\n",
    "        group_letter = group_letter.upper()\n",
    "        title_clean = canonicalize_title(getattr(row, \"title\", \"\"))\n",
    "        form_clean = str(getattr(row, \"form\", \"\")).strip().title()\n",
    "        if group_letter and title_clean:\n",
    "            lookup[(group_letter, title_clean)] = form_clean\n",
    "    return lookup\n",
    "\n",
    "def load_post_survey_raw(post_dir: Path) -> pd.DataFrame:\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    for path in sorted(post_dir.rglob(\"*.csv\")):\n",
    "        if not path.is_file() or path.name.startswith(\"~$\"):\n",
    "            continue\n",
    "        df_raw = pd.read_csv(path, dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        df_norm = merge_duplicate_columns(df_raw)\n",
    "        participant_col = next((col for col in df_norm.columns if \"participant number\" in col.lower()), None)\n",
    "        if participant_col is None:\n",
    "            raise KeyError(f\"Participant number column not found in {path.name}\")\n",
    "        df_norm[\"respondent\"] = df_norm[participant_col].astype(str).str.strip()\n",
    "        df_norm[\"respondent\"] = df_norm[\"respondent\"].replace({\"\": np.nan, \"nan\": np.nan})\n",
    "        df_norm[\"respondent\"] = df_norm[\"respondent\"].str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        df_norm = df_norm[df_norm[\"respondent\"].notna()]\n",
    "        if df_norm.empty:\n",
    "            continue\n",
    "        timestamp_col = next((col for col in df_norm.columns if col.strip().lower() == \"timestamp\"), None)\n",
    "        if timestamp_col:\n",
    "            timestamp_series = pd.to_datetime(df_norm[timestamp_col], errors=\"coerce\")\n",
    "            df_norm[\"submitted_timestamp\"] = timestamp_series.dt.tz_localize(None).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            df_norm[\"submitted_timestamp\"] = np.nan\n",
    "        group_match = re.search(r\"Group\\s+([A-F])\", path.stem, re.IGNORECASE)\n",
    "        df_norm[\"file_group\"] = group_match.group(1).upper() if group_match else np.nan\n",
    "        try:\n",
    "            relative_path = path.relative_to(project_root)\n",
    "            df_norm[\"source_path\"] = relative_path.as_posix()\n",
    "        except ValueError:\n",
    "            df_norm[\"source_path\"] = path.as_posix()\n",
    "        df_norm[\"questionnaire\"] = \"Post\"\n",
    "        frames.append(df_norm)\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined[\"respondent\"] = combined[\"respondent\"].astype(str).str.strip()\n",
    "    combined.drop_duplicates(subset=[\"respondent\", \"source_path\", \"submitted_timestamp\"], keep=\"last\", inplace=True)\n",
    "    return combined\n",
    "\n",
    "def reshape_post_survey_long(wide_df: pd.DataFrame, value_columns: Iterable[str]) -> pd.DataFrame:\n",
    "    if wide_df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"respondent\", \"group\", \"questionnaire\", \"submitted_timestamp\", \"source_path\",\n",
    "            \"question_text\", \"question_code\", \"response_raw\", \"file_group\", \"short_form_title\", \"long_form_title\"\n",
    "        ])\n",
    "    records: list[dict] = []\n",
    "    base_columns = {\n",
    "        \"respondent\", \"group\", \"questionnaire\", \"submitted_timestamp\", \"source_path\", \"file_group\",\n",
    "        \"short_form_title\", \"long_form_title\"\n",
    "    }\n",
    "    for row in wide_df.to_dict(\"records\"):\n",
    "        base_info = {key: row.get(key) for key in base_columns}\n",
    "        for column in value_columns:\n",
    "            value = row.get(column, np.nan)\n",
    "            records.append({\n",
    "                **base_info,\n",
    "                \"question_text\": column,\n",
    "                \"question_code\": extract_question_code(column),\n",
    "                \"response_raw\": value,\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def attach_post_metadata(long_df: pd.DataFrame, metadata: pd.DataFrame, respondent_forms: Dict[str, Dict[str, str]], group_title_form_lookup: Dict[tuple[str, str], str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if long_df.empty:\n",
    "        return long_df.copy(), pd.DataFrame()\n",
    "    meta = metadata.copy()\n",
    "    meta[\"question_norm\"] = meta[\"question\"].map(normalize_question_text)\n",
    "    group_columns = [col for col in meta.columns if col.startswith(\"Group \")]\n",
    "    merge_columns = [\"question_norm\", \"question\", \"question_code\", \"type\", \"subscale\", \"category\", \"accuracy\", *group_columns]\n",
    "    enriched = long_df.copy()\n",
    "    enriched[\"question_norm\"] = enriched[\"question_text\"].map(normalize_question_text)\n",
    "    enriched = enriched.merge(meta[merge_columns], on=\"question_norm\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "    enriched[\"question_code\"] = enriched[\"question_code\"].fillna(enriched[\"question_code_meta\"])\n",
    "    issues: list[dict] = []\n",
    "    stimulus_titles: list[str] = []\n",
    "    stimulus_forms: list[str] = []\n",
    "    for _, row in enriched.iterrows():\n",
    "        respondent_id = str(row.get(\"respondent\", \"\")).strip()\n",
    "        group_letter = str(row.get(\"group\", \"\") or row.get(\"file_group\", \"\")).strip().upper()\n",
    "        title_value = \"\"\n",
    "        if group_letter:\n",
    "            group_col = f\"Group {group_letter}\"\n",
    "            if group_col in row and pd.notna(row[group_col]):\n",
    "                title_value = canonicalize_title(row[group_col])\n",
    "        if not title_value:\n",
    "            title_value = \"\"\n",
    "        form_value = \"\"\n",
    "        forms_info = respondent_forms.get(respondent_id, {})\n",
    "        if title_value:\n",
    "            for form_label, form_title in forms_info.items():\n",
    "                if canonicalize_title(form_title) == title_value:\n",
    "                    form_value = form_label\n",
    "                    break\n",
    "            if not form_value and group_letter:\n",
    "                form_value = group_title_form_lookup.get((group_letter, title_value), \"\")\n",
    "        if title_value and not form_value and (str(row.get(\"type\", \"\")).lower() == \"recognition\" or str(row.get(\"subscale\", \"\")).lower() == \"recognition\"):\n",
    "            issues.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"question_code\": row.get(\"question_code\"),\n",
    "                \"question_text\": row.get(\"question_text\"),\n",
    "                \"issue\": \"unable_to_resolve_form_for_recognition\",\n",
    "                \"group\": group_letter,\n",
    "            })\n",
    "        stimulus_titles.append(title_value if title_value else np.nan)\n",
    "        stimulus_forms.append(form_value if form_value else np.nan)\n",
    "    enriched[\"stimulus_title\"] = stimulus_titles\n",
    "    enriched[\"stimulus_form\"] = stimulus_forms\n",
    "    enriched.drop(columns=[col for col in enriched.columns if col.startswith(\"Group \")], inplace=True)\n",
    "    enriched.drop(columns=[\"question_norm\", \"question_code_meta\"], inplace=True, errors=\"ignore\")\n",
    "    issues_df = pd.DataFrame(issues)\n",
    "    return enriched, issues_df\n",
    "\n",
    "def build_recognition_score_frame(stage3_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if stage3_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    pattern = re.compile(r\"^(Long|Short)_(.+)_Post_Recognition_Q(\\d+)-(\\d+)$\")\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    for column in stage3_df.columns:\n",
    "        match = pattern.match(column)\n",
    "        if not match:\n",
    "            continue\n",
    "        form_value, raw_title, question_num, variant = match.groups()\n",
    "        question_code = f\"{int(question_num)}.{variant}\"\n",
    "        method = \"stage3_recognition_binary\" if variant == \"1\" else \"stage3_recognition_confidence\"\n",
    "        explanation = (\n",
    "            \"Stage 3 recognition correctness (1 = aligns with accuracy expectation, 0 = incorrect).\"\n",
    "            if variant == \"1\"\n",
    "            else \"Stage 3 recognition confidence parsed to a 0-4 scale.\"\n",
    "        )\n",
    "        frame = stage3_df[[\"respondent\", column]].copy()\n",
    "        frame.rename(columns={column: \"score_value\"}, inplace=True)\n",
    "        frame[\"respondent\"] = frame[\"respondent\"].astype(str).str.strip()\n",
    "        frame[\"stimulus_form\"] = form_value\n",
    "        frame[\"stimulus_title\"] = canonicalize_title(raw_title)\n",
    "        frame[\"question_code\"] = question_code\n",
    "        frame[\"score_method\"] = method\n",
    "        frame[\"score_explanation\"] = explanation\n",
    "        frames.append(frame)\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    recognition_scores = pd.concat(frames, ignore_index=True)\n",
    "    recognition_scores[\"score_value\"] = pd.to_numeric(recognition_scores[\"score_value\"], errors=\"coerce\")\n",
    "    mask_confidence = recognition_scores[\"score_method\"] == \"stage3_recognition_confidence\"\n",
    "    recognition_scores.loc[mask_confidence, \"score_confidence\"] = recognition_scores.loc[mask_confidence, \"score_value\"]\n",
    "    recognition_scores.loc[~mask_confidence, \"score_confidence\"] = np.nan\n",
    "    return recognition_scores\n",
    "\n",
    "def build_recall_score_frame(results_dir: Path) -> pd.DataFrame:\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    file_specs = [\n",
    "        (\"recall_coded_responses_full.csv\", \"stage5_recall_full\"),\n",
    "        (\"recall_coded_responses_key_moment.csv\", \"stage5_recall_keymoment\"),\n",
    "    ]\n",
    "    for filename, method in file_specs:\n",
    "        path = results_dir / filename\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(path)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"respondent\"] = df[\"respondent\"].astype(str).str.strip()\n",
    "        df[\"question_code\"] = df[\"question_code\"].astype(str).str.strip().str.replace(r\"^Q\", \"\", regex=True)\n",
    "        df[\"stimulus_form\"] = df.get(\"form\", \"\").astype(str).str.title()\n",
    "        df[\"stimulus_title\"] = df.get(\"title\", \"\").map(canonicalize_title)\n",
    "        df[\"score_value\"] = pd.to_numeric(df.get(\"recall_score\"), errors=\"coerce\")\n",
    "        df[\"score_confidence\"] = pd.to_numeric(df.get(\"confidence_score\"), errors=\"coerce\")\n",
    "        df[\"score_explanation\"] = df.get(\"rationale\")\n",
    "        df[\"score_method\"] = method\n",
    "        frames.append(df[[\n",
    "            \"respondent\", \"question_code\", \"stimulus_form\", \"stimulus_title\",\n",
    "            \"score_value\", \"score_confidence\", \"score_method\", \"score_explanation\"\n",
    "        ]])\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    scores = pd.concat(frames, ignore_index=True)\n",
    "    priority_map = {\"stage5_recall_keymoment\": 0, \"stage5_recall_full\": 1}\n",
    "    scores[\"priority\"] = scores[\"score_method\"].map(priority_map).fillna(2)\n",
    "    key_cols = [\"respondent\", \"question_code\", \"stimulus_form\", \"stimulus_title\"]\n",
    "    scores.sort_values(key_cols + [\"priority\"], inplace=True)\n",
    "    scores = scores.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    scores.drop(columns=\"priority\", inplace=True)\n",
    "    return scores\n",
    "\n",
    "def append_post_scores(long_df: pd.DataFrame, recognition_scores: pd.DataFrame, recall_scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    combined = long_df.copy()\n",
    "    score_frames: list[pd.DataFrame] = []\n",
    "    for frame in (recognition_scores, recall_scores):\n",
    "        if frame is not None and not frame.empty:\n",
    "            temp = frame.copy()\n",
    "            temp[\"stimulus_form_key\"] = temp[\"stimulus_form\"].fillna(\"__NA__\")\n",
    "            temp[\"stimulus_title_key\"] = temp[\"stimulus_title\"].fillna(\"__NA__\")\n",
    "            score_frames.append(temp)\n",
    "    if not score_frames:\n",
    "        combined[\"score_value\"] = np.nan\n",
    "        combined[\"score_confidence\"] = np.nan\n",
    "        combined[\"score_method\"] = np.nan\n",
    "        combined[\"score_explanation\"] = np.nan\n",
    "        return combined\n",
    "    all_scores = pd.concat(score_frames, ignore_index=True)\n",
    "    combined[\"stimulus_form_key\"] = combined[\"stimulus_form\"].fillna(\"__NA__\")\n",
    "    combined[\"stimulus_title_key\"] = combined[\"stimulus_title\"].fillna(\"__NA__\")\n",
    "    key_cols = [\"respondent\", \"question_code\", \"stimulus_form_key\", \"stimulus_title_key\"]\n",
    "    all_scores = all_scores.sort_values(key_cols)\n",
    "    all_scores = all_scores.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    combined = combined.merge(\n",
    "        all_scores.drop(columns=[\"stimulus_form\", \"stimulus_title\"]),\n",
    "        on=key_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    fallback = all_scores.drop(columns=[\"stimulus_form_key\", \"stimulus_title_key\"])\n",
    "    fallback = fallback.drop_duplicates(subset=[\"respondent\", \"question_code\"], keep=\"first\")\n",
    "    missing_mask = combined[\"score_value\"].isna() & combined[\"question_code\"].notna()\n",
    "    if missing_mask.any():\n",
    "        merged_fallback = (\n",
    "            combined.loc[missing_mask, [\"respondent\", \"question_code\"]]\n",
    "            .merge(fallback, on=[\"respondent\", \"question_code\"], how=\"left\")\n",
    "        )\n",
    "        combined.loc[missing_mask, [\"score_value\", \"score_confidence\", \"score_method\", \"score_explanation\"]] = merged_fallback[[\"score_value\", \"score_confidence\", \"score_method\", \"score_explanation\"]].values\n",
    "    combined.drop(columns=[\"stimulus_form_key\", \"stimulus_title_key\"], inplace=True)\n",
    "    return combined\n",
    "\n",
    "def compute_response_numeric(row) -> float:\n",
    "    question_type = str(row.get(\"type\", \"\")).strip().lower()\n",
    "    if question_type in {\"binary\", \"watch\"}:\n",
    "        return parse_binary(row.get(\"response_clean\"))\n",
    "    if question_type == \"likert\":\n",
    "        return parse_likert(row.get(\"response_clean\"))\n",
    "    return np.nan\n",
    "\n",
    "def finalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"response_clean\"] = df[\"response_raw\"].apply(clean_response)\n",
    "    df[\"response_numeric\"] = df.apply(compute_response_numeric, axis=1)\n",
    "    df[\"submitted_timestamp\"] = df[\"submitted_timestamp\"].fillna(\"\")\n",
    "    ordered_columns = [\n",
    "        \"respondent\", \"group\", \"questionnaire\", \"submitted_timestamp\", \"source_path\",\n",
    "        \"question_code\", \"question_text\", \"type\", \"subscale\", \"category\", \"accuracy\",\n",
    "        \"stimulus_form\", \"stimulus_title\", \"response_raw\", \"response_clean\", \"response_numeric\",\n",
    "        \"score_value\", \"score_confidence\", \"score_method\", \"score_explanation\"\n",
    "    ]\n",
    "    for column in ordered_columns:\n",
    "        if column not in df.columns:\n",
    "            df[column] = np.nan\n",
    "    return df.loc[:, ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c7449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 73 duplicate rows (kept latest submission per respondent/question).\n",
      "Post deliverable rows: 6,059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>submitted_timestamp</th>\n",
       "      <th>source_path</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subscale</th>\n",
       "      <th>category</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>stimulus_form</th>\n",
       "      <th>stimulus_title</th>\n",
       "      <th>response_raw</th>\n",
       "      <th>response_clean</th>\n",
       "      <th>response_numeric</th>\n",
       "      <th>score_value</th>\n",
       "      <th>score_confidence</th>\n",
       "      <th>score_method</th>\n",
       "      <th>score_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-15 13:56:04</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Short</td>\n",
       "      <td>The Town</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage3_recognition_binary</td>\n",
       "      <td>Stage 3 recognition correctness (1 = aligns wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-15 13:56:04</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>likert</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Short</td>\n",
       "      <td>The Town</td>\n",
       "      <td>4 = Extremely confident</td>\n",
       "      <td>4 = Extremely confident</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>stage3_recognition_confidence</td>\n",
       "      <td>Stage 3 recognition confidence parsed to a 0-4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-15 13:56:04</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3. This scene was part of the videos you wat...</td>\n",
       "      <td>multiple</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>hit</td>\n",
       "      <td>Short</td>\n",
       "      <td>The Town</td>\n",
       "      <td>2 = The scene didn't seem memorable</td>\n",
       "      <td>2 = The scene didn't seem memorable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-15 13:56:04</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "      <td>10</td>\n",
       "      <td>10. Did you understand the scene while watchin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 = Understood it well</td>\n",
       "      <td>3 = Understood it well</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-15 13:56:04</td>\n",
       "      <td>data/Post/Group C_Post Viewing Questionnaire P...</td>\n",
       "      <td>10.1</td>\n",
       "      <td>10.1. Did you see this scene in the videos you...</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>distractor</td>\n",
       "      <td>hit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ironman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage3_recognition_binary</td>\n",
       "      <td>Stage 3 recognition correctness (1 = aligns wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     respondent group questionnaire  submitted_timestamp  \\\n",
       "2557          1     C          Post  2025-10-15 13:56:04   \n",
       "2558          1     C          Post  2025-10-15 13:56:04   \n",
       "2559          1     C          Post  2025-10-15 13:56:04   \n",
       "2620          1     C          Post  2025-10-15 13:56:04   \n",
       "2584          1     C          Post  2025-10-15 13:56:04   \n",
       "\n",
       "                                            source_path question_code  \\\n",
       "2557  data/Post/Group C_Post Viewing Questionnaire P...           1.1   \n",
       "2558  data/Post/Group C_Post Viewing Questionnaire P...           1.2   \n",
       "2559  data/Post/Group C_Post Viewing Questionnaire P...           1.3   \n",
       "2620  data/Post/Group C_Post Viewing Questionnaire P...            10   \n",
       "2584  data/Post/Group C_Post Viewing Questionnaire P...          10.1   \n",
       "\n",
       "                                          question_text      type  \\\n",
       "2557  1.1. Did you see this scene in the videos you ...    binary   \n",
       "2558  1.2. How confident are you in your answer? Ple...    likert   \n",
       "2559  1.3. This scene was part of the videos you wat...  multiple   \n",
       "2620  10. Did you understand the scene while watchin...       NaN   \n",
       "2584  10.1. Did you see this scene in the videos you...    binary   \n",
       "\n",
       "         subscale    category accuracy stimulus_form stimulus_title  \\\n",
       "2557  recognition         key      hit         Short       The Town   \n",
       "2558  recognition         key      hit         Short       The Town   \n",
       "2559  recognition         key      hit         Short       The Town   \n",
       "2620          NaN         NaN      NaN           NaN            NaN   \n",
       "2584  recognition  distractor      hit           NaN        Ironman   \n",
       "\n",
       "                             response_raw  \\\n",
       "2557                                   No   \n",
       "2558              4 = Extremely confident   \n",
       "2559  2 = The scene didn't seem memorable   \n",
       "2620               3 = Understood it well   \n",
       "2584                                  NaN   \n",
       "\n",
       "                           response_clean  response_numeric  score_value  \\\n",
       "2557                                   No               0.0          0.0   \n",
       "2558              4 = Extremely confident               4.0          4.0   \n",
       "2559  2 = The scene didn't seem memorable               NaN          NaN   \n",
       "2620               3 = Understood it well               NaN          NaN   \n",
       "2584                                  NaN               NaN          NaN   \n",
       "\n",
       "      score_confidence                   score_method  \\\n",
       "2557               NaN      stage3_recognition_binary   \n",
       "2558               4.0  stage3_recognition_confidence   \n",
       "2559               NaN                            NaN   \n",
       "2620               NaN                            NaN   \n",
       "2584               NaN      stage3_recognition_binary   \n",
       "\n",
       "                                      score_explanation  \n",
       "2557  Stage 3 recognition correctness (1 = aligns wi...  \n",
       "2558  Stage 3 recognition confidence parsed to a 0-4...  \n",
       "2559                                                NaN  \n",
       "2620                                                NaN  \n",
       "2584  Stage 3 recognition correctness (1 = aligns wi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble client deliverable dataset\n",
    "stage1_path = RESULTS_DIR / \"uv_stage1.csv\"\n",
    "stage3_path = RESULTS_DIR / \"uv_stage3.csv\"\n",
    "metadata_path = DATA_DIR / \"post_survey_map.csv\"\n",
    "stimulus_map_path = DATA_DIR / \"stimulus_rename.csv\"\n",
    "recall_full_path = RESULTS_DIR / \"recall_coded_responses_full.csv\"\n",
    "recall_key_path = RESULTS_DIR / \"recall_coded_responses_key_moment.csv\"\n",
    "deliverable_path = RESULTS_DIR / \"post_study_survey_responses.csv\"\n",
    "\n",
    "stage1_df = pd.read_csv(stage1_path)\n",
    "stage3_df = pd.read_csv(stage3_path)\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "stimulus_map_df = pd.read_csv(stimulus_map_path)\n",
    "\n",
    "post_raw_df = load_post_survey_raw(POST_DIR)\n",
    "post_raw_df[\"respondent\"] = post_raw_df[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "stage1_subset = stage1_df[[\"respondent\", \"group\", \"Short Form\", \"Long Form\"]].copy()\n",
    "stage1_subset[\"respondent\"] = stage1_subset[\"respondent\"].astype(str).str.strip()\n",
    "stage1_subset[\"group\"] = stage1_subset[\"group\"].astype(str).str.strip().str.upper()\n",
    "stage1_subset[\"short_form_title\"] = stage1_subset[\"Short Form\"].map(canonicalize_title)\n",
    "stage1_subset[\"long_form_title\"] = stage1_subset[\"Long Form\"].map(canonicalize_title)\n",
    "stage1_subset.drop(columns=[\"Short Form\", \"Long Form\"], inplace=True)\n",
    "\n",
    "post_wide = post_raw_df.merge(stage1_subset, on=\"respondent\", how=\"left\", suffixes=(\"\", \"_stage1\"))\n",
    "post_wide[\"group\"] = post_wide[\"group\"].fillna(post_wide.get(\"file_group\"))\n",
    "post_wide[\"group\"] = post_wide[\"group\"].astype(str).str.strip().str.upper()\n",
    "post_wide.loc[post_wide[\"group\"].isin({\"\", \"NAN\"}), \"group\"] = np.nan\n",
    "post_wide[\"short_form_title\"] = post_wide[\"short_form_title\"].fillna(\"\").map(canonicalize_title)\n",
    "post_wide[\"long_form_title\"] = post_wide[\"long_form_title\"].fillna(\"\").map(canonicalize_title)\n",
    "post_wide.loc[post_wide[\"short_form_title\"] == \"\", \"short_form_title\"] = np.nan\n",
    "post_wide.loc[post_wide[\"long_form_title\"] == \"\", \"long_form_title\"] = np.nan\n",
    "\n",
    "respondent_forms = build_forms_lookup(stage1_df)\n",
    "group_title_form_lookup = build_group_title_form_lookup(stimulus_map_df)\n",
    "\n",
    "base_columns = {\n",
    "    \"respondent\",\n",
    "    \"group\",\n",
    "    \"questionnaire\",\n",
    "    \"submitted_timestamp\",\n",
    "    \"source_path\",\n",
    "    \"file_group\",\n",
    "    \"short_form_title\",\n",
    "    \"long_form_title\",\n",
    "}\n",
    "value_columns = [col for col in post_wide.columns if col not in base_columns and not col.endswith(\"_stage1\") and col not in {\"group_stage1\"}]\n",
    "value_columns = list(dict.fromkeys(value_columns))\n",
    "\n",
    "post_long_df = reshape_post_survey_long(post_wide, value_columns)\n",
    "post_enriched_df, metadata_issues_df = attach_post_metadata(\n",
    "    post_long_df, metadata_df, respondent_forms, group_title_form_lookup\n",
    ")\n",
    "\n",
    "recognition_scores_df = build_recognition_score_frame(stage3_df)\n",
    "recall_scores_df = build_recall_score_frame(RESULTS_DIR)\n",
    "\n",
    "post_scored_df = append_post_scores(post_enriched_df, recognition_scores_df, recall_scores_df)\n",
    "post_final_df = finalize_columns(post_scored_df)\n",
    "\n",
    "post_final_df[\"submitted_dt\"] = pd.to_datetime(post_final_df[\"submitted_timestamp\"], errors=\"coerce\")\n",
    "post_final_df[\"question_code_fill\"] = post_final_df[\"question_code\"].fillna(\"__NA__\")\n",
    "post_final_df[\"question_text_fill\"] = post_final_df[\"question_text\"].fillna(\"__NA__\")\n",
    "post_final_df[\"source_path_fill\"] = post_final_df[\"source_path\"].fillna(\"__NA__\")\n",
    "post_final_df.sort_values(\n",
    "    [\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path_fill\", \"submitted_dt\"],\n",
    "    inplace=True,\n",
    ")\n",
    "dedupe_subset = [\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path_fill\"]\n",
    "before_dedupe_rows = len(post_final_df)\n",
    "post_final_df = post_final_df.drop_duplicates(subset=dedupe_subset, keep=\"last\")\n",
    "timestamp_dedupes = before_dedupe_rows - len(post_final_df)\n",
    "\n",
    "post_final_df.drop(columns=[\"submitted_dt\", \"question_code_fill\", \"question_text_fill\", \"source_path_fill\"], inplace=True)\n",
    "if timestamp_dedupes:\n",
    "    print(\n",
    "        f\"Removed {timestamp_dedupes} duplicate rows (kept latest submission per respondent/question).\",\n",
    "    )\n",
    "\n",
    "print(f\"Post deliverable rows: {len(post_final_df):,}\")\n",
    "post_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4466b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to c:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\results\\post_study_survey_responses.csv (3,910 rows).\n"
     ]
    }
   ],
   "source": [
    "deliverable_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add an extra step here to drop NA values in response column\n",
    "post_final_df = post_final_df[post_final_df[\"response_raw\"].notna()]\n",
    "\n",
    "post_final_df.to_csv(deliverable_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV to {deliverable_path} ({len(post_final_df):,} rows).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc907a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows                          3910\n",
       "unique_respondents              83\n",
       "unique_question_codes           59\n",
       "rows_missing_question_code     166\n",
       "rows_missing_type              162\n",
       "rows_with_scores              1584\n",
       "rows_missing_group               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_summary = {\n",
    "    \"rows\": len(post_final_df),\n",
    "    \"unique_respondents\": post_final_df[\"respondent\"].nunique(),\n",
    "    \"unique_question_codes\": post_final_df[\"question_code\"].nunique(),\n",
    "    \"rows_missing_question_code\": int(post_final_df[\"question_code\"].isna().sum()),\n",
    "    \"rows_missing_type\": int(post_final_df[\"type\"].isna().sum()),\n",
    "    \"rows_with_scores\": int(post_final_df[\"score_value\"].notna().sum()),\n",
    "    \"rows_missing_group\": int(post_final_df[\"group\"].isna().sum()),\n",
    "}\n",
    "pd.Series(validation_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb1a8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_count = post_final_df.assign(\n",
    "    question_code_fill=post_final_df[\"question_code\"].fillna(\"__NA__\"),\n",
    "    question_text_fill=post_final_df[\"question_text\"].fillna(\"__NA__\"),\n",
    ").duplicated(subset=[\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"]).sum()\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47ce8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question_text</th>\n",
       "      <th>source_path</th>\n",
       "      <th>response_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [respondent, question_code, question_text, source_path, response_raw]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_df = post_final_df.assign(\n",
    "    question_code_fill=post_final_df[\"question_code\"].fillna(\"__NA__\"),\n",
    "    question_text_fill=post_final_df[\"question_text\"].fillna(\"__NA__\"),\n",
    ")\n",
    "duplicates_df[duplicates_df.duplicated(subset=[\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"], keep=False)][\n",
    "    [\"respondent\", \"question_code\", \"question_text\", \"source_path\", \"response_raw\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d1623c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_keys_mask = duplicates_df.duplicated(subset=[\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"], keep=False)\n",
    "duplicate_group_sizes = (\n",
    "    duplicates_df.loc[duplicate_keys_mask]\n",
    "    .groupby([\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"])\n",
    ")\n",
    "duplicate_group_sizes.size().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac73a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['respondent',\n",
       " 'group',\n",
       " 'questionnaire',\n",
       " 'submitted_timestamp',\n",
       " 'source_path',\n",
       " 'question_code',\n",
       " 'question_text',\n",
       " 'type',\n",
       " 'subscale',\n",
       " 'category',\n",
       " 'accuracy',\n",
       " 'stimulus_form',\n",
       " 'stimulus_title',\n",
       " 'response_raw',\n",
       " 'response_clean',\n",
       " 'response_numeric',\n",
       " 'score_value',\n",
       " 'score_confidence',\n",
       " 'score_method',\n",
       " 'score_explanation',\n",
       " 'question_code_fill',\n",
       " 'question_text_fill']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(duplicates_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f125312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>submitted_timestamp</th>\n",
       "      <th>source_path</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subscale</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>stimulus_title</th>\n",
       "      <th>response_raw</th>\n",
       "      <th>response_clean</th>\n",
       "      <th>response_numeric</th>\n",
       "      <th>score_value</th>\n",
       "      <th>score_confidence</th>\n",
       "      <th>score_method</th>\n",
       "      <th>score_explanation</th>\n",
       "      <th>question_code_fill</th>\n",
       "      <th>question_text_fill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>100</td>\n",
       "      <td>E</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-23 22:44:28</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>...</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage3_recognition_binary</td>\n",
       "      <td>Stage 3 recognition correctness (1 = aligns wi...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>100</td>\n",
       "      <td>E</td>\n",
       "      <td>Post</td>\n",
       "      <td>2025-10-25 16:21:27</td>\n",
       "      <td>data/Post/Group E_ Post Viewing Questionnaire ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>binary</td>\n",
       "      <td>recognition</td>\n",
       "      <td>key</td>\n",
       "      <td>...</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stage3_recognition_binary</td>\n",
       "      <td>Stage 3 recognition correctness (1 = aligns wi...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1. Did you see this scene in the videos you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     respondent group questionnaire  submitted_timestamp  \\\n",
       "4528        100     E          Post  2025-10-23 22:44:28   \n",
       "5039        100     E          Post  2025-10-25 16:21:27   \n",
       "\n",
       "                                            source_path question_code  \\\n",
       "4528  data/Post/Group E_ Post Viewing Questionnaire ...           1.1   \n",
       "5039  data/Post/Group E_ Post Viewing Questionnaire ...           1.1   \n",
       "\n",
       "                                          question_text    type     subscale  \\\n",
       "4528  1.1. Did you see this scene in the videos you ...  binary  recognition   \n",
       "5039  1.1. Did you see this scene in the videos you ...  binary  recognition   \n",
       "\n",
       "     category  ... stimulus_title response_raw response_clean  \\\n",
       "4528      key  ...        Mad Max          Yes            Yes   \n",
       "5039      key  ...        Mad Max          Yes            Yes   \n",
       "\n",
       "     response_numeric score_value  score_confidence  \\\n",
       "4528              1.0         1.0               NaN   \n",
       "5039              1.0         1.0               NaN   \n",
       "\n",
       "                   score_method  \\\n",
       "4528  stage3_recognition_binary   \n",
       "5039  stage3_recognition_binary   \n",
       "\n",
       "                                      score_explanation question_code_fill  \\\n",
       "4528  Stage 3 recognition correctness (1 = aligns wi...                1.1   \n",
       "5039  Stage 3 recognition correctness (1 = aligns wi...                1.1   \n",
       "\n",
       "                                     question_text_fill  \n",
       "4528  1.1. Did you see this scene in the videos you ...  \n",
       "5039  1.1. Did you see this scene in the videos you ...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_keys = duplicates_df.duplicated(subset=[\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"], keep=False)\n",
    "duplicate_groups = duplicates_df.loc[duplicate_keys].groupby([\n",
    "    \"respondent\",\n",
    "    \"question_code_fill\",\n",
    "    \"question_text_fill\",\n",
    "    \"source_path\",\n",
    "])\n",
    "first_key = next(iter(duplicate_groups.groups))\n",
    "example_rows = duplicates_df.loc[duplicate_groups.groups[first_key]]\n",
    "example_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deac909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows captured: 0\n",
      "No duplicate groups with differing values found.\n",
      "Groups inspected: 0\n"
     ]
    }
   ],
   "source": [
    "display_columns = [\n",
    "    \"respondent\",\n",
    "    \"group\",\n",
    "    \"questionnaire\",\n",
    "    \"submitted_timestamp\",\n",
    "    \"question_code\",\n",
    "    \"question_text\",\n",
    "    \"source_path\",\n",
    "    \"response_raw\",\n",
    "    \"response_clean\",\n",
    "    \"response_numeric\",\n",
    "    \"type\",\n",
    "    \"category\",\n",
    "    \"subscale\",\n",
    "    \"accuracy\",\n",
    "    \"stimulus_form\",\n",
    "    \"stimulus_title\",\n",
    "    \"score_value\",\n",
    "    \"score_confidence\",\n",
    "    \"score_method\",\n",
    "    \"score_explanation\",\n",
    "]\n",
    "comparison_columns = [\n",
    "    column\n",
    "    for column in duplicates_df.columns\n",
    "    if column not in {\n",
    "        \"question_code_fill\",\n",
    "        \"question_text_fill\",\n",
    "    }\n",
    "]\n",
    "duplicate_keys_mask = duplicates_df.duplicated(subset=[\"respondent\", \"question_code_fill\", \"question_text_fill\", \"source_path\"], keep=False)\n",
    "duplicate_rows = duplicates_df.loc[duplicate_keys_mask]\n",
    "print(f\"Duplicate rows captured: {len(duplicate_rows):,}\")\n",
    "grouped_duplicates = duplicate_rows.groupby([\n",
    "    \"respondent\",\n",
    "    \"question_code_fill\",\n",
    "    \"question_text_fill\",\n",
    "    \"source_path\",\n",
    "])\n",
    "example_tables = []\n",
    "diagnostic_examples = []\n",
    "for key, group in grouped_duplicates:\n",
    "    subset = group.loc[:, comparison_columns]\n",
    "    differing_columns = [\n",
    "        column\n",
    "        for column in comparison_columns\n",
    "        if subset[column].nunique(dropna=False) > 1\n",
    "]\n",
    "    if not differing_columns:\n",
    "        diagnostic_examples.append(group)\n",
    "        continue\n",
    "    example_frame = group.loc[:, display_columns].copy()\n",
    "    duplicate_label = \" | \".join(str(value) for value in key)\n",
    "    example_frame.insert(0, \"different_columns\", \", \".join(sorted(differing_columns)))\n",
    "    example_frame.insert(0, \"duplicate_key\", duplicate_label)\n",
    "    example_frame.insert(0, \"example\", f\"Example {len(example_tables) + 1}\")\n",
    "    example_tables.append(example_frame)\n",
    "    if len(example_tables) == 3:\n",
    "        break\n",
    "if example_tables:\n",
    "    examples_df = pd.concat(example_tables, ignore_index=True)\n",
    "    examples_df\n",
    "else:\n",
    "    print(\"No duplicate groups with differing values found.\")\n",
    "    print(f\"Groups inspected: {len(diagnostic_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef4750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue                                   question_code\n",
       "unable_to_resolve_form_for_recognition  11.2             84\n",
       "                                        11.1             84\n",
       "                                        6.3              84\n",
       "                                        8.1              84\n",
       "                                        11.3             84\n",
       "                                        2.1              84\n",
       "                                        6.2              84\n",
       "                                        6.1              84\n",
       "                                        2.3              84\n",
       "                                        2.2              84\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_summary = metadata_issues_df.groupby(['issue', 'question_code']).size().sort_values(ascending=False)\n",
    "issue_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7baa871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>group</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.1</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.2</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.3</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.1</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.2</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.3</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.1</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.2</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.3</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "group           A   B   C   D   E   F\n",
       "question_code                        \n",
       "10.1            0  10  15   0   0   0\n",
       "10.2            0  10  15   0   0   0\n",
       "10.3            0  10  15   0   0   0\n",
       "11.1           12  10  15  18  15  14\n",
       "11.2           12  10  15  18  15  14\n",
       "11.3           12  10  15  18  15  14\n",
       "12.1            0  10  15   0   0   0\n",
       "12.2            0  10  15   0   0   0\n",
       "12.3            0  10  15   0   0   0\n",
       "2.1            12  10  15  18  15  14\n",
       "2.2            12  10  15  18  15  14\n",
       "2.3            12  10  15  18  15  14\n",
       "6.1            12  10  15  18  15  14\n",
       "6.2            12  10  15  18  15  14\n",
       "6.3            12  10  15  18  15  14\n",
       "8.1            12  10  15  18  15  14\n",
       "8.2            12  10  15  18  15  14\n",
       "8.3            12  10  15  18  15  14"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_issues_df.groupby(['question_code', 'group']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549dda44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>question_code</th>\n",
       "      <th>question_text</th>\n",
       "      <th>issue</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>unable_to_resolve_form_for_recognition</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>unable_to_resolve_form_for_recognition</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3. This scene was not in the videos you watc...</td>\n",
       "      <td>unable_to_resolve_form_for_recognition</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1. Did you see this scene in the videos you ...</td>\n",
       "      <td>unable_to_resolve_form_for_recognition</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.2. How confident are you in your answer? Ple...</td>\n",
       "      <td>unable_to_resolve_form_for_recognition</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent question_code                                      question_text  \\\n",
       "0          3           2.1  2.1. Did you see this scene in the videos you ...   \n",
       "1          3           2.2  2.2. How confident are you in your answer? Ple...   \n",
       "2          3           2.3  2.3. This scene was not in the videos you watc...   \n",
       "3          3           6.1  6.1. Did you see this scene in the videos you ...   \n",
       "4          3           6.2  6.2. How confident are you in your answer? Ple...   \n",
       "\n",
       "                                    issue group  \n",
       "0  unable_to_resolve_form_for_recognition     A  \n",
       "1  unable_to_resolve_form_for_recognition     A  \n",
       "2  unable_to_resolve_form_for_recognition     A  \n",
       "3  unable_to_resolve_form_for_recognition     A  \n",
       "4  unable_to_resolve_form_for_recognition     A  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ace6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "C    270\n",
       "D    216\n",
       "B    180\n",
       "E    180\n",
       "F    168\n",
       "A    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_issues_df['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3af2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>short_form_title</th>\n",
       "      <th>long_form_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent group short_form_title long_form_title\n",
       "4          3     A          Mad Max        The Town"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_subset.loc[stage1_subset['respondent'] == '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e962628",
   "metadata": {},
   "outputs": [],
   "source": [
    "forms_info = respondent_forms.get('3')\n",
    "forms_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
