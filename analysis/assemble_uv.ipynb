{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a9680a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f48a0",
   "metadata": {},
   "source": [
    "## Function to read iMotions sensor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5da1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_imotions(path, metadata=None):\n",
    "    \"\"\"\n",
    "    Reads an iMotions CSV file while extracting optional metadata fields.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the iMotions CSV file.\n",
    "        metadata (list[str], optional): List of metadata keys to extract.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The data as a DataFrame.\n",
    "        meta_dict (dict): Dictionary containing requested metadata fields.\n",
    "    \"\"\"\n",
    "    meta_dict = {}\n",
    "    metadata = metadata or []\n",
    "    meta_lines = []\n",
    "    count = 0\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if '#' in line.split(',')[0]:\n",
    "                meta_lines.append('#'.join(line.strip().split('#')[1:]))\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Parse requested metadata\n",
    "    for line in meta_lines:\n",
    "        # Remove leading '#' and split by comma\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 1:\n",
    "            key, value = parts[0].strip(), ','.join(parts[1:])\n",
    "            if key in metadata:\n",
    "                meta_dict[key] = value\n",
    "\n",
    "    # Read data using header row after metadata\n",
    "    df = pd.read_csv(path, header=count, low_memory=False)\n",
    "    return df, meta_dict\n",
    "\n",
    "def get_files(folder, tags=['',]):\n",
    "    return [f for f in os.listdir(folder) if not f.startswith('.') and all(x in f for x in tags)] \n",
    "\n",
    "\n",
    "def get_biometric_data(in_folder, results_folder):\n",
    "\n",
    "    ######## Define ########\n",
    "    # Define paths\n",
    "    out_path = f\"{results_folder}/\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    respondents = [1,2,3] #define list of respondent ids\n",
    "\n",
    "    # Define signal columns\n",
    "    cols_afdex = [\n",
    "                \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "                \"Surprise\", \"Engagement\", \"Valence\", \"Sentimentality\",\n",
    "                \"Confusion\", \"Neutral\"\n",
    "        ]\n",
    "    cols_eeg = ['High Engagement',\n",
    "        'Low Engagement',\n",
    "        'Distraction',\n",
    "        'Drowsy',\n",
    "        'Workload Average',\n",
    "        'Frontal Asymmetry Alpha',\n",
    "        ]\n",
    "\n",
    "\n",
    "    #Define window lengths in seconds\n",
    "    window_lengths = [3,]\n",
    "\n",
    "    ######## Read Inputs #######\n",
    "    #Get input files\n",
    "    sensor_files = get_files(f'{in_folder}/Sensors/',tags=['.csv',])\n",
    "\n",
    "    ### Begin ###\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    for respondent in respondents:\n",
    "        error = {'respondent':respondent, 'FAC':None, 'EEG':None, 'GSR':None, 'Blinks':None, 'ET':None}\n",
    "        interaction = {'respondent':respondent}\n",
    "        try:\n",
    "            file = [f for f in sensor_files if respondent in f][0] #may need adjustment\n",
    "            df_sens_resp,_ = read_imotions(f'{in_folder}/Sensors/{file}')\n",
    "\n",
    "            # Get sensor data per stimulus\n",
    "            for task in df_sens_resp['SourceStimuliName'].unique():\n",
    "                df_sens_task = df_sens_resp.loc[(df_sens_resp['SourceStimuliName']==task)]\n",
    "                window = task\n",
    "\n",
    "                # Get facial coding data\n",
    "                for a in cols_afdex:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_FAC_{a}_mean']=df_sens_task[a].dropna().mean()\n",
    "                        auc_data = df_sens_task[['Timestamp',a]].dropna()\n",
    "                        interaction[f'sens_{window}_FAC_{a}_AUC']=np.trapz(auc_data[a],x=auc_data['Timestamp'])/1000\n",
    "                        interaction[f'sens_{window}_FAC_{a}_Binary']=df_sens_task[a].dropna().max()>= 50\n",
    "                    except:\n",
    "                        error['FAC']='Missing'\n",
    "\n",
    "                for e in cols_eeg:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_EEG_{e}_mean']=df_sens_task[e].dropna()[df_sens_int[e] > -9000].mean()\n",
    "                        auc_data = df_sens_task.loc[df_sens_task[e].notna() & (df_sens_task[e] > -9000), ['Timestamp', e]]\n",
    "                        interaction[f'sens_{window}_EEG_{e}_AUC']=np.trapz(auc_data[e],x=auc_data['Timestamp'])/1000\n",
    "                    except:\n",
    "                        error['EEG']='Missing'\n",
    "\n",
    "                try:\n",
    "                    interaction[f'sens_{window}_GSR_PeakDetected_Binary'] =1 if df_sens_task['Peak Detected'].sum()>0 else 0\n",
    "                    gsr_data = df_sens_task[['Timestamp','Peak Detected']].dropna()\n",
    "                    mask = gsr_data['Peak Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = gsr_data.loc[mask, 'Peak Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_GSR_Peaks_Count'] =count_patches\n",
    "                except:\n",
    "                    error['GSR']='Missing'\n",
    "\n",
    "                try:\n",
    "                    blink_data = df_sens_task[['Timestamp','Blink Detected']].dropna()\n",
    "                    mask = blink_data['Blink Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = blink_data.loc[mask, 'Blink Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_ET_Blink_Count'] =count_patches\n",
    "                    interaction[f'sens_{window}_ET_Blink_Rate'] =count_patches/((df_sens_task['Timestamp'].values[-1]-df_sens_task['Timestamp'].values[0])/(1000 * 60))\n",
    "                except:\n",
    "                    error['ET']='Missing'\n",
    "\n",
    "            # TODO Get sensor data for non-interaction\n",
    "            ##################################### Add this in\n",
    "            results.append(interaction)\n",
    "            errors.append(error)\n",
    "\n",
    "            pass\n",
    "        except IndexError:\n",
    "            print(f'>>> Could not find {respondent} sensor data')\n",
    "        except:\n",
    "            print(f'>>> Failed {respondent}')\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(f'{out_path}biometric_results.csv')\n",
    "\n",
    "    errors = pd.DataFrame(errors)\n",
    "    errors.to_csv(f'{out_path}errors_biometric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef61f8",
   "metadata": {},
   "source": [
    "## Explanation of functions\n",
    "\n",
    "The above functions are used to read in the sesor data files, one csv at a time, and extract single features per stimulus, and write these features to a simple results file.\n",
    "\n",
    "The functions must be adjusted to:\n",
    "- Discern between long form and short form\n",
    "- Isolate key moments from timings file provided by client\n",
    "- Extract time series\n",
    "- Compute group-wide features such as inter-subject correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84624884",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- Create naming dictionary for all stims\n",
    "- Get total times of all stims\n",
    "- Prepare key_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ef81e",
   "metadata": {},
   "source": [
    "## Stimulus duration scan\n",
    "We load one sensor recording per group, extract the unique stimulus names, and estimate the average exposure duration per stimulus using the `Timestamp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d78dad9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sensor_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>001_116.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>001_58.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>001_114.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>001_102.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>001_108.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>001_107.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  sensor_file\n",
       "0  Group A  001_116.csv\n",
       "1  Group B   001_58.csv\n",
       "2  Group C  001_114.csv\n",
       "3  Group D  001_102.csv\n",
       "4  Group E  001_108.csv\n",
       "5  Group F  001_107.csv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate one sensor export per group for duration scanning\n",
    "project_root = Path.cwd().parent\n",
    "data_export_dir = project_root / \"data\" / \"Export\"\n",
    "\n",
    "group_sensor_files = {}\n",
    "for group_dir in sorted(data_export_dir.glob(\"Group *\")):\n",
    "    if not group_dir.is_dir():\n",
    "        continue\n",
    "    sensor_dirs = sorted(group_dir.glob(\"Analyses/*/Sensor Data\"))\n",
    "    csv_candidates = []\n",
    "    for sensor_dir in sensor_dirs:\n",
    "        csv_candidates.extend(sorted(sensor_dir.glob(\"*.csv\")))\n",
    "    group_sensor_files[group_dir.name] = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "sensor_selection = pd.DataFrame([\n",
    "    {\n",
    "        \"group\": group,\n",
    "        \"sensor_file\": path.name if path else None\n",
    "    }\n",
    "    for group, path in group_sensor_files.items()\n",
    "]).sort_values(\"group\").reset_index(drop=True)\n",
    "\n",
    "sensor_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9521e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-group stimulus durations without aggregating across groups\n",
    "duration_tables = []\n",
    "issues = {}\n",
    "\n",
    "for group, path in group_sensor_files.items():\n",
    "    if path is None:\n",
    "        issues[group] = \"No sensor CSV found\"\n",
    "        continue\n",
    "    try:\n",
    "        df_group, _ = read_imotions(path)\n",
    "    except Exception as exc:\n",
    "        issues[group] = f\"read_imotions failed: {exc}\"\n",
    "        continue\n",
    "\n",
    "    required_cols = {\"SourceStimuliName\", \"Timestamp\"}\n",
    "    if not required_cols.issubset(df_group.columns):\n",
    "        issues[group] = \"Missing SourceStimuliName or Timestamp\"\n",
    "        continue\n",
    "\n",
    "    df_clean = df_group[[\"SourceStimuliName\", \"Timestamp\"]].copy()\n",
    "    df_clean = df_clean.dropna(subset=[\"SourceStimuliName\"])\n",
    "    df_clean[\"Timestamp\"] = pd.to_numeric(df_clean[\"Timestamp\"], errors=\"coerce\")\n",
    "    df_clean = df_clean.dropna(subset=[\"Timestamp\"])\n",
    "    if df_clean.empty:\n",
    "        issues[group] = \"No valid timestamp data\"\n",
    "        continue\n",
    "\n",
    "    group_duration = (\n",
    "        df_clean.groupby(\"SourceStimuliName\")[\"Timestamp\"]\n",
    "        .apply(lambda s: s.max() - s.min())\n",
    "        .reset_index(name=\"duration_ms\")\n",
    "    )\n",
    "\n",
    "    if group_duration.empty:\n",
    "        issues[group] = \"No stimuli with duration\"\n",
    "        continue\n",
    "\n",
    "    group_duration[\"duration_seconds\"] = group_duration[\"duration_ms\"] / 1000.0\n",
    "    group_duration[\"duration_minutes\"] = group_duration[\"duration_seconds\"] / 60.0\n",
    "    group_duration.insert(0, \"group\", group)\n",
    "    group_duration.rename(columns={\"SourceStimuliName\": \"stimulus_name\"}, inplace=True)\n",
    "    duration_tables.append(group_duration[[\"group\", \"stimulus_name\", \"duration_seconds\", \"duration_minutes\"]])\n",
    "\n",
    "if duration_tables:\n",
    "    stimulus_summary = pd.concat(duration_tables, ignore_index=True)\n",
    "    stimulus_summary.sort_values([\"group\", \"stimulus_name\"], inplace=True)\n",
    "    stimulus_summary[\"duration_seconds\"] = stimulus_summary[\"duration_seconds\"].round(2)\n",
    "    stimulus_summary[\"duration_minutes\"] = stimulus_summary[\"duration_minutes\"].round(2)\n",
    "    stimulus_summary.reset_index(drop=True, inplace=True)\n",
    "    stimulus_summary\n",
    "else:\n",
    "    print(\"No duration records computed.\")\n",
    "\n",
    "if issues:\n",
    "    pd.DataFrame(\n",
    "        {\"group\": list(issues.keys()), \"issue\": list(issues.values())}\n",
    "    ).sort_values(\"group\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9548d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>A STAR IS BORN</td>\n",
       "      <td>248.67</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group A</td>\n",
       "      <td>HOME ALONE</td>\n",
       "      <td>115.12</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group A</td>\n",
       "      <td>MAD MAX FURY ROAD</td>\n",
       "      <td>226.47</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group A</td>\n",
       "      <td>THE CONJURING</td>\n",
       "      <td>171.31</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group A</td>\n",
       "      <td>THE TOWN</td>\n",
       "      <td>1744.51</td>\n",
       "      <td>29.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group      stimulus_name  duration_seconds  duration_minutes\n",
       "0  Group A     A STAR IS BORN            248.67              4.14\n",
       "1  Group A         HOME ALONE            115.12              1.92\n",
       "2  Group A  MAD MAX FURY ROAD            226.47              3.77\n",
       "3  Group A      THE CONJURING            171.31              2.86\n",
       "4  Group A           THE TOWN           1744.51             29.08"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d38df137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ec5417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cd2d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      59.82\n",
       "max    1811.54\n",
       "Name: duration_seconds, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary['duration_seconds'].agg(['min','max']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6057f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>unique_stimuli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  unique_stimuli\n",
       "0  Group A               6\n",
       "1  Group B               6\n",
       "2  Group C               6\n",
       "3  Group D               6\n",
       "4  Group E               6\n",
       "5  Group F               6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_per_group = stimulus_summary.groupby('group')['stimulus_name'].nunique().reset_index(name='unique_stimuli')\n",
    "stimuli_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_summary.to_csv(project_root / \"results\" / \"stimulus_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322effdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf37ed",
   "metadata": {},
   "source": [
    "## Stimulus Annotation Overview\n",
    "- `stimulus_rename` links each group-specific stimulus from `stimulus_summary` to a clean `title` and its presentation `Form` (`Long` or `Short`).\n",
    "- Some titles appear in both forms; the long cut (≈30 min) includes the short-form key moment as an embedded segment.\n",
    "- `key_moments` pinpoints, for every long-form title, when the key moment begins (`Lead-up Duration`) and how long it lasts (`Key moment Duration_LF`).\n",
    "- These tables let us align short-form clips with the corresponding segment inside the long-form presentation for downstream comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f5419",
   "metadata": {},
   "source": [
    "## Stage 1: Demographics\n",
    "We extract respondent-level identifiers and timing information from the metadata embedded in each sensor export to seed the unified view (UV). This pass scans every sensor CSV, captures study name, respondent attributes, and recording timestamps, and prepares the foundation for later feature merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8bff719",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m demographic_records = []\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m csv_path \u001b[38;5;129;01min\u001b[39;00m sensor_file_paths:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     _, meta = \u001b[43mread_imotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     study_clean = first_segment(meta.get(\u001b[33m\"\u001b[39m\u001b[33mStudy name\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     54\u001b[39m     respondent_group_clean = first_segment(meta.get(\u001b[33m\"\u001b[39m\u001b[33mRespondent Group\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mread_imotions\u001b[39m\u001b[34m(path, metadata)\u001b[39m\n\u001b[32m     41\u001b[39m             meta_dict[key] = value\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Read data using header row after metadata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df, meta_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._first_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:820\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:921\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1083\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_column_data\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1456\u001b[39m, in \u001b[36mpandas._libs.parsers._maybe_upcast\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\numpy\\_core\\multiarray.py:1156\u001b[39m, in \u001b[36mputmask\u001b[39m\u001b[34m(a, mask, values)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1109\u001b[39m \u001b[33;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[32m   1110\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1151\u001b[39m \n\u001b[32m   1152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.putmask)\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mputmask\u001b[39m(a, /, mask, values):\n\u001b[32m   1158\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[33;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m \n\u001b[32m   1198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "metadata_keys = [\n",
    "    \"Study name\",\n",
    "    \"Respondent Name\",\n",
    "    \"Respondent Age\",\n",
    "    \"Respondent Gender\",\n",
    "    \"Respondent Group\",\n",
    "    \"Recording time\"\n",
    "]\n",
    "\n",
    "sensor_file_paths = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    ")\n",
    "\n",
    "def first_segment(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    return str(value).split(',')[0].strip()\n",
    "\n",
    "def parse_gender(raw_gender):\n",
    "    if not raw_gender:\n",
    "        return None\n",
    "    gender_lower = raw_gender.lower()\n",
    "    if \"female\" in gender_lower:\n",
    "        return \"Female\"\n",
    "    if \"male\" in gender_lower:\n",
    "        return \"Male\"\n",
    "    if \"other\" in gender_lower:\n",
    "        return \"Other\"\n",
    "    return raw_gender.title()\n",
    "\n",
    "def extract_group_letter(study_value, fallback_values):\n",
    "    if study_value:\n",
    "        terminal_match = re.search(r\"([A-Za-z])$\", study_value.strip())\n",
    "        if terminal_match:\n",
    "            return terminal_match.group(1).upper()\n",
    "        letters = re.findall(r\"[A-Za-z]\", study_value)\n",
    "        if letters:\n",
    "            return letters[-1].upper()\n",
    "    for candidate in fallback_values:\n",
    "        if candidate:\n",
    "            match = re.search(r\"Group\\s*([A-F])\", str(candidate), flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "demographic_records = []\n",
    "\n",
    "for csv_path in sensor_file_paths:\n",
    "    _, meta = read_imotions(csv_path, metadata=metadata_keys)\n",
    "\n",
    "    study_clean = first_segment(meta.get(\"Study name\"))\n",
    "    respondent_group_clean = first_segment(meta.get(\"Respondent Group\"))\n",
    "    group_letter = extract_group_letter(study_clean, [respondent_group_clean, csv_path.as_posix()])\n",
    "\n",
    "    respondent_raw = first_segment(meta.get(\"Respondent Name\"))\n",
    "    respondent_value = csv_path.stem\n",
    "    if respondent_raw:\n",
    "        respondent_digits = re.search(r\"\\d+\", respondent_raw)\n",
    "        if respondent_digits:\n",
    "            respondent_value = respondent_digits.group(0)\n",
    "        else:\n",
    "            respondent_value = respondent_raw\n",
    "\n",
    "    age_raw = first_segment(meta.get(\"Respondent Age\"))\n",
    "    age_numeric = pd.to_numeric(age_raw, errors=\"coerce\")\n",
    "    if pd.notna(age_numeric):\n",
    "        age_value = int(age_numeric)\n",
    "    else:\n",
    "        age_value = pd.NA\n",
    "\n",
    "    gender_raw = first_segment(meta.get(\"Respondent Gender\"))\n",
    "    gender_value = parse_gender(gender_raw)\n",
    "\n",
    "    recording_raw = meta.get(\"Recording time\")\n",
    "    date_study = None\n",
    "    time_study = None\n",
    "    if recording_raw:\n",
    "        fragments = [frag.strip() for frag in str(recording_raw).split(',') if frag.strip()]\n",
    "        date_part = None\n",
    "        time_part = None\n",
    "        for fragment in fragments:\n",
    "            if fragment.lower().startswith(\"date:\"):\n",
    "                date_part = fragment.split(':', 1)[1].strip()\n",
    "            elif fragment.lower().startswith(\"time:\"):\n",
    "                time_part = fragment.split(':', 1)[1].strip()\n",
    "        if date_part and time_part:\n",
    "            dt_string = f\"{date_part} {time_part}\"\n",
    "            ts = pd.to_datetime(dt_string, utc=True, errors=\"coerce\")\n",
    "            if pd.isna(ts):\n",
    "                ts = pd.to_datetime(dt_string, errors=\"coerce\")\n",
    "                if pd.notna(ts) and ts.tzinfo is None:\n",
    "                    try:\n",
    "                        ts = ts.tz_localize(\"America/Chicago\")\n",
    "                    except Exception:\n",
    "                        ts = ts.tz_localize(\"UTC\")\n",
    "            if pd.notna(ts):\n",
    "                if ts.tzinfo is None:\n",
    "                    ts = ts.tz_localize(\"America/Chicago\")\n",
    "                else:\n",
    "                    ts = ts.tz_convert(\"America/Chicago\")\n",
    "                date_study = ts.strftime(\"%m/%d/%Y\")\n",
    "                time_study = ts.strftime(\"%H:%M:%S\")\n",
    "            else:\n",
    "                date_study = date_part\n",
    "        elif date_part:\n",
    "            date_study = date_part\n",
    "\n",
    "    demographic_records.append({\n",
    "        \"source_file\": csv_path.name,\n",
    "        \"group\": group_letter,\n",
    "        \"respondent\": respondent_value,\n",
    "        \"age\": age_value,\n",
    "        \"gender\": gender_value,\n",
    "        \"date_study\": date_study,\n",
    "        \"time_study\": time_study\n",
    "    })\n",
    "\n",
    "uv_stage1 = pd.DataFrame(demographic_records)\n",
    "\n",
    "if not uv_stage1.empty:\n",
    "    uv_stage1 = uv_stage1.sort_values([\"group\", \"respondent\"]).reset_index(drop=True)\n",
    "    uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "    uv_stage1[\"age\"] = uv_stage1[\"age\"].astype(\"Int64\")\n",
    "\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03d7fbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>69</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>44</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>50</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent  gender\n",
       "7           8  Female\n",
       "16         56    Male\n",
       "28         16    Male\n",
       "31          6    Male\n",
       "45         46  Female\n",
       "61         69  Female\n",
       "77         44    Male\n",
       "78         50    Male"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_gender_overrides = {\n",
    "    \"8\": \"Female\",\n",
    "    \"56\": \"Male\",\n",
    "    \"16\": \"Male\",\n",
    "    \"6\": \"Male\",\n",
    "    \"46\": \"Female\",\n",
    "    \"69\": \"Female\",\n",
    "    \"44\": \"Male\",\n",
    "    \"50\": \"Male\"\n",
    "}\n",
    "\n",
    "uv_stage1[\"gender\"] = uv_stage1.apply(\n",
    "    lambda row: manual_gender_overrides.get(row[\"respondent\"], row[\"gender\"]), axis=1\n",
    ")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1[uv_stage1[\"respondent\"].isin(manual_gender_overrides.keys())][[\"respondent\", \"gender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb769cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_respondents = uv_stage1[uv_stage1.duplicated(subset=\"respondent\", keep=False)]\n",
    "if duplicate_respondents.empty:\n",
    "    print(\"No duplicate respondents detected.\")\n",
    "else:\n",
    "    duplicate_respondents.sort_values(\"respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92662888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006_11.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  age  gender  date_study time_study\n",
       "0   003_104.csv     A        104   59    Male  10/16/2025   18:09:03\n",
       "1   002_106.csv     A        106   30    Male  10/16/2025   19:35:05\n",
       "2    006_11.csv     A         11   33  Female  10/11/2025   09:32:42\n",
       "3   001_116.csv     A        116   19    Male  10/18/2025   12:37:40\n",
       "4     007_3.csv     A          3   34  Female  10/10/2025   09:19:22\n",
       "..          ...   ...        ...  ...     ...         ...        ...\n",
       "78   005_50.csv     F         50   63    Male  10/14/2025   09:54:03\n",
       "79   004_60.csv     F         60   66    Male  10/15/2025   09:34:06\n",
       "80   003_70.csv     F         70   61  Female  10/16/2025   09:49:14\n",
       "81   002_85.csv     F         85   34  Female  10/17/2025   14:37:41\n",
       "82   004_96.csv     F         96   29  Female  10/15/2025   14:32:00\n",
       "\n",
       "[83 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv_stage1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
