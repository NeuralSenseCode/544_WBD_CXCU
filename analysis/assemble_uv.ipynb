{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a9680a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f48a0",
   "metadata": {},
   "source": [
    "## Function to read iMotions sensor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_imotions(path, metadata=None):\n",
    "    \"\"\"\n",
    "    Reads an iMotions CSV file while extracting optional metadata fields.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the iMotions CSV file.\n",
    "        metadata (list[str], optional): List of metadata keys to extract.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The data as a DataFrame.\n",
    "        meta_dict (dict): Dictionary containing requested metadata fields.\n",
    "    \"\"\"\n",
    "    meta_dict = {}\n",
    "    metadata = metadata or []\n",
    "    meta_lines = []\n",
    "    count = 0\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if '#' in line.split(',')[0]:\n",
    "                meta_lines.append('#'.join(line.strip().split('#')[1:]))\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Parse requested metadata\n",
    "    for line in meta_lines:\n",
    "        # Remove leading '#' and split by comma\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 1:\n",
    "            key, value = parts[0].strip(), ','.join(parts[1:])\n",
    "            if key in metadata:\n",
    "                meta_dict[key] = value\n",
    "\n",
    "    # Read data using header row after metadata\n",
    "    df = pd.read_csv(path, header=count, low_memory=True)\n",
    "    return df, meta_dict\n",
    "\n",
    "def get_files(folder, tags=['',]):\n",
    "    return [f for f in os.listdir(folder) if not f.startswith('.') and all(x in f for x in tags)] \n",
    "\n",
    "\n",
    "def get_biometric_data(in_folder, results_folder):\n",
    "\n",
    "    ######## Define ########\n",
    "    # Define paths\n",
    "    out_path = f\"{results_folder}/\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    respondents = [1,2,3] #define list of respondent ids\n",
    "\n",
    "    # Define signal columns\n",
    "    cols_afdex = [\n",
    "                \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "                \"Surprise\", \"Engagement\", \"Valence\", \"Sentimentality\",\n",
    "                \"Confusion\", \"Neutral\"\n",
    "        ]\n",
    "    cols_eeg = ['High Engagement',\n",
    "        'Low Engagement',\n",
    "        'Distraction',\n",
    "        'Drowsy',\n",
    "        'Workload Average',\n",
    "        'Frontal Asymmetry Alpha',\n",
    "        ]\n",
    "\n",
    "\n",
    "    #Define window lengths in seconds\n",
    "    window_lengths = [3,]\n",
    "\n",
    "    ######## Read Inputs #######\n",
    "    #Get input files\n",
    "    sensor_files = get_files(f'{in_folder}/Sensors/',tags=['.csv',])\n",
    "\n",
    "    ### Begin ###\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    for respondent in respondents:\n",
    "        error = {'respondent':respondent, 'FAC':None, 'EEG':None, 'GSR':None, 'Blinks':None, 'ET':None}\n",
    "        interaction = {'respondent':respondent}\n",
    "        try:\n",
    "            file = [f for f in sensor_files if respondent in f][0] #may need adjustment\n",
    "            df_sens_resp,_ = read_imotions(f'{in_folder}/Sensors/{file}')\n",
    "\n",
    "            # Get sensor data per stimulus\n",
    "            for task in df_sens_resp['SourceStimuliName'].unique():\n",
    "                df_sens_task = df_sens_resp.loc[(df_sens_resp['SourceStimuliName']==task)]\n",
    "                window = task\n",
    "\n",
    "                # Get facial coding data\n",
    "                for a in cols_afdex:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_FAC_{a}_mean']=df_sens_task[a].dropna().mean()\n",
    "                        auc_data = df_sens_task[['Timestamp',a]].dropna()\n",
    "                        interaction[f'sens_{window}_FAC_{a}_AUC']=np.trapz(auc_data[a],x=auc_data['Timestamp'])/1000\n",
    "                        interaction[f'sens_{window}_FAC_{a}_Binary']=df_sens_task[a].dropna().max()>= 50\n",
    "                    except:\n",
    "                        error['FAC']='Missing'\n",
    "\n",
    "                for e in cols_eeg:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_EEG_{e}_mean']=df_sens_task[e].dropna()[df_sens_int[e] > -9000].mean()\n",
    "                        auc_data = df_sens_task.loc[df_sens_task[e].notna() & (df_sens_task[e] > -9000), ['Timestamp', e]]\n",
    "                        interaction[f'sens_{window}_EEG_{e}_AUC']=np.trapz(auc_data[e],x=auc_data['Timestamp'])/1000\n",
    "                    except:\n",
    "                        error['EEG']='Missing'\n",
    "\n",
    "                try:\n",
    "                    interaction[f'sens_{window}_GSR_PeakDetected_Binary'] =1 if df_sens_task['Peak Detected'].sum()>0 else 0\n",
    "                    gsr_data = df_sens_task[['Timestamp','Peak Detected']].dropna()\n",
    "                    mask = gsr_data['Peak Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = gsr_data.loc[mask, 'Peak Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_GSR_Peaks_Count'] =count_patches\n",
    "                except:\n",
    "                    error['GSR']='Missing'\n",
    "\n",
    "                try:\n",
    "                    blink_data = df_sens_task[['Timestamp','Blink Detected']].dropna()\n",
    "                    mask = blink_data['Blink Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = blink_data.loc[mask, 'Blink Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_ET_Blink_Count'] =count_patches\n",
    "                    interaction[f'sens_{window}_ET_Blink_Rate'] =count_patches/((df_sens_task['Timestamp'].values[-1]-df_sens_task['Timestamp'].values[0])/(1000 * 60))\n",
    "                except:\n",
    "                    error['ET']='Missing'\n",
    "\n",
    "            # TODO Get sensor data for non-interaction\n",
    "            ##################################### Add this in\n",
    "            results.append(interaction)\n",
    "            errors.append(error)\n",
    "\n",
    "            pass\n",
    "        except IndexError:\n",
    "            print(f'>>> Could not find {respondent} sensor data')\n",
    "        except:\n",
    "            print(f'>>> Failed {respondent}')\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(f'{out_path}biometric_results.csv')\n",
    "\n",
    "    errors = pd.DataFrame(errors)\n",
    "    errors.to_csv(f'{out_path}errors_biometric.csv')\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_export_dir = project_root / \"data\" / \"Export\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef61f8",
   "metadata": {},
   "source": [
    "## Explanation of functions\n",
    "\n",
    "The above functions are used to read in the sesor data files, one csv at a time, and extract single features per stimulus, and write these features to a simple results file.\n",
    "\n",
    "The functions must be adjusted to:\n",
    "- Discern between long form and short form\n",
    "- Isolate key moments from timings file provided by client\n",
    "- Extract time series\n",
    "- Compute group-wide features such as inter-subject correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84624884",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- Create naming dictionary for all stims\n",
    "- Get total times of all stims\n",
    "- Prepare key_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ef81e",
   "metadata": {},
   "source": [
    "## Stimulus duration scan\n",
    "We load one sensor recording per group, extract the unique stimulus names, and estimate the average exposure duration per stimulus using the `Timestamp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78dad9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sensor_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>001_116.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>001_58.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>001_114.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>001_102.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>001_108.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>001_107.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  sensor_file\n",
       "0  Group A  001_116.csv\n",
       "1  Group B   001_58.csv\n",
       "2  Group C  001_114.csv\n",
       "3  Group D  001_102.csv\n",
       "4  Group E  001_108.csv\n",
       "5  Group F  001_107.csv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate one sensor export per group for duration scanning\n",
    "\n",
    "\n",
    "group_sensor_files = {}\n",
    "for group_dir in sorted(data_export_dir.glob(\"Group *\")):\n",
    "    if not group_dir.is_dir():\n",
    "        continue\n",
    "    sensor_dirs = sorted(group_dir.glob(\"Analyses/*/Sensor Data\"))\n",
    "    csv_candidates = []\n",
    "    for sensor_dir in sensor_dirs:\n",
    "        csv_candidates.extend(sorted(sensor_dir.glob(\"*.csv\")))\n",
    "    group_sensor_files[group_dir.name] = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "sensor_selection = pd.DataFrame([\n",
    "    {\n",
    "        \"group\": group,\n",
    "        \"sensor_file\": path.name if path else None\n",
    "    }\n",
    "    for group, path in group_sensor_files.items()\n",
    "]).sort_values(\"group\").reset_index(drop=True)\n",
    "\n",
    "sensor_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9521e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n"
     ]
    }
   ],
   "source": [
    "# Collect per-group stimulus durations without aggregating across groups\n",
    "duration_tables = []\n",
    "issues = {}\n",
    "\n",
    "for group, path in group_sensor_files.items():\n",
    "    if path is None:\n",
    "        issues[group] = \"No sensor CSV found\"\n",
    "        continue\n",
    "    try:\n",
    "        df_group, _ = read_imotions(path)\n",
    "    except Exception as exc:\n",
    "        issues[group] = f\"read_imotions failed: {exc}\"\n",
    "        continue\n",
    "\n",
    "    required_cols = {\"SourceStimuliName\", \"Timestamp\"}\n",
    "    if not required_cols.issubset(df_group.columns):\n",
    "        issues[group] = \"Missing SourceStimuliName or Timestamp\"\n",
    "        continue\n",
    "\n",
    "    df_clean = df_group[[\"SourceStimuliName\", \"Timestamp\"]].copy()\n",
    "    df_clean = df_clean.dropna(subset=[\"SourceStimuliName\"])\n",
    "    df_clean[\"Timestamp\"] = pd.to_numeric(df_clean[\"Timestamp\"], errors=\"coerce\")\n",
    "    df_clean = df_clean.dropna(subset=[\"Timestamp\"])\n",
    "    if df_clean.empty:\n",
    "        issues[group] = \"No valid timestamp data\"\n",
    "        continue\n",
    "\n",
    "    group_duration = (\n",
    "        df_clean.groupby(\"SourceStimuliName\")[\"Timestamp\"]\n",
    "        .apply(lambda s: s.max() - s.min())\n",
    "        .reset_index(name=\"duration_ms\")\n",
    "    )\n",
    "\n",
    "    if group_duration.empty:\n",
    "        issues[group] = \"No stimuli with duration\"\n",
    "        continue\n",
    "\n",
    "    group_duration[\"duration_seconds\"] = group_duration[\"duration_ms\"] / 1000.0\n",
    "    group_duration[\"duration_minutes\"] = group_duration[\"duration_seconds\"] / 60.0\n",
    "    group_duration.insert(0, \"group\", group)\n",
    "    group_duration.rename(columns={\"SourceStimuliName\": \"stimulus_name\"}, inplace=True)\n",
    "    duration_tables.append(group_duration[[\"group\", \"stimulus_name\", \"duration_seconds\", \"duration_minutes\"]])\n",
    "\n",
    "if duration_tables:\n",
    "    stimulus_summary = pd.concat(duration_tables, ignore_index=True)\n",
    "    stimulus_summary.sort_values([\"group\", \"stimulus_name\"], inplace=True)\n",
    "    stimulus_summary[\"duration_seconds\"] = stimulus_summary[\"duration_seconds\"].round(2)\n",
    "    stimulus_summary[\"duration_minutes\"] = stimulus_summary[\"duration_minutes\"].round(2)\n",
    "    stimulus_summary.reset_index(drop=True, inplace=True)\n",
    "    stimulus_summary\n",
    "else:\n",
    "    print(\"No duration records computed.\")\n",
    "\n",
    "if issues:\n",
    "    pd.DataFrame(\n",
    "        {\"group\": list(issues.keys()), \"issue\": list(issues.values())}\n",
    "    ).sort_values(\"group\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9548d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>07 The Notebook</td>\n",
       "      <td>65.39</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group C</td>\n",
       "      <td>09 I Am Legend - Infected encounter</td>\n",
       "      <td>118.49</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>10 The Town - Bank robbery in nun masks</td>\n",
       "      <td>263.21</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group C</td>\n",
       "      <td>Abbott Elementary - S1E9 - Step Class</td>\n",
       "      <td>1291.75</td>\n",
       "      <td>21.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group C</td>\n",
       "      <td>HOME ALONE</td>\n",
       "      <td>115.09</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group                            stimulus_name  duration_seconds  \\\n",
       "0  Group C                          07 The Notebook             65.39   \n",
       "1  Group C      09 I Am Legend - Infected encounter            118.49   \n",
       "2  Group C  10 The Town - Bank robbery in nun masks            263.21   \n",
       "3  Group C    Abbott Elementary - S1E9 - Step Class           1291.75   \n",
       "4  Group C                               HOME ALONE            115.09   \n",
       "\n",
       "   duration_minutes  \n",
       "0              1.09  \n",
       "1              1.97  \n",
       "2              4.39  \n",
       "3             21.53  \n",
       "4              1.92  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38df137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group A': 'read_imotions failed: Unable to allocate 853. MiB for an array with shape (210, 532611) and data type float64',\n",
       " 'Group B': 'read_imotions failed: Unable to allocate 838. MiB for an array with shape (212, 518407) and data type float64',\n",
       " 'Group D': 'read_imotions failed: Unable to allocate 843. MiB for an array with shape (221, 499863) and data type float64',\n",
       " 'Group E': 'read_imotions failed: Unable to allocate 825. MiB for an array with shape (212, 510043) and data type float64',\n",
       " 'Group F': 'read_imotions failed: Unable to allocate 701. MiB for an array with shape (212, 433145) and data type float64'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ec5417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd2d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      65.39\n",
       "max    1291.75\n",
       "Name: duration_seconds, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary['duration_seconds'].agg(['min','max']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6057f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>unique_stimuli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  unique_stimuli\n",
       "0  Group C               6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_per_group = stimulus_summary.groupby('group')['stimulus_name'].nunique().reset_index(name='unique_stimuli')\n",
    "stimuli_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_summary.to_csv(project_root / \"results\" / \"stimulus_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322effdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf37ed",
   "metadata": {},
   "source": [
    "## Stimulus Annotation Overview\n",
    "- `stimulus_rename` links each group-specific stimulus from `stimulus_summary` to a clean `title` and its presentation `Form` (`Long` or `Short`).\n",
    "- Some titles appear in both forms; the long cut (≈30 min) includes the short-form key moment as an embedded segment.\n",
    "- `key_moments` pinpoints, for every long-form title, when the key moment begins (`Lead-up Duration`) and how long it lasts (`Key moment Duration_LF`).\n",
    "- These tables let us align short-form clips with the corresponding segment inside the long-form presentation for downstream comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f5419",
   "metadata": {},
   "source": [
    "## Stage 1: Demographics\n",
    "We extract respondent-level identifiers and timing information from the metadata embedded in each sensor export to seed the unified view (UV). This pass scans every sensor CSV, captures study name, respondent attributes, and recording timestamps, and prepares the foundation for later feature merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bff719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 853. MiB for an array with shape (210, 532611) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m demographic_records = []\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m csv_path \u001b[38;5;129;01min\u001b[39;00m sensor_file_paths:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     _, meta = \u001b[43mread_imotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     study_clean = first_segment(meta.get(\u001b[33m\"\u001b[39m\u001b[33mStudy name\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     54\u001b[39m     respondent_group_clean = first_segment(meta.get(\u001b[33m\"\u001b[39m\u001b[33mRespondent Group\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mread_imotions\u001b[39m\u001b[34m(path, metadata)\u001b[39m\n\u001b[32m     41\u001b[39m             meta_dict[key] = value\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Read data using header row after metadata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df, meta_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2139\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[32m   2122\u001b[39m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[32m   2123\u001b[39m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2135\u001b[39m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[32m   2136\u001b[39m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[32m   2138\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m         blocks = \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2140\u001b[39m         mgr = BlockManager(blocks, axes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2212\u001b[39m, in \u001b[36m_form_blocks\u001b[39m\u001b[34m(arrays, consolidate, refs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype.type, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m   2210\u001b[39m     dtype = np.dtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m values, placement = \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[32m   2214\u001b[39m     values = ensure_wrapped_if_datetimelike(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashra\\Documents\\NeuralSense\\NeuralData\\clients\\544_WBD_CXCU\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2252\u001b[39m, in \u001b[36m_stack_arrays\u001b[39m\u001b[34m(tuples, dtype)\u001b[39m\n\u001b[32m   2249\u001b[39m first = arrays[\u001b[32m0\u001b[39m]\n\u001b[32m   2250\u001b[39m shape = (\u001b[38;5;28mlen\u001b[39m(arrays),) + first.shape\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m stacked = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m   2254\u001b[39m     stacked[i] = arr\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 853. MiB for an array with shape (210, 532611) and data type float64"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "metadata_keys = [\n",
    "    \"Study name\",\n",
    "    \"Respondent Name\",\n",
    "    \"Respondent Age\",\n",
    "    \"Respondent Gender\",\n",
    "    \"Respondent Group\",\n",
    "    \"Recording time\"\n",
    "]\n",
    "\n",
    "sensor_file_paths = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    ")\n",
    "\n",
    "def first_segment(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    return str(value).split(',')[0].strip()\n",
    "\n",
    "def parse_gender(raw_gender):\n",
    "    if not raw_gender:\n",
    "        return None\n",
    "    gender_lower = raw_gender.lower()\n",
    "    if \"female\" in gender_lower:\n",
    "        return \"Female\"\n",
    "    if \"male\" in gender_lower:\n",
    "        return \"Male\"\n",
    "    if \"other\" in gender_lower:\n",
    "        return \"Other\"\n",
    "    return raw_gender.title()\n",
    "\n",
    "def extract_group_letter(study_value, fallback_values):\n",
    "    if study_value:\n",
    "        terminal_match = re.search(r\"([A-Za-z])$\", study_value.strip())\n",
    "        if terminal_match:\n",
    "            return terminal_match.group(1).upper()\n",
    "        letters = re.findall(r\"[A-Za-z]\", study_value)\n",
    "        if letters:\n",
    "            return letters[-1].upper()\n",
    "    for candidate in fallback_values:\n",
    "        if candidate:\n",
    "            match = re.search(r\"Group\\s*([A-F])\", str(candidate), flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "demographic_records = []\n",
    "\n",
    "for csv_path in sensor_file_paths:\n",
    "    _, meta = read_imotions(csv_path, metadata=metadata_keys)\n",
    "\n",
    "    study_clean = first_segment(meta.get(\"Study name\"))\n",
    "    respondent_group_clean = first_segment(meta.get(\"Respondent Group\"))\n",
    "    group_letter = extract_group_letter(study_clean, [respondent_group_clean, csv_path.as_posix()])\n",
    "\n",
    "    respondent_raw = first_segment(meta.get(\"Respondent Name\"))\n",
    "    respondent_value = csv_path.stem\n",
    "    if respondent_raw:\n",
    "        respondent_digits = re.search(r\"\\d+\", respondent_raw)\n",
    "        if respondent_digits:\n",
    "            respondent_value = respondent_digits.group(0)\n",
    "        else:\n",
    "            respondent_value = respondent_raw\n",
    "\n",
    "    age_raw = first_segment(meta.get(\"Respondent Age\"))\n",
    "    age_numeric = pd.to_numeric(age_raw, errors=\"coerce\")\n",
    "    if pd.notna(age_numeric):\n",
    "        age_value = int(age_numeric)\n",
    "    else:\n",
    "        age_value = pd.NA\n",
    "\n",
    "    gender_raw = first_segment(meta.get(\"Respondent Gender\"))\n",
    "    gender_value = parse_gender(gender_raw)\n",
    "\n",
    "    recording_raw = meta.get(\"Recording time\")\n",
    "    date_study = None\n",
    "    time_study = None\n",
    "    if recording_raw:\n",
    "        fragments = [frag.strip() for frag in str(recording_raw).split(',') if frag.strip()]\n",
    "        date_part = None\n",
    "        time_part = None\n",
    "        for fragment in fragments:\n",
    "            if fragment.lower().startswith(\"date:\"):\n",
    "                date_part = fragment.split(':', 1)[1].strip()\n",
    "            elif fragment.lower().startswith(\"time:\"):\n",
    "                time_part = fragment.split(':', 1)[1].strip()\n",
    "        if date_part and time_part:\n",
    "            dt_string = f\"{date_part} {time_part}\"\n",
    "            ts = pd.to_datetime(dt_string, utc=True, errors=\"coerce\")\n",
    "            if pd.isna(ts):\n",
    "                ts = pd.to_datetime(dt_string, errors=\"coerce\")\n",
    "                if pd.notna(ts) and ts.tzinfo is None:\n",
    "                    try:\n",
    "                        ts = ts.tz_localize(\"America/Chicago\")\n",
    "                    except Exception:\n",
    "                        ts = ts.tz_localize(\"UTC\")\n",
    "            if pd.notna(ts):\n",
    "                if ts.tzinfo is None:\n",
    "                    ts = ts.tz_localize(\"America/Chicago\")\n",
    "                else:\n",
    "                    ts = ts.tz_convert(\"America/Chicago\")\n",
    "                date_study = ts.strftime(\"%m/%d/%Y\")\n",
    "                time_study = ts.strftime(\"%H:%M:%S\")\n",
    "            else:\n",
    "                date_study = date_part\n",
    "        elif date_part:\n",
    "            date_study = date_part\n",
    "\n",
    "    demographic_records.append({\n",
    "        \"source_file\": csv_path.name,\n",
    "        \"group\": group_letter,\n",
    "        \"respondent\": respondent_value,\n",
    "        \"age\": age_value,\n",
    "        \"gender\": gender_value,\n",
    "        \"date_study\": date_study,\n",
    "        \"time_study\": time_study\n",
    "    })\n",
    "\n",
    "uv_stage1 = pd.DataFrame(demographic_records)\n",
    "\n",
    "if not uv_stage1.empty:\n",
    "    uv_stage1 = uv_stage1.sort_values([\"group\", \"respondent\"]).reset_index(drop=True)\n",
    "    uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "    uv_stage1[\"age\"] = uv_stage1[\"age\"].astype(\"Int64\")\n",
    "\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7fbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [respondent, gender]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_gender_overrides = {\n",
    "    \"8\": \"Female\",\n",
    "    \"56\": \"Male\",\n",
    "    \"16\": \"Male\",\n",
    "    \"6\": \"Male\",\n",
    "    \"46\": \"Female\",\n",
    "    \"69\": \"Female\",\n",
    "    \"44\": \"Male\",\n",
    "    \"50\": \"Male\"\n",
    "}\n",
    "\n",
    "uv_stage1[\"gender\"] = uv_stage1.apply(\n",
    "    lambda row: manual_gender_overrides.get(row[\"respondent\"], row[\"gender\"]), axis=1\n",
    ")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1[uv_stage1[\"respondent\"].isin(manual_gender_overrides.keys())][[\"respondent\", \"gender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da774bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  age age_group ethnicity                income_group  \\\n",
       "0        104   59     44-59     White    $60,000 or more per year   \n",
       "1        106   30     28-43     White    $60,000 or more per year   \n",
       "2         11   33     28-43     White  $35,000  $60,000 per year   \n",
       "3        116   19     18-27     White  $35,000  $60,000 per year   \n",
       "4          3   34     28-43     White    $60,000 or more per year   \n",
       "\n",
       "           content_consumption  content_consumption_movies  \\\n",
       "0  More than 24 hours per week                          10   \n",
       "1       3 to 12 hours per week                          25   \n",
       "2      12 to 24 hours per week                          40   \n",
       "3       3 to 12 hours per week                          25   \n",
       "4      12 to 24 hours per week                          10   \n",
       "\n",
       "   content_consumption_series  content_consumption_short  \n",
       "0                          90                          0  \n",
       "1                          50                         25  \n",
       "2                          40                         20  \n",
       "3                          50                         25  \n",
       "4                          70                         20  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach supplemental demographics from grid.csv\n",
    "grid_path = project_root / \"data\" / \"grid.csv\"\n",
    "grid_rename_map = {\n",
    "    \"QB2. Age.1\": \"age_group\",\n",
    "    \"QC. Ethnicity\": \"ethnicity\",\n",
    "    \"QD. Income\": \"income_group\",\n",
    "    \"Q1. Content Hours Per Week\": \"content_consumption\",\n",
    "    \"Q2. Program Type %- Movies\": \"content_consumption_movies\",\n",
    "    \"Q2. Program Type %- Series\": \"content_consumption_series\",\n",
    "    \"Q2. Program Type %- Short\": \"content_consumption_short\",\n",
    "}\n",
    "grid_columns = [\"respondent\", *grid_rename_map.keys()]\n",
    "grid_raw = pd.read_csv(grid_path, encoding=\"latin1\", usecols=grid_columns)\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "grid_raw[\"respondent\"] = pd.to_numeric(grid_raw[\"respondent\"], errors=\"coerce\").astype(\"Int64\")\n",
    "grid_raw = grid_raw.dropna(subset=[\"respondent\"])\n",
    "grid_subset = grid_raw.rename(columns=grid_rename_map).copy()\n",
    "\n",
    "grid_subset[\"respondent\"] = grid_subset[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "text_cols = [\"age_group\", \"ethnicity\", \"income_group\", \"content_consumption\"]\n",
    "for col in text_cols:\n",
    "    grid_subset[col] = grid_subset[col].apply(lambda value: value.strip() if isinstance(value, str) else value)\n",
    "\n",
    "numeric_cols = [\"content_consumption_movies\", \"content_consumption_series\", \"content_consumption_short\"]\n",
    "for col in numeric_cols:\n",
    "    grid_subset[col] = pd.to_numeric(grid_subset[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "grid_subset = grid_subset.drop_duplicates(subset=\"respondent\", keep=\"first\")\n",
    "\n",
    "uv_stage1 = uv_stage1.merge(grid_subset, on=\"respondent\", how=\"left\", validate=\"many_to_one\")\n",
    "uv = uv_stage1.copy()\n",
    "\n",
    "uv_stage1.loc[:, [\n",
    "    \"respondent\",\n",
    "    \"age\",\n",
    "    \"age_group\",\n",
    "    \"ethnicity\",\n",
    "    \"income_group\",\n",
    "    \"content_consumption\",\n",
    "    \"content_consumption_movies\",\n",
    "    \"content_consumption_series\",\n",
    "    \"content_consumption_short\",\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb769cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_respondents = uv_stage1[uv_stage1.duplicated(subset=\"respondent\", keep=False)]\n",
    "if duplicate_respondents.empty:\n",
    "    print(\"No duplicate respondents detected.\")\n",
    "else:\n",
    "    duplicate_respondents.sort_values(\"respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92662888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age_group</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>44-59</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006_11.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>18-27</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "      <td>Other</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>60-69</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  age  gender  date_study time_study  \\\n",
       "0   003_104.csv     A        104   59    Male  10/16/2025   18:09:03   \n",
       "1   002_106.csv     A        106   30    Male  10/16/2025   19:35:05   \n",
       "2    006_11.csv     A         11   33  Female  10/11/2025   09:32:42   \n",
       "3   001_116.csv     A        116   19    Male  10/18/2025   12:37:40   \n",
       "4     007_3.csv     A          3   34  Female  10/10/2025   09:19:22   \n",
       "..          ...   ...        ...  ...     ...         ...        ...   \n",
       "78   005_50.csv     F         50   63   Other  10/14/2025   09:54:03   \n",
       "79   004_60.csv     F         60   66    Male  10/15/2025   09:34:06   \n",
       "80   003_70.csv     F         70   61  Female  10/16/2025   09:49:14   \n",
       "81   002_85.csv     F         85   34  Female  10/17/2025   14:37:41   \n",
       "82   004_96.csv     F         96   29  Female  10/15/2025   14:32:00   \n",
       "\n",
       "   age_group               ethnicity                income_group  \\\n",
       "0      44-59                   White    $60,000 or more per year   \n",
       "1      28-43                   White    $60,000 or more per year   \n",
       "2      28-43                   White  $35,000  $60,000 per year   \n",
       "3      18-27                   White  $35,000  $60,000 per year   \n",
       "4      28-43                   White    $60,000 or more per year   \n",
       "..       ...                     ...                         ...   \n",
       "78     60-69  Black/African American    $60,000 or more per year   \n",
       "79     60-69                   White  $35,000  $60,000 per year   \n",
       "80     60-69  Black/African American  $35,000  $60,000 per year   \n",
       "81     28-43                   White    $60,000 or more per year   \n",
       "82     28-43                   White    $60,000 or more per year   \n",
       "\n",
       "            content_consumption  content_consumption_movies  \\\n",
       "0   More than 24 hours per week                          10   \n",
       "1        3 to 12 hours per week                          25   \n",
       "2       12 to 24 hours per week                          40   \n",
       "3        3 to 12 hours per week                          25   \n",
       "4       12 to 24 hours per week                          10   \n",
       "..                          ...                         ...   \n",
       "78      12 to 24 hours per week                          70   \n",
       "79       3 to 12 hours per week                          10   \n",
       "80      12 to 24 hours per week                          30   \n",
       "81      12 to 24 hours per week                          40   \n",
       "82      12 to 24 hours per week                          20   \n",
       "\n",
       "    content_consumption_series  content_consumption_short  \n",
       "0                           90                          0  \n",
       "1                           50                         25  \n",
       "2                           40                         20  \n",
       "3                           50                         25  \n",
       "4                           70                         20  \n",
       "..                         ...                        ...  \n",
       "78                          20                         10  \n",
       "79                          80                         10  \n",
       "80                          30                         40  \n",
       "81                          60                          0  \n",
       "82                          60                         20  \n",
       "\n",
       "[83 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv_stage1.to_csv(project_root / \"results\" / \"uv_stage1_demographics.csv\", index=False)\n",
    "uv_stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fa7e8",
   "metadata": {},
   "source": [
    "## Stage 2: Sensor Data\n",
    "After validating the workflow on the pilot cohort, we apply the sensor feature extraction pipeline to the complete respondent roster, using the finalized mappings, windowing logic, and feature naming to generate the full unified view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f5575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>title</th>\n",
       "      <th>form</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_letter</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>A STAR IS BORN</th>\n",
       "      <td>Group A</td>\n",
       "      <td>A Star Is Born</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOME ALONE</th>\n",
       "      <td>Group A</td>\n",
       "      <td>Home Alone</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAD MAX FURY ROAD</th>\n",
       "      <td>Group A</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE CONJURING</th>\n",
       "      <td>Group A</td>\n",
       "      <td>The Conjuring</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE TOWN</th>\n",
       "      <td>Group A</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  group           title   form\n",
       "group_letter stimulus_name                                    \n",
       "A            A STAR IS BORN     Group A  A Star Is Born  Short\n",
       "             HOME ALONE         Group A      Home Alone  Short\n",
       "             MAD MAX FURY ROAD  Group A         Mad Max  Short\n",
       "             THE CONJURING      Group A   The Conjuring  Short\n",
       "             THE TOWN           Group A        The Town   Long"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stimulus annotations and key-moment timing tables\n",
    "uv_stage1 = pd.read_csv(project_root / \"results\" / \"uv_stage1_demographics.csv\")\n",
    "\n",
    "stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"Group\\s*([A-F])\", expand=False).str.upper()\n",
    "stimulus_map_lookup = stimulus_map.set_index([\"group_letter\", \"stimulus_name\"]).sort_index()\n",
    "\n",
    "key_moments_raw = pd.read_csv(project_root / \"data\" / \"key_moments.csv\")\n",
    "time_columns = [\"Lead-up Duration\", \"Key moment Duration_LF\"]\n",
    "key_moments = key_moments_raw[[\"title\", *time_columns]].dropna(subset=[\"title\"]).copy()\n",
    "\n",
    "def hhmmss_to_ms(value):\n",
    "    \"\"\"Convert hh:mm:ss strings to integer milliseconds (None on failure).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        duration = pd.to_timedelta(text)\n",
    "    except ValueError:\n",
    "        # Handle mm:ss formatted entries by padding hours when possible\n",
    "        parts = text.split(\":\")\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                duration = pd.to_timedelta(f\"00:{text}\")\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    return int(duration.total_seconds() * 1000)\n",
    "\n",
    "key_moments[\"lead_up_ms\"] = key_moments[\"Lead-up Duration\"].apply(hhmmss_to_ms)\n",
    "key_moments[\"key_moment_ms\"] = key_moments[\"Key moment Duration_LF\"].apply(hhmmss_to_ms)\n",
    "key_moment_lookup = key_moments.set_index(\"title\")[\"lead_up_ms\"].to_dict()\n",
    "key_duration_lookup = key_moments.set_index(\"title\")[\"key_moment_ms\"].to_dict()\n",
    "\n",
    "stimulus_map_lookup.head()\n",
    "\n",
    "# Clear in-memory sensor dataframes to free memory\n",
    "import gc\n",
    "_sensor_df_names = []\n",
    "for _name, _obj in list(globals().items()):\n",
    "    if isinstance(_obj, pd.DataFrame) and \"SourceStimuliName\" in _obj.columns:\n",
    "        _sensor_df_names.append(_name)\n",
    "        del globals()[_name]\n",
    "gc.collect()\n",
    "print(f\"Cleared sensor dataframes: {_sensor_df_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "433227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "fac_columns = [\n",
    "    \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "    \"Surprise\", \"Engagement\", \"Sentimentality\",\n",
    "    \"Confusion\", \"Neutral\",\n",
    " ]\n",
    "fac_adaptive_metrics = {\n",
    "    \"AdaptiveEngagement\": \"Adaptive Engagement\",\n",
    "    \"PositiveAdaptiveValence\": \"Positive Adaptive Valence\",\n",
    "    \"NegativeAdaptiveValence\": \"Negative Adaptive Valence\",\n",
    "    \"NeutralAdaptiveValence\": \"Neutral Adaptive Valence\",\n",
    "}\n",
    "eeg_columns = [\n",
    "    \"High Engagement\",\n",
    "    \"Low Engagement\",\n",
    "    \"Distraction\",\n",
    "    \"Drowsy\",\n",
    "    \"Workload Average\",\n",
    "    \"Frontal Alpha Asymmetry\",\n",
    " ]\n",
    "eeg_alternate_columns = {\n",
    "    \"Frontal Alpha Asymmetry\": [\"Frontal Alpha Asymmetry\", \"Frontal Asymmetry Alpha\"]\n",
    "}\n",
    "eeg_metric_alias = {\n",
    "    \"High Engagement\": \"HighEngagement\",\n",
    "    \"Low Engagement\": \"LowEngagement\",\n",
    "    \"Distraction\": \"Distraction\",\n",
    "    \"Drowsy\": \"Drowsy\",\n",
    "    \"Workload Average\": \"Workload\",\n",
    "    \"Frontal Alpha Asymmetry\": \"FrontalAlphaAsymmetry\",\n",
    "}\n",
    "sensor_required_columns = {\n",
    "    \"FAC\": fac_columns + list(fac_adaptive_metrics.values()),\n",
    "    \"EEG\": eeg_columns,\n",
    "    \"GSR\": [\"Peak Detected\"],\n",
    "    \"ET\": [\n",
    "        \"Blink Detected\",\n",
    "        \"Fixation Dispersion\",\n",
    "        \"Fixation Index\",\n",
    "        \"Fixation Duration\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def _trapezoid_integral(values: np.ndarray, time_axis: np.ndarray) -> float:\n",
    "    \"\"\"Integrate using numpy.trapezoid when available, falling back to trapz.\"\"\"\n",
    "    integrate = getattr(np, \"trapezoid\", np.trapz)\n",
    "    return float(integrate(values, x=time_axis) / 1000.0)\n",
    "\n",
    "def prepare_stimulus_segment(df_sensor: pd.DataFrame, raw_name: str, form: str, title: str) -> pd.DataFrame:\n",
    "    \"\"\"Return the time-zeroed slice for the requested stimulus, clipping to key moments when needed.\"\"\"\n",
    "    if \"SourceStimuliName\" not in df_sensor.columns or \"Timestamp\" not in df_sensor.columns:\n",
    "        return pd.DataFrame()\n",
    "    subset = df_sensor.loc[df_sensor[\"SourceStimuliName\"] == raw_name].copy()\n",
    "    if subset.empty:\n",
    "        return subset\n",
    "    subset[\"Timestamp\"] = pd.to_numeric(subset[\"Timestamp\"], errors=\"coerce\")\n",
    "    subset = subset.dropna(subset=[\"Timestamp\"])\n",
    "    if subset.empty:\n",
    "        return subset\n",
    "    subset.sort_values(\"Timestamp\", inplace=True)\n",
    "    if \"SlideEvent\" in subset.columns:\n",
    "        slide_events = subset[\"SlideEvent\"].astype(str)\n",
    "        start_candidates = subset.loc[slide_events == \"StartMedia\", \"Timestamp\"]\n",
    "    else:\n",
    "        start_candidates = pd.Series(dtype=float)\n",
    "    if not start_candidates.empty:\n",
    "        start_timestamp = start_candidates.iloc[0]\n",
    "    else:\n",
    "        start_timestamp = subset[\"Timestamp\"].min()\n",
    "    subset[\"time_from_start\"] = subset[\"Timestamp\"] - start_timestamp\n",
    "    if form == \"Long\":\n",
    "        lead_ms = key_moment_lookup.get(title)\n",
    "        duration_ms = key_duration_lookup.get(title)\n",
    "        if lead_ms is None or duration_ms is None:\n",
    "            return pd.DataFrame()\n",
    "        window_start = lead_ms\n",
    "        window_end = lead_ms + duration_ms\n",
    "        subset = subset.loc[(subset[\"time_from_start\"] >= window_start) & (subset[\"time_from_start\"] <= window_end)].copy()\n",
    "        if subset.empty:\n",
    "            return subset\n",
    "        subset[\"time_from_start\"] = subset[\"time_from_start\"] - window_start\n",
    "    return subset\n",
    "\n",
    "def register_feature(container: Dict[str, float], form: str, title: str, sensor: str, metric: str, method: str, value: float) -> None:\n",
    "    if value is None:\n",
    "        return\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return\n",
    "    key = f\"{form}_{title}_{sensor}_{metric}_{method}\"\n",
    "    container[key] = value\n",
    "\n",
    "def compute_sensor_features(segment: pd.DataFrame, form: str, title: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute FAC, EEG, GSR, and ET summary statistics for a stimulus segment.\"\"\"\n",
    "    features: Dict[str, float] = {}\n",
    "    if segment.empty:\n",
    "        return features\n",
    "    if \"time_from_start\" not in segment.columns or segment[\"time_from_start\"].empty:\n",
    "        return features\n",
    "    # Duration in seconds for this stimulus window\n",
    "    duration_ms = float(segment[\"time_from_start\"].max() - segment[\"time_from_start\"].min())\n",
    "    if duration_ms <= 0:\n",
    "        return features\n",
    "    duration_seconds = duration_ms / 1000.0\n",
    "    features[f\"{form}_{title}_duration\"] = duration_seconds\n",
    "    duration_minutes = duration_seconds / 60.0\n",
    "    # Facial coding summaries\n",
    "    for metric in fac_columns:\n",
    "        if metric not in segment.columns:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[metric], errors=\"coerce\").dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[values.index, \"time_from_start\"].values\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Mean\", float(values.mean()))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"AUC\", _trapezoid_integral(values.values, time_axis))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Binary\", int(values.max() >= 50))\n",
    "    for metric, column_name in fac_adaptive_metrics.items():\n",
    "        if column_name not in segment.columns:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[column_name], errors=\"coerce\").dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[values.index, \"time_from_start\"].values\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Mean\", float(values.mean()))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"AUC\", _trapezoid_integral(values.values, time_axis))\n",
    "    # EEG summaries\n",
    "    for metric in eeg_columns:\n",
    "        candidate_columns = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "        actual_column = next((col for col in candidate_columns if col in segment.columns), None)\n",
    "        if actual_column is None:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[actual_column], errors=\"coerce\")\n",
    "        valid = values.loc[values > -9000].dropna()\n",
    "        if valid.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[valid.index, \"time_from_start\"].values\n",
    "        label = eeg_metric_alias.get(metric, metric)\n",
    "        register_feature(features, form, title, \"EEG\", label, \"Mean\", float(valid.mean()))\n",
    "        register_feature(features, form, title, \"EEG\", label, \"AUC\", _trapezoid_integral(valid.values, time_axis))\n",
    "    # GSR summaries\n",
    "    if \"Peak Detected\" in segment.columns:\n",
    "        peak_series = pd.to_numeric(segment[\"Peak Detected\"], errors=\"coerce\").fillna(0)\n",
    "        peak_mask = peak_series >= 1\n",
    "        register_feature(features, form, title, \"GSR\", \"PeakDetected\", \"Binary\", int(peak_mask.any()))\n",
    "        if peak_mask.any():\n",
    "            segments = (peak_mask != peak_mask.shift()).cumsum()\n",
    "            peak_blocks = segments.loc[peak_mask]\n",
    "            peak_count = int(peak_blocks.nunique())\n",
    "            register_feature(features, form, title, \"GSR\", \"Peaks\", \"Count\", peak_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"GSR\", \"Peaks\", \"PerMinute\", float(peak_count / duration_minutes))\n",
    "    # ET metrics\n",
    "    if \"Blink Detected\" in segment.columns:\n",
    "        blink_series = pd.to_numeric(segment[\"Blink Detected\"], errors=\"coerce\").fillna(0)\n",
    "        blink_mask = blink_series >= 1\n",
    "        if blink_mask.any():\n",
    "            segments = (blink_mask != blink_mask.shift()).cumsum()\n",
    "            blink_blocks = segments.loc[blink_mask]\n",
    "            blink_count = int(blink_blocks.nunique())\n",
    "            register_feature(features, form, title, \"ET\", \"Blink\", \"Count\", blink_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"ET\", \"Blink\", \"Rate\", float(blink_count / duration_minutes))\n",
    "    if \"Fixation Dispersion\" in segment.columns:\n",
    "        dispersion = pd.to_numeric(segment[\"Fixation Dispersion\"], errors=\"coerce\").dropna()\n",
    "        if not dispersion.empty:\n",
    "            register_feature(features, form, title, \"ET\", \"FixationDispersion\", \"Mean\", float(dispersion.mean()))\n",
    "    if \"Fixation Index\" in segment.columns:\n",
    "        fixation_index = pd.to_numeric(segment[\"Fixation Index\"], errors=\"coerce\").dropna()\n",
    "        if not fixation_index.empty:\n",
    "            fixation_count = int(fixation_index.nunique())\n",
    "            register_feature(features, form, title, \"ET\", \"Fixation\", \"Count\", fixation_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"ET\", \"Fixation\", \"PerMinute\", float(fixation_count / duration_minutes))\n",
    "    if \"Fixation Duration\" in segment.columns:\n",
    "        fixation_duration = pd.to_numeric(segment[\"Fixation Duration\"], errors=\"coerce\").dropna()\n",
    "        if not fixation_duration.empty:\n",
    "            register_feature(features, form, title, \"ET\", \"FixationDuration\", \"Mean\", float(fixation_duration.mean()))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feddd2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>FAC_data_missing</th>\n",
       "      <th>EEG_data_missing</th>\n",
       "      <th>GSR_data_missing</th>\n",
       "      <th>ET_data_missing</th>\n",
       "      <th>Short_The Big Bang Theory_duration</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_Mean</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_AUC</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_Binary</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Contempt_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Titanic_EEG_FrontalAlphaAsymmetry_AUC</th>\n",
       "      <th>Short_Titanic_GSR_PeakDetected_Binary</th>\n",
       "      <th>Short_Titanic_GSR_Peaks_Count</th>\n",
       "      <th>Short_Titanic_GSR_Peaks_PerMinute</th>\n",
       "      <th>Short_Titanic_ET_Blink_Count</th>\n",
       "      <th>Short_Titanic_ET_Blink_Rate</th>\n",
       "      <th>Short_Titanic_ET_FixationDispersion_Mean</th>\n",
       "      <th>Short_Titanic_ET_Fixation_Count</th>\n",
       "      <th>Short_Titanic_ET_Fixation_PerMinute</th>\n",
       "      <th>Short_Titanic_ET_FixationDuration_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.751817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>591.261358</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.098343</td>\n",
       "      <td>0.261135</td>\n",
       "      <td>156.0</td>\n",
       "      <td>156.333511</td>\n",
       "      <td>459.838233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  FAC_data_missing  EEG_data_missing  GSR_data_missing  \\\n",
       "0          2                 0                 1                 0   \n",
       "1         58                 0                 0                 0   \n",
       "2        116                 0                 0                 0   \n",
       "\n",
       "   ET_data_missing  Short_The Big Bang Theory_duration  \\\n",
       "0                0                              186.82   \n",
       "1                0                                 NaN   \n",
       "2                0                                 NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_Mean  \\\n",
       "0                                       0.0   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_AUC  \\\n",
       "0                                      0.0   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_Binary  \\\n",
       "0                                         0.0   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Contempt_Mean  ...  \\\n",
       "0                                     0.001964  ...   \n",
       "1                                          NaN  ...   \n",
       "2                                          NaN  ...   \n",
       "\n",
       "   Short_Titanic_EEG_FrontalAlphaAsymmetry_AUC  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                     4.751817   \n",
       "\n",
       "   Short_Titanic_GSR_PeakDetected_Binary  Short_Titanic_GSR_Peaks_Count  \\\n",
       "0                                    NaN                            NaN   \n",
       "1                                    NaN                            NaN   \n",
       "2                                    1.0                          590.0   \n",
       "\n",
       "   Short_Titanic_GSR_Peaks_PerMinute  Short_Titanic_ET_Blink_Count  \\\n",
       "0                                NaN                           NaN   \n",
       "1                                NaN                           NaN   \n",
       "2                         591.261358                          46.0   \n",
       "\n",
       "   Short_Titanic_ET_Blink_Rate  Short_Titanic_ET_FixationDispersion_Mean  \\\n",
       "0                          NaN                                       NaN   \n",
       "1                          NaN                                       NaN   \n",
       "2                    46.098343                                  0.261135   \n",
       "\n",
       "   Short_Titanic_ET_Fixation_Count  Short_Titanic_ET_Fixation_PerMinute  \\\n",
       "0                              NaN                                  NaN   \n",
       "1                              NaN                                  NaN   \n",
       "2                            156.0                           156.333511   \n",
       "\n",
       "   Short_Titanic_ET_FixationDuration_Mean  \n",
       "0                                     NaN  \n",
       "1                                     NaN  \n",
       "2                              459.838233  \n",
       "\n",
       "[3 rows x 943 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Compute pilot sensor features for selected respondents\n",
    "sensor_file_index = {\n",
    "    path.name: path\n",
    "    for path in (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    "}\n",
    "\n",
    "pilot_ids = [\"2\", \"58\", \"116\"]\n",
    "pilot_subset = uv_stage1.loc[uv_stage1[\"respondent\"].astype(str).isin(pilot_ids)].copy()\n",
    "pilot_subset[\"respondent_numeric\"] = pd.to_numeric(pilot_subset[\"respondent\"], errors=\"coerce\")\n",
    "pilot_subset = pilot_subset.sort_values([\"respondent_numeric\", \"respondent\"])\n",
    "\n",
    "pilot_feature_rows = []\n",
    "pilot_issue_log = []\n",
    "\n",
    "for _, row in pilot_subset.iterrows():\n",
    "    respondent_id = str(row[\"respondent\"]).strip()\n",
    "    group_letter = str(row.get(\"group\", \"\")).strip().upper() if pd.notna(row.get(\"group\")) else None\n",
    "    source_file = row.get(\"source_file\")\n",
    "    if not source_file or source_file not in sensor_file_index:\n",
    "        pilot_issue_log.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"stimulus\": None,\n",
    "            \"issue\": \"Sensor export not located.\",\n",
    "        })\n",
    "        continue\n",
    "    df_sensor, _ = read_imotions(sensor_file_index[source_file])\n",
    "    feature_row: Dict[str, float] = {\"respondent\": respondent_id}\n",
    "    try:\n",
    "        if df_sensor.empty or \"SourceStimuliName\" not in df_sensor.columns:\n",
    "            pilot_issue_log.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"stimulus\": None,\n",
    "                \"issue\": \"Sensor export missing SourceStimuliName column.\",\n",
    "            })\n",
    "            continue\n",
    "        # Assess sensor coverage for this respondent\n",
    "        for sensor_label, required_columns in sensor_required_columns.items():\n",
    "            if sensor_label == \"EEG\":\n",
    "                missing_metrics = []\n",
    "                for metric in required_columns:\n",
    "                    candidates = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "                    if not any(column in df_sensor.columns for column in candidates):\n",
    "                        missing_metrics.append(metric)\n",
    "                feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_metrics))\n",
    "                if missing_metrics:\n",
    "                    metrics_display = \", \".join(missing_metrics)\n",
    "                    pilot_issue_log.append({\n",
    "                        \"respondent\": respondent_id,\n",
    "                        \"stimulus\": None,\n",
    "                        \"issue\": f\"Missing EEG columns: {metrics_display}.\",\n",
    "                    })\n",
    "                continue\n",
    "            missing_columns = [col for col in required_columns if col not in df_sensor.columns]\n",
    "            feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_columns))\n",
    "            if missing_columns:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": None,\n",
    "                    \"issue\": f\"Missing {sensor_label} columns: {', '.join(missing_columns)}.\",\n",
    "                })\n",
    "        unique_stimuli = sorted({str(s).strip() for s in df_sensor[\"SourceStimuliName\"].dropna().unique()})\n",
    "        for raw_stimulus in unique_stimuli:\n",
    "            lookup_key = (group_letter, raw_stimulus)\n",
    "            if lookup_key not in stimulus_map_lookup.index:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"Stimulus missing from rename map.\",\n",
    "                })\n",
    "                continue\n",
    "            map_row = stimulus_map_lookup.loc[lookup_key]\n",
    "            if isinstance(map_row, pd.DataFrame):\n",
    "                map_row = map_row.iloc[0]\n",
    "            title = map_row[\"title\"]\n",
    "            form = map_row[\"form\"]\n",
    "            if form == \"Long\" and (key_moment_lookup.get(title) is None or key_duration_lookup.get(title) is None):\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"Key moment timing not defined for long-form title.\",\n",
    "                })\n",
    "                continue\n",
    "            segment = prepare_stimulus_segment(df_sensor, raw_stimulus, form, title)\n",
    "            if segment.empty:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"No data after windowing (check key moment timings).\",\n",
    "                })\n",
    "                del segment\n",
    "                continue\n",
    "            try:\n",
    "                features = compute_sensor_features(segment, form, title)\n",
    "                if not features:\n",
    "                    pilot_issue_log.append({\n",
    "                        \"respondent\": respondent_id,\n",
    "                        \"stimulus\": raw_stimulus,\n",
    "                        \"issue\": \"No features computed for segment.\",\n",
    "                    })\n",
    "                    continue\n",
    "                feature_row.update(features)\n",
    "            finally:\n",
    "                del segment\n",
    "        pilot_feature_rows.append(feature_row)\n",
    "    finally:\n",
    "        del df_sensor\n",
    "        gc.collect()\n",
    "\n",
    "pilot_features = pd.DataFrame(pilot_feature_rows)\n",
    "pilot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "effd718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pilot features back into the UV and review any issues\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "if not pilot_features.empty:\n",
    "    pilot_features[\"respondent\"] = pilot_features[\"respondent\"].astype(str)\n",
    "pilot_uv = (\n",
    "    uv_stage1.loc[uv_stage1[\"respondent\"].isin(pilot_ids)]\n",
    "    .copy()\n",
    "    .merge(pilot_features, on=\"respondent\", how=\"left\")\n",
    ")\n",
    "output_path = project_root / \"results\" / \"uv_pilot_features.csv\"\n",
    "try:\n",
    "    pilot_uv.to_csv(output_path, index=False)\n",
    "except PermissionError:\n",
    "    fallback_path = output_path.with_name(\n",
    "        f\"{output_path.stem}_{pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')}.csv\"\n",
    ")\n",
    "    pilot_uv.to_csv(fallback_path, index=False)\n",
    "    print(\n",
    "        f\"Permission denied for {output_path}. Saved pilot UV to {fallback_path.name} instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "056f38e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>Missing EEG columns: High Engagement, Low Enga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent stimulus                                              issue\n",
       "0          2     None  Missing EEG columns: High Engagement, Low Enga..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = pd.DataFrame(pilot_issue_log)\n",
    "issues_df.sort_values([\"respondent\", \"stimulus\"], na_position=\"last\") if not issues_df.empty else \"No issues logged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4ee526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>title</th>\n",
       "      <th>observed_seconds</th>\n",
       "      <th>expected_seconds</th>\n",
       "      <th>diff_seconds</th>\n",
       "      <th>within_tolerance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The Town</td>\n",
       "      <td>262.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>The Town</td>\n",
       "      <td>262.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent     title  observed_seconds  expected_seconds  diff_seconds  \\\n",
       "0          2  The Town             262.0             262.0          0.01   \n",
       "1         58   Mad Max             225.0             225.0          0.00   \n",
       "2        116  The Town             262.0             262.0          0.00   \n",
       "\n",
       "   within_tolerance  \n",
       "0              True  \n",
       "1              True  \n",
       "2              True  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate long-form durations against key-moment specifications\n",
    "tolerance_seconds = 3\n",
    "duration_columns = [col for col in pilot_features.columns if col.endswith(\"_duration\")]\n",
    "validation_records = []\n",
    "for _, feat_row in pilot_features.iterrows():\n",
    "    respondent_id = feat_row.get(\"respondent\")\n",
    "    for col in duration_columns:\n",
    "        value = feat_row.get(col)\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        form = col.split('_', 1)[0]\n",
    "        if form != \"Long\":\n",
    "            continue\n",
    "        title = col[len(form) + 1: -len(\"_duration\")]\n",
    "        expected_ms = key_duration_lookup.get(title)\n",
    "        if expected_ms is None:\n",
    "            continue\n",
    "        observed_ms = float(value) * 1000.0\n",
    "        diff_seconds = abs(observed_ms - expected_ms) / 1000.0\n",
    "        validation_records.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"title\": title,\n",
    "            \"observed_seconds\": round(observed_ms / 1000.0, 2),\n",
    "            \"expected_seconds\": round(expected_ms / 1000.0, 2),\n",
    "            \"diff_seconds\": round(diff_seconds, 2),\n",
    "            \"within_tolerance\": diff_seconds <= tolerance_seconds,\n",
    "        })\n",
    "duration_validation = pd.DataFrame(validation_records)\n",
    "duration_validation if not duration_validation.empty else \"No long-form durations to validate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "457f5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sensor_feature_pipeline(respondent_ids=None, export_label=\"uv_stage2_full\", save_outputs=True):\n",
    "    \"\"\"Compute sensor features for the specified respondents and optionally persist outputs.\"\"\"\n",
    "    sensor_file_index = {\n",
    "        path.name: path\n",
    "        for path in (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    "    }\n",
    "    if respondent_ids is None:\n",
    "        target_ids = sorted({str(r).strip() for r in uv_stage1[\"respondent\"].astype(str)})\n",
    "    else:\n",
    "        cleaned_ids = [str(r).strip() for r in respondent_ids if pd.notna(r)]\n",
    "        target_ids = sorted(set(cleaned_ids))\n",
    "    if not target_ids:\n",
    "        raise ValueError(\"No respondents provided for sensor feature processing.\")\n",
    "    subset = uv_stage1.loc[uv_stage1[\"respondent\"].astype(str).isin(target_ids)].copy()\n",
    "    if subset.empty:\n",
    "        raise ValueError(\"No matching respondents found in uv_stage1 for the requested IDs.\")\n",
    "    subset[\"respondent_numeric\"] = pd.to_numeric(subset[\"respondent\"], errors=\"coerce\")\n",
    "    subset = subset.sort_values([\"respondent_numeric\", \"respondent\"])\n",
    "    feature_rows = []\n",
    "    issue_rows = []\n",
    "\n",
    "    def log_issue(respondent_id, stimulus, message):\n",
    "        issue_rows.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"stimulus\": stimulus,\n",
    "            \"issue\": message,\n",
    "        })\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        respondent_id = str(row[\"respondent\"]).strip()\n",
    "        group_letter = str(row.get(\"group\", \"\")).strip().upper() if pd.notna(row.get(\"group\")) else None\n",
    "        source_file = row.get(\"source_file\")\n",
    "        if not source_file or source_file not in sensor_file_index:\n",
    "            log_issue(respondent_id, None, \"Sensor export not located.\")\n",
    "            continue\n",
    "        df_sensor, _ = read_imotions(sensor_file_index[source_file])\n",
    "        feature_row: Dict[str, float] = {\"respondent\": respondent_id}\n",
    "        try:\n",
    "            if df_sensor.empty or \"SourceStimuliName\" not in df_sensor.columns:\n",
    "                log_issue(respondent_id, None, \"Sensor export missing SourceStimuliName column.\")\n",
    "                continue\n",
    "            for sensor_label, required_columns in sensor_required_columns.items():\n",
    "                if sensor_label == \"EEG\":\n",
    "                    missing_metrics = []\n",
    "                    for metric in required_columns:\n",
    "                        candidates = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "                        if not any(column in df_sensor.columns for column in candidates):\n",
    "                            missing_metrics.append(metric)\n",
    "                    feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_metrics))\n",
    "                    if missing_metrics:\n",
    "                        log_issue(respondent_id, None, f\"Missing EEG columns: {', '.join(missing_metrics)}.\")\n",
    "                    continue\n",
    "                missing_columns = [col for col in required_columns if col not in df_sensor.columns]\n",
    "                feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_columns))\n",
    "                if missing_columns:\n",
    "                    log_issue(respondent_id, None, f\"Missing {sensor_label} columns: {', '.join(missing_columns)}.\")\n",
    "            unique_stimuli = sorted({str(s).strip() for s in df_sensor[\"SourceStimuliName\"].dropna().unique()})\n",
    "            for raw_stimulus in unique_stimuli:\n",
    "                lookup_key = (group_letter, raw_stimulus)\n",
    "                if lookup_key not in stimulus_map_lookup.index:\n",
    "                    log_issue(respondent_id, raw_stimulus, \"Stimulus missing from rename map.\")\n",
    "                    continue\n",
    "                map_row = stimulus_map_lookup.loc[lookup_key]\n",
    "                if isinstance(map_row, pd.DataFrame):\n",
    "                    map_row = map_row.iloc[0]\n",
    "                title = map_row[\"title\"]\n",
    "                form = map_row[\"form\"]\n",
    "                if form == \"Long\" and (key_moment_lookup.get(title) is None or key_duration_lookup.get(title) is None):\n",
    "                    log_issue(respondent_id, raw_stimulus, \"Key moment timing not defined for long-form title.\")\n",
    "                    continue\n",
    "                segment = prepare_stimulus_segment(df_sensor, raw_stimulus, form, title)\n",
    "                if segment.empty:\n",
    "                    log_issue(respondent_id, raw_stimulus, \"No data after windowing (check key moment timings).\")\n",
    "                    del segment\n",
    "                    continue\n",
    "                try:\n",
    "                    features = compute_sensor_features(segment, form, title)\n",
    "                    if not features:\n",
    "                        log_issue(respondent_id, raw_stimulus, \"No features computed for segment.\")\n",
    "                        continue\n",
    "                    feature_row.update(features)\n",
    "                finally:\n",
    "                    del segment\n",
    "            feature_rows.append(feature_row)\n",
    "        finally:\n",
    "            del df_sensor\n",
    "            gc.collect()\n",
    "\n",
    "    features_df = pd.DataFrame(feature_rows)\n",
    "    issues_df = pd.DataFrame(issue_rows)\n",
    "    merged_uv = (\n",
    "        uv_stage1.loc[uv_stage1[\"respondent\"].astype(str).isin(target_ids)]\n",
    "        .copy()\n",
    "    )\n",
    "    merged_uv[\"respondent\"] = merged_uv[\"respondent\"].astype(str)\n",
    "    if not features_df.empty:\n",
    "        features_df[\"respondent\"] = features_df[\"respondent\"].astype(str)\n",
    "        merged_uv = merged_uv.merge(features_df, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    def safe_to_csv(df, path):\n",
    "        if df is None:\n",
    "            return\n",
    "        try:\n",
    "            df.to_csv(path, index=False)\n",
    "        except PermissionError:\n",
    "            fallback_path = path.with_name(\n",
    "                f\"{path.stem}_{pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')}.csv\"\n",
    "            )\n",
    "            df.to_csv(fallback_path, index=False)\n",
    "            print(f\"Permission denied for {path}. Saved to {fallback_path.name} instead.\")\n",
    "\n",
    "    if save_outputs:\n",
    "        results_dir = project_root / \"results\"\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        safe_to_csv(features_df, results_dir / f\"{export_label}_features.csv\")\n",
    "        safe_to_csv(merged_uv, results_dir / f\"{export_label}_uv.csv\")\n",
    "        if not issues_df.empty:\n",
    "            issues_sorted = issues_df.sort_values([\"respondent\", \"stimulus\"], na_position=\"last\")\n",
    "            safe_to_csv(issues_sorted, results_dir / f\"{export_label}_issues.csv\")\n",
    "    gc.collect()\n",
    "    return features_df, issues_df, merged_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2884ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_46356\\2272591584.py:44: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=count, low_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features for 83 respondents; 38 issues logged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83, 1076)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Stage 2 sensor feature extraction for the full respondent list\n",
    "full_features, full_issues, full_uv = run_sensor_feature_pipeline()\n",
    "print(f\"Computed features for {len(full_features)} respondents; {len(full_issues)} issues logged.\")\n",
    "full_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe8719",
   "metadata": {},
   "source": [
    "## Stage 3: Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b2525",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Stage 3 integrates the post-exposure survey responses into the unified view so downstream modeling can pair perceptual metrics with the Stage 1 demographics and Stage 2 sensor features. The pipeline now also ingests the screening familiarity composites, aligning them to the survey schema before export. This section documents the ingestion logic and feature engineering steps applied to the raw TSV exports and the supplemental screening file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd25fa2",
   "metadata": {},
   "source": [
    "### Workflow Summary\n",
    "1. Load the survey question catalog and rename map to build a metadata lookup for scoring rules.\n",
    "2. Read every group-level `MERGED_SURVEY_RESPONSE_MATRIX` export, standardize respondent identifiers, and apply group-specific column renames.\n",
    "3. Convert Likert-style answers to numeric scales (including familiarity and recency variants), compute enjoyment/familiarity composites, and split open-ended responses for archival review.\n",
    "4. Merge the engineered survey metrics back into the unified view.\n",
    "5. Append screening familiarity composites from `results/individual_composite_scores.csv`, canonicalizing title strings, inferring `Long`/`Short` form per group, and emitting columns named `{form}_{title}_Screening_Familiarity_{question_code}`.\n",
    "6. Write the Stage 3 feature, open-ended, and UV exports with timestamped fallbacks when the base filenames are locked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c8c179d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 14)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"uv_stage1\" not in globals():\n",
    "    uv_stage1 = pd.read_csv(project_root / \"results\" / \"uv_stage1_demographics.csv\")\n",
    "    uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "if \"full_uv\" not in globals() and (project_root / \"results\" / \"uv_stage2_full_uv.csv\").exists():\n",
    "    full_uv = pd.read_csv(project_root / \"results\" / \"uv_stage2_full_uv.csv\")\n",
    "    full_uv[\"respondent\"] = full_uv[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "uv_stage1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1dbea40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "survey_rename_map = pd.read_csv(project_root / \"data\" / \"survey_column_rename_stage3.csv\")\n",
    "survey_questions = pd.read_csv(project_root / \"data\" / \"survey_questions.csv\")\n",
    "\n",
    "survey_questions[\"question_code\"] = survey_questions[\"question_code\"].astype(str).str.strip()\n",
    "survey_questions[\"question_type\"] = survey_questions[\"question_type\"].str.lower()\n",
    "survey_questions[\"subscale\"] = survey_questions[\"subscale\"].fillna(\"\")\n",
    "survey_questions[\"polarity\"] = survey_questions[\"polarity\"].fillna(\"\")\n",
    "\n",
    "survey_metadata = (\n",
    "    survey_rename_map\n",
    "    .merge(survey_questions, on=\"question_code\", how=\"left\", suffixes=(\"\", \"_details\"))\n",
    ")\n",
    "\n",
    "survey_metadata[\"question_type\"] = survey_metadata[\"question_type\"].fillna(\"likert\")\n",
    "survey_metadata[\"subscale\"] = survey_metadata[\"subscale\"].fillna(\"\")\n",
    "survey_metadata[\"polarity\"] = survey_metadata[\"polarity\"].fillna(\"\")\n",
    "survey_metadata_lookup = (\n",
    "    survey_metadata\n",
    "    .drop_duplicates(subset=[\"target_column\"])\n",
    "    .set_index(\"target_column\")\n",
    ")\n",
    "\n",
    "survey_files = sorted(\n",
    "    (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Survey/MERGED_SURVEY_RESPONSE_MATRIX-*.txt\")\n",
    ")\n",
    "\n",
    "if not survey_files:\n",
    "    raise FileNotFoundError(\"No survey response text files detected under data/Export/*/Survey/.\")\n",
    "\n",
    "len(survey_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "04435f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIKERT_PATTERN = re.compile(r\"^\\s*(\\d+)(?:\\.\\d+)?\")\n",
    "\n",
    "LIKERT_KEYWORDS = [\n",
    "    (\"strongly disagree\", 1.0),\n",
    "    (\"disagree\", 2.0),\n",
    "    (\"neither agree nor disagree\", 3.0),\n",
    "    (\"strongly agree\", 5.0),\n",
    "    (\"agree\", 4.0),\n",
    "]\n",
    "\n",
    "FAMILIARITY_KEY_PATTERNS = [\n",
    "    (0.0, (\"never heard\", \"not familiar\")),\n",
    "    (1.0, (\"heard of it, but never watched\", \"heard of it only\")),\n",
    "    (2.0, (\"seen a clip\", \"seen clips\", \"seen part\")),\n",
    "    (3.0, (\"watched it in full\", \"just once\")),\n",
    "    (4.0, (\"watched multiple\", \"very familiar\")),\n",
    "]\n",
    "\n",
    "LASTWATCHED_KEY_PATTERNS = [\n",
    "    (4.0, (\"past week\",)),\n",
    "    (3.0, (\"past month\", \"past 6 months\", \"past six months\")),\n",
    "    (2.0, (\"past 3 months\", \"past three months\")),\n",
    "    (1.0, (\"more than 3 months\", \"over 3 months\")),\n",
    "    (0.0, (\"more than 6 months\", \"don't remember\", \"never watched this movie in full\")),\n",
    "]\n",
    "\n",
    "\n",
    "def _clean_response(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if not text or text.upper() == \"EMPTY FIELD\":\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "\n",
    "def _parse_likert_value(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    lowered = text.lower()\n",
    "    for keyword, score in LIKERT_KEYWORDS:\n",
    "        if keyword in lowered:\n",
    "            return score\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _score_familiarity(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        numeric = float(match.group(1))\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    lowered = text.lower()\n",
    "    for score, patterns in FAMILIARITY_KEY_PATTERNS:\n",
    "        if any(pattern in lowered for pattern in patterns):\n",
    "            return score\n",
    "    try:\n",
    "        numeric = float(text)\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _score_last_watched(value):\n",
    "    text = _clean_response(value)\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    match = LIKERT_PATTERN.match(text)\n",
    "    if match:\n",
    "        numeric = float(match.group(1))\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    lowered = text.lower()\n",
    "    for score, patterns in LASTWATCHED_KEY_PATTERNS:\n",
    "        if any(pattern in lowered for pattern in patterns):\n",
    "            return score\n",
    "    try:\n",
    "        numeric = float(text)\n",
    "        return float(np.clip(numeric - 1.0, 0.0, 4.0))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _reverse_likert(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    return 6.0 - float(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7119b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\1871314504.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"EMPTY FIELD\": np.nan})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>11</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>30</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "81         11      Default      Group F        FEMALE          30   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "81  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E12 Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "1                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "2                           4 = Agree                          4 = Agree   \n",
       "3                           4 = Agree                 5 = Strongly Agree   \n",
       "4                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "..                                ...                                ...   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "81                                NaN                                NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                  5 = Strongly Agree  ...   \n",
       "1                  5 = Strongly Agree  ...   \n",
       "2                           4 = Agree  ...   \n",
       "3                  5 = Strongly Agree  ...   \n",
       "4                  5 = Strongly Agree  ...   \n",
       "..                                ...  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "81                                NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "81                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "81                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "81                                          NaN  \n",
       "\n",
       "[82 rows x 120 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _extract_group_letter(path: Path) -> str:\n",
    "    for part in path.parts:\n",
    "        if part.startswith(\"Group \") and \"-\" not in part:\n",
    "            return part.split()[-1].strip().upper()\n",
    "    raise ValueError(f\"Unable to determine group letter from path: {path}\")\n",
    "\n",
    "\n",
    "def _rename_survey_columns(df: pd.DataFrame, group_letter: str) -> pd.DataFrame:\n",
    "    rename_subset = survey_metadata.loc[survey_metadata[\"group\"] == group_letter]\n",
    "    rename_dict = {\n",
    "        raw: target\n",
    "        for raw, target in zip(rename_subset[\"raw_column\"], rename_subset[\"target_column\"])\n",
    "        if raw in df.columns\n",
    "    }\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    columns_to_keep = [\n",
    "        \"respondent\",\n",
    "        \"survey_group\",\n",
    "        \"survey_study\",\n",
    "        \"survey_gender\",\n",
    "        \"survey_age\",\n",
    "        \"survey_file\",\n",
    "        *sorted(rename_dict.values()),\n",
    "    ]\n",
    "    existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "    return df.loc[:, existing_columns]\n",
    "\n",
    "\n",
    "def _load_survey_file(path: Path) -> pd.DataFrame:\n",
    "    # Some open-ended answers contain newline characters and stray quotes; use python engine with minimal parsing assumptions.\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        dtype=str,\n",
    "        engine=\"python\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        encoding=\"utf-8\",\n",
    "        on_bad_lines=\"warn\",\n",
    "    )\n",
    "\n",
    "\n",
    "survey_frames = []\n",
    "\n",
    "for survey_path in survey_files:\n",
    "    group_letter = _extract_group_letter(survey_path)\n",
    "    df = _load_survey_file(survey_path)\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    df = df.replace({\"EMPTY FIELD\": np.nan})\n",
    "    df[\"RESPONDENT\"] = df[\"RESPONDENT\"].astype(str).str.strip()\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"RESPONDENT\": \"respondent\",\n",
    "            \"GROUP\": \"survey_group\",\n",
    "            \"STUDY\": \"survey_study\",\n",
    "            \"GENDER\": \"survey_gender\",\n",
    "            \"AGE\": \"survey_age\",\n",
    "        }\n",
    "    )\n",
    "    if \"survey_group\" not in df.columns:\n",
    "        df[\"survey_group\"] = group_letter\n",
    "    df[\"survey_group\"] = df[\"survey_group\"].fillna(group_letter).astype(str).str.strip()\n",
    "    if \"survey_study\" in df.columns:\n",
    "        df[\"survey_study\"] = df[\"survey_study\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_study\"] = np.nan\n",
    "    if \"survey_gender\" in df.columns:\n",
    "        df[\"survey_gender\"] = df[\"survey_gender\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"survey_gender\"] = np.nan\n",
    "    df[\"survey_file\"] = survey_path.name\n",
    "    if \"survey_age\" in df.columns:\n",
    "        df[\"survey_age\"] = pd.to_numeric(df[\"survey_age\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"survey_age\"] = np.nan\n",
    "    df = _rename_survey_columns(df, group_letter)\n",
    "    survey_frames.append(df)\n",
    "\n",
    "survey_responses = pd.concat(survey_frames, ignore_index=True)\n",
    "survey_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e7d5a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: duplicate survey rows detected for respondents: ['11']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>4 = Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>5 = Strongly Agree</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>107</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "76        107      Default      Group F          MALE          33   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "\n",
       "                             survey_file Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                         4 = Agree   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                5 = Strongly Agree   \n",
       "..                                   ...                               ...   \n",
       "76  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                               NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E12 Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "1                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "2                           4 = Agree                          4 = Agree   \n",
       "3                           4 = Agree                 5 = Strongly Agree   \n",
       "4                  5 = Strongly Agree                 5 = Strongly Agree   \n",
       "..                                ...                                ...   \n",
       "76                                NaN                                NaN   \n",
       "77                                NaN                                NaN   \n",
       "78                                NaN                                NaN   \n",
       "79                                NaN                                NaN   \n",
       "80                                NaN                                NaN   \n",
       "\n",
       "   Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                  5 = Strongly Agree  ...   \n",
       "1                  5 = Strongly Agree  ...   \n",
       "2                           4 = Agree  ...   \n",
       "3                  5 = Strongly Agree  ...   \n",
       "4                  5 = Strongly Agree  ...   \n",
       "..                                ...  ...   \n",
       "76                                NaN  ...   \n",
       "77                                NaN  ...   \n",
       "78                                NaN  ...   \n",
       "79                                NaN  ...   \n",
       "80                                NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "76                                        NaN   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "76                                        NaN   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "76                                          NaN   \n",
       "77                                          NaN   \n",
       "78                                          NaN   \n",
       "79                                          NaN   \n",
       "80                                          NaN   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                           NaN  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4                                           NaN  \n",
       "..                                          ...  \n",
       "76                                          NaN  \n",
       "77                                          NaN  \n",
       "78                                          NaN  \n",
       "79                                          NaN  \n",
       "80                                          NaN  \n",
       "\n",
       "[81 rows x 120 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_responses[\"respondent\"] = survey_responses[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "duplicate_ids = sorted(survey_responses.loc[survey_responses[\"respondent\"].duplicated(), \"respondent\"].unique())\n",
    "if duplicate_ids:\n",
    "    print(f\"Warning: duplicate survey rows detected for respondents: {duplicate_ids}\")\n",
    "\n",
    "survey_numeric = survey_responses.drop_duplicates(subset=[\"respondent\"], keep=\"first\").copy()\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d2e556ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E6</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_E8</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD3</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD4</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Enjoyment_WBD5</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>107</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "76        107      Default      Group F          MALE          33   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "76  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "76                                 NaN                                 NaN   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "76                                 NaN  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "\n",
       "   Short_Abbot Elementary_Survey_Enjoyment_E6  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "76                                        NaN   \n",
       "77                                        NaN   \n",
       "78                                        NaN   \n",
       "79                                        NaN   \n",
       "80                                        NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_E8  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "..                                         ...   \n",
       "76                                         NaN   \n",
       "77                                         NaN   \n",
       "78                                         NaN   \n",
       "79                                         NaN   \n",
       "80                                         NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD3  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD4  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Enjoyment_WBD5  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F1  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F2  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "76                                           NaN   \n",
       "77                                           NaN   \n",
       "78                                           NaN   \n",
       "79                                           NaN   \n",
       "80                                           NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_F3  \n",
       "0                                            NaN  \n",
       "1                                            NaN  \n",
       "2                                            NaN  \n",
       "3                                            NaN  \n",
       "4                                            NaN  \n",
       "..                                           ...  \n",
       "76                                           NaN  \n",
       "77                                           NaN  \n",
       "78                                           NaN  \n",
       "79                                           NaN  \n",
       "80                                           NaN  \n",
       "\n",
       "[81 rows x 120 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "\n",
    "    question_code = (meta.get(\"question_code\") or \"\").strip().upper()\n",
    "    question_type = (meta.get(\"question_type\") or \"\").strip().lower()\n",
    "    polarity = (meta.get(\"polarity\") or \"\").strip().lower()\n",
    "\n",
    "    if question_code in {\"F1\", \"F3\"} or column.endswith(\"_Survey_Familiarity_F1\") or column.endswith(\"_Survey_Familiarity_F3\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_score_familiarity)\n",
    "    elif question_code == \"F2\" or column.endswith(\"_Survey_Familiarity_F2\"):\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_score_last_watched)\n",
    "    elif question_type == \"likert\":\n",
    "        survey_numeric[column] = survey_numeric[column].apply(_parse_likert_value)\n",
    "        if polarity == \"negative\":\n",
    "            survey_numeric[column] = survey_numeric[column].apply(_reverse_likert)\n",
    "\n",
    "def _clip_zero_to_four(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    try:\n",
    "        numeric = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return np.nan\n",
    "    return float(np.clip(numeric, 0.0, 4.0))\n",
    "\n",
    "familiarity_columns = [\n",
    "    column\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F2\")\n",
    "    or column.endswith(\"_Survey_Familiarity_F3\")\n",
    "]\n",
    "\n",
    "for column in familiarity_columns:\n",
    "    survey_numeric[column] = survey_numeric[column].apply(_clip_zero_to_four)\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6336772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[sum_col] = raw_sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[corrected_col] = corrected_sum\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[mean_col] = corrected_mean\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = raw_normalized\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\3342736310.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_corrected_col] = corrected_normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>107</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "76        107      Default      Group F          MALE          33   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "76  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "76                                 NaN                                 NaN   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "76                                 NaN  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "\n",
       "   Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN          \n",
       "1                                                 NaN          \n",
       "2                                                 NaN          \n",
       "3                                                 NaN          \n",
       "4                                                 NaN          \n",
       "..                                                ...          \n",
       "76                                               43.0          \n",
       "77                                               43.0          \n",
       "78                                               59.0          \n",
       "79                                               53.0          \n",
       "80                                               47.0          \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "76                                           3.583333      \n",
       "77                                           3.583333      \n",
       "78                                           4.916667      \n",
       "79                                           4.416667      \n",
       "80                                           3.916667      \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "76                                           0.645833            \n",
       "77                                           0.687500            \n",
       "78                                           0.895833            \n",
       "79                                           0.770833            \n",
       "80                                           0.645833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "76                                           0.645833                     \n",
       "77                                           0.645833                     \n",
       "78                                           0.979167                     \n",
       "79                                           0.854167                     \n",
       "80                                           0.729167                     \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "0                                                   0        \n",
       "1                                                   0        \n",
       "2                                                   0        \n",
       "3                                                   0        \n",
       "4                                                   0        \n",
       "..                                                ...        \n",
       "76                                                  0        \n",
       "77                                                  0        \n",
       "78                                                  0        \n",
       "79                                                  0        \n",
       "80                                                  0        \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "..                                                ...      \n",
       "76                                                NaN      \n",
       "77                                                NaN      \n",
       "78                                                NaN      \n",
       "79                                                NaN      \n",
       "80                                                NaN      \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "0                                                 NaN            \n",
       "1                                                 NaN            \n",
       "2                                                 NaN            \n",
       "3                                                 NaN            \n",
       "4                                                 NaN            \n",
       "..                                                ...            \n",
       "76                                                NaN            \n",
       "77                                                NaN            \n",
       "78                                                NaN            \n",
       "79                                                NaN            \n",
       "80                                                NaN            \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "0                                                 NaN       \n",
       "1                                                 NaN       \n",
       "2                                                 NaN       \n",
       "3                                                 NaN       \n",
       "4                                                 NaN       \n",
       "..                                                ...       \n",
       "76                                                NaN       \n",
       "77                                                NaN       \n",
       "78                                                NaN       \n",
       "79                                                NaN       \n",
       "80                                                NaN       \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "0                                                 NaN             \n",
       "1                                                 NaN             \n",
       "2                                                 NaN             \n",
       "3                                                 NaN             \n",
       "4                                                 NaN             \n",
       "..                                                ...             \n",
       "76                                                NaN             \n",
       "77                                                NaN             \n",
       "78                                                NaN             \n",
       "79                                                NaN             \n",
       "80                                                NaN             \n",
       "\n",
       "    Short_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \n",
       "0                                                 NaN                     \n",
       "1                                                 NaN                     \n",
       "2                                                 NaN                     \n",
       "3                                                 NaN                     \n",
       "4                                                 NaN                     \n",
       "..                                                ...                     \n",
       "76                                                NaN                     \n",
       "77                                                NaN                     \n",
       "78                                                NaN                     \n",
       "79                                                NaN                     \n",
       "80                                                NaN                     \n",
       "\n",
       "[81 rows x 336 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscale_columns = defaultdict(list)\n",
    "overall_enjoyment_columns = defaultdict(list)\n",
    "\n",
    "for column, meta in survey_metadata_lookup.iterrows():\n",
    "    if column not in survey_numeric.columns:\n",
    "        continue\n",
    "    if meta.get(\"topic\") != \"enjoyment\":\n",
    "        continue\n",
    "    if (meta.get(\"question_type\") or \"\").strip().lower() != \"likert\":\n",
    "        continue\n",
    "    prefix = column.split(\"_Survey_\")[0]\n",
    "    subscale = (meta.get(\"subscale\") or \"\").strip()\n",
    "    overall_enjoyment_columns[prefix].append(column)\n",
    "    if subscale:\n",
    "        subscale_columns[(prefix, subscale)].append(column)\n",
    "\n",
    "for (prefix, subscale), cols in subscale_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_{subscale}_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_{subscale}_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_{subscale}_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_{subscale}_Normalized\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "    sum_values = values.sum(axis=1, min_count=1)\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = sum_values\n",
    "    survey_numeric[mean_col] = np.where(count_values > 0, sum_values / count_values, np.nan)\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "for prefix, cols in overall_enjoyment_columns.items():\n",
    "    values = survey_numeric[cols]\n",
    "    sum_col = f\"{prefix}_Survey_EnjoymentComposite_Sum\"\n",
    "    count_col = f\"{prefix}_Survey_EnjoymentComposite_Count\"\n",
    "    mean_col = f\"{prefix}_Survey_EnjoymentComposite_Mean\"\n",
    "    norm_col = f\"{prefix}_Survey_EnjoymentComposite_Normalized\"\n",
    "    norm_corrected_col = f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\"\n",
    "    corrected_col = f\"{prefix}_Survey_EnjoymentComposite_Corrected\"\n",
    "    count_values = values.notna().sum(axis=1)\n",
    "\n",
    "    raw_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "    corrected_components = pd.DataFrame(index=values.index, dtype=float)\n",
    "\n",
    "    for column in cols:\n",
    "        polarity_meta = \"\"\n",
    "        if column in survey_metadata_lookup.index:\n",
    "            polarity_meta = (survey_metadata_lookup.loc[column].get(\"polarity\") or \"\").strip().lower()\n",
    "        if column in survey_responses.columns:\n",
    "            raw_series = survey_responses.loc[values.index, column]\n",
    "            parsed_series = raw_series.apply(_parse_likert_value)\n",
    "        else:\n",
    "            fallback_series = pd.to_numeric(survey_numeric.loc[values.index, column], errors=\"coerce\")\n",
    "            if polarity_meta == \"negative\":\n",
    "                parsed_series = fallback_series.apply(_reverse_likert)\n",
    "            else:\n",
    "                parsed_series = fallback_series\n",
    "        raw_components[column] = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        corrected_series = pd.to_numeric(parsed_series, errors=\"coerce\")\n",
    "        if polarity_meta == \"negative\":\n",
    "            corrected_series = corrected_series.apply(_reverse_likert)\n",
    "        corrected_components[column] = corrected_series\n",
    "\n",
    "    raw_sum_values = raw_components.sum(axis=1, min_count=1)\n",
    "    corrected_sum = corrected_components.sum(axis=1, min_count=1)\n",
    "    raw_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((raw_sum_values - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_normalized = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip((corrected_sum - count_values) / (4.0 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    corrected_mean = np.where(count_values > 0, corrected_sum / count_values, np.nan)\n",
    "\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[sum_col] = raw_sum_values\n",
    "    survey_numeric[corrected_col] = corrected_sum\n",
    "    survey_numeric[mean_col] = corrected_mean\n",
    "    survey_numeric[norm_col] = raw_normalized\n",
    "    survey_numeric[norm_corrected_col] = corrected_normalized\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c762d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[c1_col] = sum_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[count_col] = count_values\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_27212\\4132188637.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  survey_numeric[norm_col] = np.where(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Abbot Elementary_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_Mad Max_Survey_Familiarity_C1_Normalized</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Count</th>\n",
       "      <th>Short_The Town_Survey_Familiarity_C1_Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>107</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "76        107      Default      Group F          MALE          33   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "76  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "76                                 NaN                                 NaN   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "76                                 NaN  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "\n",
       "   Long_The Town_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                           0.000   \n",
       "1                                           0.750   \n",
       "2                                           0.000   \n",
       "3                                           0.000   \n",
       "4                                           0.125   \n",
       "..                                            ...   \n",
       "76                                            NaN   \n",
       "77                                            NaN   \n",
       "78                                            NaN   \n",
       "79                                            NaN   \n",
       "80                                            NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "..                                           ...   \n",
       "76                                           0.0   \n",
       "77                                           0.0   \n",
       "78                                           0.0   \n",
       "79                                           0.0   \n",
       "80                                           0.0   \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Count  \\\n",
       "0                                                   0    \n",
       "1                                                   0    \n",
       "2                                                   0    \n",
       "3                                                   0    \n",
       "4                                                   0    \n",
       "..                                                ...    \n",
       "76                                                  0    \n",
       "77                                                  0    \n",
       "78                                                  0    \n",
       "79                                                  0    \n",
       "80                                                  0    \n",
       "\n",
       "    Short_Abbot Elementary_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                                 NaN         \n",
       "1                                                 NaN         \n",
       "2                                                 NaN         \n",
       "3                                                 NaN         \n",
       "4                                                 NaN         \n",
       "..                                                ...         \n",
       "76                                                NaN         \n",
       "77                                                NaN         \n",
       "78                                                NaN         \n",
       "79                                                NaN         \n",
       "80                                                NaN         \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1  \\\n",
       "0                                   0.0   \n",
       "1                                   4.0   \n",
       "2                                   1.0   \n",
       "3                                   0.0   \n",
       "4                                   3.0   \n",
       "..                                  ...   \n",
       "76                                  4.0   \n",
       "77                                  5.0   \n",
       "78                                  0.0   \n",
       "79                                  3.0   \n",
       "80                                  0.0   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Count  \\\n",
       "0                                           2   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "3                                           2   \n",
       "4                                           2   \n",
       "..                                        ...   \n",
       "76                                          2   \n",
       "77                                          2   \n",
       "78                                          2   \n",
       "79                                          2   \n",
       "80                                          2   \n",
       "\n",
       "    Short_Mad Max_Survey_Familiarity_C1_Normalized  \\\n",
       "0                                            0.000   \n",
       "1                                            0.500   \n",
       "2                                            0.125   \n",
       "3                                            0.000   \n",
       "4                                            0.375   \n",
       "..                                             ...   \n",
       "76                                           0.500   \n",
       "77                                           0.625   \n",
       "78                                           0.000   \n",
       "79                                           0.375   \n",
       "80                                           0.000   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "..                                   ...   \n",
       "76                                   0.0   \n",
       "77                                   0.0   \n",
       "78                                   0.0   \n",
       "79                                   0.0   \n",
       "80                                   0.0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Count  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "76                                           0   \n",
       "77                                           0   \n",
       "78                                           0   \n",
       "79                                           0   \n",
       "80                                           0   \n",
       "\n",
       "    Short_The Town_Survey_Familiarity_C1_Normalized  \n",
       "0                                               NaN  \n",
       "1                                               NaN  \n",
       "2                                               NaN  \n",
       "3                                               NaN  \n",
       "4                                               NaN  \n",
       "..                                              ...  \n",
       "76                                              NaN  \n",
       "77                                              NaN  \n",
       "78                                              NaN  \n",
       "79                                              NaN  \n",
       "80                                              NaN  \n",
       "\n",
       "[81 rows x 354 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "familiarity_prefixes = sorted({\n",
    "    column.split(\"_Survey_\")[0]\n",
    "    for column in survey_numeric.columns\n",
    "    if column.endswith(\"_Survey_Familiarity_F1\")\n",
    "})\n",
    "\n",
    "for prefix in familiarity_prefixes:\n",
    "    f1_col = f\"{prefix}_Survey_Familiarity_F1\"\n",
    "    f2_col = f\"{prefix}_Survey_Familiarity_F2\"\n",
    "    if f1_col not in survey_numeric.columns or f2_col not in survey_numeric.columns:\n",
    "        continue\n",
    "    c1_col = f\"{prefix}_Survey_Familiarity_C1\"\n",
    "    count_col = f\"{prefix}_Survey_Familiarity_C1_Count\"\n",
    "    norm_col = f\"{prefix}_Survey_Familiarity_C1_Normalized\"\n",
    "    pair = survey_numeric[[f1_col, f2_col]]\n",
    "    sum_values = pair.fillna(0).sum(axis=1)\n",
    "    count_values = pair.notna().sum(axis=1)\n",
    "    survey_numeric[c1_col] = sum_values\n",
    "    survey_numeric[count_col] = count_values\n",
    "    survey_numeric[norm_col] = np.where(\n",
    "        count_values > 0,\n",
    "        np.clip(sum_values / (4 * count_values), 0, 1),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "survey_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a062347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>survey_group</th>\n",
       "      <th>survey_study</th>\n",
       "      <th>survey_gender</th>\n",
       "      <th>survey_age</th>\n",
       "      <th>survey_file</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E1</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E12</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E14</th>\n",
       "      <th>Long_The Town_Survey_Enjoyment_E15</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>MALE</td>\n",
       "      <td>69</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>24</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>50</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group A</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>51</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-A1.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>107</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>MALE</td>\n",
       "      <td>33</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>85</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>34</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>61</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>96</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>Default</td>\n",
       "      <td>Group F</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>53</td>\n",
       "      <td>MERGED_SURVEY_RESPONSE_MATRIX-F2.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent survey_group survey_study survey_gender  survey_age  \\\n",
       "0          83      Default      Group A          MALE          69   \n",
       "1          81      Default      Group A        FEMALE          24   \n",
       "2          99      Default      Group A        FEMALE          25   \n",
       "3          52      Default      Group A        FEMALE          50   \n",
       "4           8      Default      Group A         OTHER          51   \n",
       "..        ...          ...          ...           ...         ...   \n",
       "76        107      Default      Group F          MALE          33   \n",
       "77         85      Default      Group F        FEMALE          34   \n",
       "78         70      Default      Group F        FEMALE          61   \n",
       "79         96      Default      Group F        FEMALE          29   \n",
       "80         41      Default      Group F        FEMALE          53   \n",
       "\n",
       "                             survey_file  Long_The Town_Survey_Enjoyment_E1  \\\n",
       "0   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "1   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "2   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                4.0   \n",
       "3   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "4   MERGED_SURVEY_RESPONSE_MATRIX-A1.txt                                5.0   \n",
       "..                                   ...                                ...   \n",
       "76  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "77  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "78  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "79  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "80  MERGED_SURVEY_RESPONSE_MATRIX-F2.txt                                NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E12  Long_The Town_Survey_Enjoyment_E14  \\\n",
       "0                                  5.0                                 5.0   \n",
       "1                                  5.0                                 5.0   \n",
       "2                                  4.0                                 4.0   \n",
       "3                                  4.0                                 5.0   \n",
       "4                                  5.0                                 5.0   \n",
       "..                                 ...                                 ...   \n",
       "76                                 NaN                                 NaN   \n",
       "77                                 NaN                                 NaN   \n",
       "78                                 NaN                                 NaN   \n",
       "79                                 NaN                                 NaN   \n",
       "80                                 NaN                                 NaN   \n",
       "\n",
       "    Long_The Town_Survey_Enjoyment_E15  ...  \\\n",
       "0                                  5.0  ...   \n",
       "1                                  5.0  ...   \n",
       "2                                  4.0  ...   \n",
       "3                                  5.0  ...   \n",
       "4                                  5.0  ...   \n",
       "..                                 ...  ...   \n",
       "76                                 NaN  ...   \n",
       "77                                 NaN  ...   \n",
       "78                                 NaN  ...   \n",
       "79                                 NaN  ...   \n",
       "80                                 NaN  ...   \n",
       "\n",
       "    Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      2.0   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "76                                     NaN   \n",
       "77                                     NaN   \n",
       "78                                     NaN   \n",
       "79                                     NaN   \n",
       "80                                     NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               5.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               4.0   \n",
       "..                                              ...   \n",
       "76                                              NaN   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               4.0   \n",
       "1                                               1.0   \n",
       "2                                               2.0   \n",
       "3                                               1.0   \n",
       "4                                               3.0   \n",
       "..                                              ...   \n",
       "76                                              NaN   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               1.0   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "76                                              NaN   \n",
       "77                                              NaN   \n",
       "78                                              NaN   \n",
       "79                                              NaN   \n",
       "80                                              NaN   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                      1.0   \n",
       "1                                      4.0   \n",
       "2                                      4.0   \n",
       "3                                      2.0   \n",
       "4                                      4.0   \n",
       "..                                     ...   \n",
       "76                                     5.0   \n",
       "77                                     4.0   \n",
       "78                                     1.0   \n",
       "79                                     4.0   \n",
       "80                                     4.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                      1.0   \n",
       "1                                      3.0   \n",
       "2                                      3.0   \n",
       "3                                      2.0   \n",
       "4                                      3.0   \n",
       "..                                     ...   \n",
       "76                                     4.0   \n",
       "77                                     3.0   \n",
       "78                                     1.0   \n",
       "79                                     3.0   \n",
       "80                                     3.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "76                                     1.0   \n",
       "77                                     1.0   \n",
       "78                                     NaN   \n",
       "79                                     1.0   \n",
       "80                                     1.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "76                                      5.0   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      4.0   \n",
       "80                                      4.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "76                                      4.0   \n",
       "77                                      0.0   \n",
       "78                                      1.0   \n",
       "79                                      3.0   \n",
       "80                                      3.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F2  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                       NaN  \n",
       "..                                      ...  \n",
       "76                                      1.0  \n",
       "77                                      NaN  \n",
       "78                                      NaN  \n",
       "79                                      1.0  \n",
       "80                                      1.0  \n",
       "\n",
       "[81 rows x 357 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_ended_columns = [\n",
    "    column\n",
    "    for column, meta in survey_metadata_lookup.iterrows()\n",
    "    if column in survey_numeric.columns and (meta.get(\"question_type\") or \"\").strip().lower() == \"open ended\"\n",
    "]\n",
    "\n",
    "survey_open_ended = survey_responses[[\n",
    "    \"respondent\",\n",
    "    \"survey_group\",\n",
    "    \"survey_study\",\n",
    "    \"survey_gender\",\n",
    "    \"survey_age\",\n",
    "    \"survey_file\",\n",
    "    *open_ended_columns,\n",
    "]].copy()\n",
    "\n",
    "survey_features = survey_numeric.drop(columns=[col for col in open_ended_columns if col in survey_numeric.columns])\n",
    "\n",
    "# Integrate screening familiarity composites\n",
    "screening_path = project_root / \"results\" / \"individual_composite_scores.csv\"\n",
    "if screening_path.exists():\n",
    "    screening_raw = pd.read_csv(screening_path)\n",
    "    screening_raw[\"respondent\"] = screening_raw[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "    screening_value_columns = [\n",
    "        col\n",
    "        for col in screening_raw.columns\n",
    "        if col.endswith(\"_Survey_Familiarity_F1\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F2\")\n",
    "        or col.endswith(\"_Survey_Familiarity_F3\")\n",
    "        or col.endswith(\"_Survey_Familiarity_C1\")\n",
    "    ]\n",
    "\n",
    "    respondent_groups = (\n",
    "        uv_stage1\n",
    "        .loc[:, [\"respondent\", \"group\"]]\n",
    "        .assign(group=lambda df: df[\"group\"].astype(str).str.strip().str.upper())\n",
    "        .set_index(\"respondent\")\n",
    "        .to_dict()\n",
    "    )[\"group\"]\n",
    "\n",
    "    title_normalization = {\n",
    "        \"mad max fury road\": \"Mad Max\",\n",
    "        \"mad max\": \"Mad Max\",\n",
    "        \"the town\": \"The Town\",\n",
    "        \"abbot elementary\": \"Abbot Elementary\",\n",
    "        \"abbott elementary\": \"Abbot Elementary\",\n",
    "    }\n",
    "\n",
    "    def canonicalize_title(raw_title: str) -> str:\n",
    "        cleaned = str(raw_title).strip()\n",
    "        return title_normalization.get(cleaned.lower(), cleaned)\n",
    "\n",
    "    stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "    stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"([A-F])\", expand=False)\n",
    "    stimulus_map[\"title_clean\"] = stimulus_map[\"title\"].astype(str).str.strip()\n",
    "\n",
    "    group_title_form_lookup = {}\n",
    "    default_form_per_title = {}\n",
    "\n",
    "    for row in stimulus_map.itertuples():\n",
    "        if pd.isna(row.group_letter) or pd.isna(row.title_clean) or pd.isna(row.form):\n",
    "            continue\n",
    "        canonical_title = canonicalize_title(row.title_clean)\n",
    "        form_value = str(row.form).title()\n",
    "        group_title_form_lookup[(row.group_letter, canonical_title)] = form_value\n",
    "        default_form_per_title.setdefault(canonical_title, form_value)\n",
    "\n",
    "    screening_records = []\n",
    "\n",
    "    for _, row in screening_raw.iterrows():\n",
    "        respondent_id = row.get(\"respondent\")\n",
    "        if respondent_id is None:\n",
    "            continue\n",
    "        respondent_id = str(respondent_id).strip()\n",
    "        group_letter = respondent_groups.get(respondent_id)\n",
    "        for column in screening_value_columns:\n",
    "            value = row.get(column)\n",
    "            if pd.isna(value) or value == \"\":\n",
    "                continue\n",
    "            base_part, _, suffix_part = column.partition(\"_Survey_Familiarity_\")\n",
    "            if not suffix_part:\n",
    "                continue\n",
    "            question_code = suffix_part.strip()\n",
    "            canonical_title = canonicalize_title(base_part.strip())\n",
    "            form_value = None\n",
    "            if group_letter:\n",
    "                form_value = group_title_form_lookup.get((group_letter, canonical_title))\n",
    "            if form_value is None:\n",
    "                form_value = default_form_per_title.get(canonical_title, \"Long\")\n",
    "            target_column = f\"{form_value}_{canonical_title}_Screening_Familiarity_{question_code}\"\n",
    "            screening_records.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"target_column\": target_column,\n",
    "                \"value\": pd.to_numeric(value, errors=\"coerce\")\n",
    "            })\n",
    "\n",
    "    if screening_records:\n",
    "        screening_features = (\n",
    "            pd.DataFrame(screening_records)\n",
    "            .pivot_table(index=\"respondent\", columns=\"target_column\", values=\"value\", aggfunc=\"first\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        screening_features.columns.name = None\n",
    "        survey_features = survey_features.merge(screening_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "survey_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef38cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 survey features saved to uv_stage3_full_features.csv with 81 respondents and 356 feature columns.\n",
      "Open-ended responses archived to uv_stage3_full_open_ended.csv.\n",
      "Unified view with Stage 3 survey data exported to uv_stage3_full_uv.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age_group</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>...</th>\n",
       "      <th>Long_The Town_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Abbot Elementary_Screening_Familiarity_F2</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_C1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F1</th>\n",
       "      <th>Short_Mad Max_Screening_Familiarity_F2</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_C1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F1</th>\n",
       "      <th>Short_The Town_Screening_Familiarity_F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>44-59</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006_11.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_11.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>18-27</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>005_50.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "      <td>Other</td>\n",
       "      <td>10/14/2025</td>\n",
       "      <td>09:54:03</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>004_60.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>09:34:06</td>\n",
       "      <td>60-69</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>003_70.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>09:49:14</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>002_85.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/17/2025</td>\n",
       "      <td>14:37:41</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>004_96.csv</td>\n",
       "      <td>F</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>28-43</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 1445 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_file group respondent  age  gender  date_study time_study  \\\n",
       "0   003_104.csv     A        104   59    Male  10/16/2025   18:09:03   \n",
       "1   002_106.csv     A        106   30    Male  10/16/2025   19:35:05   \n",
       "2    006_11.csv     A         11   33  Female  10/11/2025   09:32:42   \n",
       "3    006_11.csv     A         11   33  Female  10/11/2025   09:32:42   \n",
       "4   001_116.csv     A        116   19    Male  10/18/2025   12:37:40   \n",
       "..          ...   ...        ...  ...     ...         ...        ...   \n",
       "80   005_50.csv     F         50   63   Other  10/14/2025   09:54:03   \n",
       "81   004_60.csv     F         60   66    Male  10/15/2025   09:34:06   \n",
       "82   003_70.csv     F         70   61  Female  10/16/2025   09:49:14   \n",
       "83   002_85.csv     F         85   34  Female  10/17/2025   14:37:41   \n",
       "84   004_96.csv     F         96   29  Female  10/15/2025   14:32:00   \n",
       "\n",
       "   age_group               ethnicity                income_group  ...  \\\n",
       "0      44-59                   White    $60,000 or more per year  ...   \n",
       "1      28-43                   White    $60,000 or more per year  ...   \n",
       "2      28-43                   White  $35,000  $60,000 per year  ...   \n",
       "3      28-43                   White  $35,000  $60,000 per year  ...   \n",
       "4      18-27                   White  $35,000  $60,000 per year  ...   \n",
       "..       ...                     ...                         ...  ...   \n",
       "80     60-69  Black/African American    $60,000 or more per year  ...   \n",
       "81     60-69                   White  $35,000  $60,000 per year  ...   \n",
       "82     60-69  Black/African American  $35,000  $60,000 per year  ...   \n",
       "83     28-43                   White    $60,000 or more per year  ...   \n",
       "84     28-43                   White    $60,000 or more per year  ...   \n",
       "\n",
       "   Long_The Town_Screening_Familiarity_F2  \\\n",
       "0                                     1.0   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "..                                    ...   \n",
       "80                                    NaN   \n",
       "81                                    NaN   \n",
       "82                                    NaN   \n",
       "83                                    NaN   \n",
       "84                                    NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_C1  \\\n",
       "0                                               0.0   \n",
       "1                                               4.0   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "83                                              NaN   \n",
       "84                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F1  \\\n",
       "0                                               0.0   \n",
       "1                                               3.0   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               1.0   \n",
       "..                                              ...   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "83                                              NaN   \n",
       "84                                              NaN   \n",
       "\n",
       "    Short_Abbot Elementary_Screening_Familiarity_F2  \\\n",
       "0                                               NaN   \n",
       "1                                               1.0   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "..                                              ...   \n",
       "80                                              NaN   \n",
       "81                                              NaN   \n",
       "82                                              NaN   \n",
       "83                                              NaN   \n",
       "84                                              NaN   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_C1  \\\n",
       "0                                      1.0   \n",
       "1                                      4.0   \n",
       "2                                      1.0   \n",
       "3                                      1.0   \n",
       "4                                      4.0   \n",
       "..                                     ...   \n",
       "80                                     6.0   \n",
       "81                                     1.0   \n",
       "82                                     1.0   \n",
       "83                                     4.0   \n",
       "84                                     4.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F1  \\\n",
       "0                                      1.0   \n",
       "1                                      3.0   \n",
       "2                                      1.0   \n",
       "3                                      1.0   \n",
       "4                                      3.0   \n",
       "..                                     ...   \n",
       "80                                     4.0   \n",
       "81                                     1.0   \n",
       "82                                     1.0   \n",
       "83                                     3.0   \n",
       "84                                     3.0   \n",
       "\n",
       "    Short_Mad Max_Screening_Familiarity_F2  \\\n",
       "0                                      NaN   \n",
       "1                                      1.0   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "80                                     2.0   \n",
       "81                                     NaN   \n",
       "82                                     NaN   \n",
       "83                                     1.0   \n",
       "84                                     1.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_C1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "80                                      6.0   \n",
       "81                                      1.0   \n",
       "82                                      1.0   \n",
       "83                                      0.0   \n",
       "84                                      4.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F1  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "..                                      ...   \n",
       "80                                      3.0   \n",
       "81                                      1.0   \n",
       "82                                      1.0   \n",
       "83                                      0.0   \n",
       "84                                      3.0   \n",
       "\n",
       "    Short_The Town_Screening_Familiarity_F2  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                       NaN  \n",
       "..                                      ...  \n",
       "80                                      3.0  \n",
       "81                                      NaN  \n",
       "82                                      NaN  \n",
       "83                                      NaN  \n",
       "84                                      1.0  \n",
       "\n",
       "[85 rows x 1445 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _safe_write_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except PermissionError:\n",
    "        fallback = path.with_name(f\"{path.stem}_{pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')}.csv\")\n",
    "        df.to_csv(fallback, index=False)\n",
    "        print(f\"Permission denied for {path}. Saved to {fallback.name} instead.\")\n",
    "        return fallback\n",
    "\n",
    "base_uv = full_uv.copy() if \"full_uv\" in globals() else uv_stage1.copy()\n",
    "base_uv[\"respondent\"] = base_uv[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "survey_features[\"respondent\"] = survey_features[\"respondent\"].astype(str).str.strip()\n",
    "uv_stage3 = base_uv.merge(survey_features, on=\"respondent\", how=\"left\")\n",
    "\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "features_path = _safe_write_csv(survey_features, results_dir / \"uv_stage3_full_features.csv\")\n",
    "open_ended_path = _safe_write_csv(survey_open_ended, results_dir / \"uv_stage3_full_open_ended.csv\")\n",
    "uv_path = _safe_write_csv(uv_stage3, results_dir / \"uv_stage3_full_uv.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Stage 3 survey features saved to {features_path.name} with {survey_features.shape[0]} respondents and \"\n",
    "    f\"{survey_features.shape[1] - 1} feature columns.\"\n",
    ")\n",
    "print(f\"Open-ended responses archived to {open_ended_path.name}.\")\n",
    "print(f\"Unified view with Stage 3 survey data exported to {uv_path.name}.\")\n",
    "\n",
    "uv_stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "72330f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31    25.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Sum, dtype: float64\n",
      "31    2.0\n",
      "Name: Long_Abbot Elementary_Survey_Enjoyment_E18, dtype: float64\n",
      "31    23.0\n",
      "Name: Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Sum'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_Enjoyment_E18'])\n",
    "print(survey_features.loc[(survey_features[\"respondent\"].astype(int)==82)]['Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "15074b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Sum</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Count</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected</th>\n",
       "      <th>Long_Abbot Elementary_Survey_EnjoymentComposite_Mean</th>\n",
       "      <th>normalized_from_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Sum  \\\n",
       "22                                               41.0     \n",
       "23                                               51.0     \n",
       "24                                               50.0     \n",
       "25                                               45.0     \n",
       "26                                               51.0     \n",
       "27                                               35.0     \n",
       "28                                               19.0     \n",
       "29                                               19.0     \n",
       "30                                               55.0     \n",
       "31                                               25.0     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Corrected  \\\n",
       "22                                               41.0           \n",
       "23                                               53.0           \n",
       "24                                               54.0           \n",
       "25                                               49.0           \n",
       "26                                               55.0           \n",
       "27                                               37.0           \n",
       "28                                               15.0           \n",
       "29                                               15.0           \n",
       "30                                               55.0           \n",
       "31                                               23.0           \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Count  \\\n",
       "22                                                 12       \n",
       "23                                                 12       \n",
       "24                                                 12       \n",
       "25                                                 12       \n",
       "26                                                 12       \n",
       "27                                                 12       \n",
       "28                                                 12       \n",
       "29                                                 12       \n",
       "30                                                 12       \n",
       "31                                                 12       \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Normalized  \\\n",
       "22                                           0.604167            \n",
       "23                                           0.812500            \n",
       "24                                           0.791667            \n",
       "25                                           0.687500            \n",
       "26                                           0.812500            \n",
       "27                                           0.479167            \n",
       "28                                           0.145833            \n",
       "29                                           0.145833            \n",
       "30                                           0.895833            \n",
       "31                                           0.270833            \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_NormalizedCorrected  \\\n",
       "22                                           0.604167                     \n",
       "23                                           0.854167                     \n",
       "24                                           0.875000                     \n",
       "25                                           0.770833                     \n",
       "26                                           0.895833                     \n",
       "27                                           0.520833                     \n",
       "28                                           0.062500                     \n",
       "29                                           0.062500                     \n",
       "30                                           0.895833                     \n",
       "31                                           0.229167                     \n",
       "\n",
       "    Long_Abbot Elementary_Survey_EnjoymentComposite_Mean  \\\n",
       "22                                           3.416667      \n",
       "23                                           4.416667      \n",
       "24                                           4.500000      \n",
       "25                                           4.083333      \n",
       "26                                           4.583333      \n",
       "27                                           3.083333      \n",
       "28                                           1.250000      \n",
       "29                                           1.250000      \n",
       "30                                           4.583333      \n",
       "31                                           1.916667      \n",
       "\n",
       "    normalized_from_corrected  \n",
       "22                   0.604167  \n",
       "23                   0.854167  \n",
       "24                   0.875000  \n",
       "25                   0.770833  \n",
       "26                   0.895833  \n",
       "27                   0.520833  \n",
       "28                   0.062500  \n",
       "29                   0.062500  \n",
       "30                   0.895833  \n",
       "31                   0.229167  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Long_Abbot Elementary\"\n",
    "columns_to_show = [\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Sum\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Corrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Count\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Normalized\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_NormalizedCorrected\",\n",
    "    f\"{prefix}_Survey_EnjoymentComposite_Mean\",\n",
    "]\n",
    "comparison = survey_features.loc[\n",
    "    survey_features[f\"{prefix}_Survey_EnjoymentComposite_Count\"].gt(0),\n",
    "    columns_to_show\n",
    "].head(10).copy()\n",
    "comparison[\"normalized_from_corrected\"] = np.clip(\n",
    "    (comparison[f\"{prefix}_Survey_EnjoymentComposite_Corrected\"]\n",
    "     - comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"])\n",
    "    / (4.0 * comparison[f\"{prefix}_Survey_EnjoymentComposite_Count\"]),\n",
    "    0,\n",
    "    1,\n",
    " )\n",
    "comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
