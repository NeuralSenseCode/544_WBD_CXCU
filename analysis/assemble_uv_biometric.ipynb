{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a9680a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f48a0",
   "metadata": {},
   "source": [
    "## Function to read iMotions sensor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wbdlib.io import safe_write_csv\n",
    "\n",
    "def _extract_imotions_metadata(path, metadata=None):\n",
    "    \"\"\"Read leading metadata lines from an iMotions CSV without loading the data.\"\"\"\n",
    "    metadata = metadata or []\n",
    "    requested = set(metadata) if metadata else None\n",
    "    meta_lines = []\n",
    "    header_rows = 0\n",
    "    with open(path, \"r\", encoding=\"latin1\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            first_cell = line.split(\",\", 1)[0]\n",
    "            if \"#\" in first_cell:\n",
    "                meta_lines.append(line)\n",
    "                header_rows += 1\n",
    "            else:\n",
    "                break\n",
    "    meta_dict = {}\n",
    "    for raw_line in meta_lines:\n",
    "        segments = raw_line.strip().split(\"#\", 1)\n",
    "        if len(segments) < 2:\n",
    "            continue\n",
    "        cleaned = segments[1]\n",
    "        parts = cleaned.split(\",\")\n",
    "        if len(parts) > 1:\n",
    "            key = parts[0].strip()\n",
    "            value = \",\".join(parts[1:]).strip()\n",
    "            if requested is None or key in requested:\n",
    "                meta_dict[key] = value\n",
    "    return meta_dict, header_rows\n",
    "\n",
    "def read_imotions_metadata(path, metadata=None):\n",
    "    \"\"\"Return only the requested metadata from an iMotions CSV.\"\"\"\n",
    "    meta_dict, _ = _extract_imotions_metadata(path, metadata)\n",
    "    return meta_dict\n",
    "\n",
    "def read_imotions(path, metadata=None):\n",
    "    \"\"\"\n",
    "    Reads an iMotions CSV file while extracting optional metadata fields.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the iMotions CSV file.\n",
    "        metadata (list[str], optional): List of metadata keys to extract.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The data as a DataFrame.\n",
    "        meta_dict (dict): Dictionary containing requested metadata fields.\n",
    "    \"\"\"\n",
    "    meta_dict, header_rows = _extract_imotions_metadata(path, metadata)\n",
    "    df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
    "    return df, meta_dict\n",
    "\n",
    "def get_files(folder, tags=['',]):\n",
    "    return [f for f in os.listdir(folder) if not f.startswith('.') and all(x in f for x in tags)] \n",
    "\n",
    "\n",
    "def get_biometric_data(in_folder, results_folder):\n",
    "\n",
    "    ######## Define ########\n",
    "    # Define paths\n",
    "    out_path = f\"{results_folder}/\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    respondents = [1,2,3] #define list of respondent ids\n",
    "\n",
    "    # Define signal columns\n",
    "    cols_afdex = [\n",
    "                \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "                \"Surprise\", \"Engagement\", \"Valence\", \"Sentimentality\",\n",
    "                \"Confusion\", \"Neutral\"\n",
    "        ]\n",
    "    cols_eeg = ['High Engagement',\n",
    "        'Low Engagement',\n",
    "        'Distraction',\n",
    "        'Drowsy',\n",
    "        'Workload Average',\n",
    "        'Frontal Asymmetry Alpha',\n",
    "        ]\n",
    "\n",
    "\n",
    "    #Define window lengths in seconds\n",
    "    window_lengths = [3,]\n",
    "\n",
    "    ######## Read Inputs #######\n",
    "    #Get input files\n",
    "    sensor_files = get_files(f'{in_folder}/Sensors/',tags=['.csv',])\n",
    "\n",
    "    ### Begin ###\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    for respondent in respondents:\n",
    "        error = {'respondent':respondent, 'FAC':None, 'EEG':None, 'GSR':None, 'Blinks':None, 'ET':None}\n",
    "        interaction = {'respondent':respondent}\n",
    "        try:\n",
    "            file = [f for f in sensor_files if respondent in f][0] #may need adjustment\n",
    "            df_sens_resp,_ = read_imotions(f'{in_folder}/Sensors/{file}')\n",
    "\n",
    "            # Get sensor data per stimulus\n",
    "            for task in df_sens_resp['SourceStimuliName'].unique():\n",
    "                df_sens_task = df_sens_resp.loc[(df_sens_resp['SourceStimuliName']==task)]\n",
    "                window = task\n",
    "\n",
    "                # Get facial coding data\n",
    "                for a in cols_afdex:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_FAC_{a}_mean']=df_sens_task[a].dropna().mean()\n",
    "                        auc_data = df_sens_task[['Timestamp',a]].dropna()\n",
    "                        interaction[f'sens_{window}_FAC_{a}_AUC']=np.trapz(auc_data[a],x=auc_data['Timestamp'])/1000\n",
    "                        interaction[f'sens_{window}_FAC_{a}_Binary']=df_sens_task[a].dropna().max()>= 50\n",
    "                    except:\n",
    "                        error['FAC']='Missing'\n",
    "\n",
    "                for e in cols_eeg:\n",
    "                    try:\n",
    "                        interaction[f'sens_{window}_EEG_{e}_mean']=df_sens_task[e].dropna()[df_sens_int[e] > -9000].mean()\n",
    "                        auc_data = df_sens_task.loc[df_sens_task[e].notna() & (df_sens_task[e] > -9000), ['Timestamp', e]]\n",
    "                        interaction[f'sens_{window}_EEG_{e}_AUC']=np.trapz(auc_data[e],x=auc_data['Timestamp'])/1000\n",
    "                    except:\n",
    "                        error['EEG']='Missing'\n",
    "\n",
    "                try:\n",
    "                    interaction[f'sens_{window}_GSR_PeakDetected_Binary'] =1 if df_sens_task['Peak Detected'].sum()>0 else 0\n",
    "                    gsr_data = df_sens_task[['Timestamp','Peak Detected']].dropna()\n",
    "                    mask = gsr_data['Peak Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = gsr_data.loc[mask, 'Peak Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_GSR_Peaks_Count'] =count_patches\n",
    "                except:\n",
    "                    error['GSR']='Missing'\n",
    "\n",
    "                try:\n",
    "                    blink_data = df_sens_task[['Timestamp','Blink Detected']].dropna()\n",
    "                    mask = blink_data['Blink Detected'] == 1\n",
    "                    segments = (mask != mask.shift()).cumsum()  # Assign unique numbers to patches\n",
    "                    count_patches = blink_data.loc[mask, 'Blink Detected'].groupby(segments).ngroup().nunique()\n",
    "                    interaction[f'sens_{window}_ET_Blink_Count'] =count_patches\n",
    "                    interaction[f'sens_{window}_ET_Blink_Rate'] =count_patches/((df_sens_task['Timestamp'].values[-1]-df_sens_task['Timestamp'].values[0])/(1000 * 60))\n",
    "                except:\n",
    "                    error['ET']='Missing'\n",
    "\n",
    "            # TODO Get sensor data for non-interaction\n",
    "            ##################################### Add this in\n",
    "            results.append(interaction)\n",
    "            errors.append(error)\n",
    "\n",
    "            pass\n",
    "        except IndexError:\n",
    "            print(f'>>> Could not find {respondent} sensor data')\n",
    "        except:\n",
    "            print(f'>>> Failed {respondent}')\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(f'{out_path}biometric_results.csv')\n",
    "\n",
    "    errors = pd.DataFrame(errors)\n",
    "    errors.to_csv(f'{out_path}errors_biometric.csv')\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_export_dir = project_root / \"data\" / \"Export\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef61f8",
   "metadata": {},
   "source": [
    "## Explanation of functions\n",
    "\n",
    "The above functions are used to read in the sesor data files, one csv at a time, and extract single features per stimulus, and write these features to a simple results file.\n",
    "\n",
    "The functions must be adjusted to:\n",
    "- Discern between long form and short form\n",
    "- Isolate key moments from timings file provided by client\n",
    "- Extract time series\n",
    "- Compute group-wide features such as inter-subject correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84624884",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- Create naming dictionary for all stims\n",
    "- Get total times of all stims\n",
    "- Prepare key_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6248d4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sensor_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>001_116.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>001_58.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>001_114.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>001_102.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>001_108.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>001_107.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  sensor_file\n",
       "0  Group A  001_116.csv\n",
       "1  Group B   001_58.csv\n",
       "2  Group C  001_114.csv\n",
       "3  Group D  001_102.csv\n",
       "4  Group E  001_108.csv\n",
       "5  Group F  001_107.csv"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate one sensor export per group for duration scanning\n",
    "\n",
    "\n",
    "group_sensor_files = {}\n",
    "for group_dir in sorted(data_export_dir.glob(\"Group *\")):\n",
    "    if not group_dir.is_dir():\n",
    "        continue\n",
    "    sensor_dirs = sorted(group_dir.glob(\"Analyses/*/Sensor Data\"))\n",
    "    csv_candidates = []\n",
    "    for sensor_dir in sensor_dirs:\n",
    "        csv_candidates.extend(sorted(sensor_dir.glob(\"*.csv\")))\n",
    "    group_sensor_files[group_dir.name] = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "sensor_selection = pd.DataFrame([\n",
    "    {\n",
    "        \"group\": group,\n",
    "        \"sensor_file\": path.name if path else None\n",
    "    }\n",
    "    for group, path in group_sensor_files.items()\n",
    "]).sort_values(\"group\").reset_index(drop=True)\n",
    "\n",
    "sensor_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99e8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n"
     ]
    }
   ],
   "source": [
    "# Collect per-group stimulus durations without aggregating across groups\n",
    "duration_tables = []\n",
    "issues = {}\n",
    "\n",
    "for group, path in group_sensor_files.items():\n",
    "    if path is None:\n",
    "        issues[group] = \"No sensor CSV found\"\n",
    "        continue\n",
    "    try:\n",
    "        df_group, _ = read_imotions(path)\n",
    "    except Exception as exc:\n",
    "        issues[group] = f\"read_imotions failed: {exc}\"\n",
    "        continue\n",
    "\n",
    "    required_cols = {\"SourceStimuliName\", \"Timestamp\"}\n",
    "    if not required_cols.issubset(df_group.columns):\n",
    "        issues[group] = \"Missing SourceStimuliName or Timestamp\"\n",
    "        continue\n",
    "    df_clean = df_group[[\"SourceStimuliName\", \"Timestamp\"]].copy()\n",
    "    df_clean = df_clean.dropna(subset=[\"SourceStimuliName\"])\n",
    "    df_clean[\"Timestamp\"] = pd.to_numeric(df_clean[\"Timestamp\"], errors=\"coerce\")\n",
    "    df_clean = df_clean.dropna(subset=[\"Timestamp\"])\n",
    "    if df_clean.empty:\n",
    "        issues[group] = \"No valid timestamp data\"\n",
    "        continue\n",
    "\n",
    "    group_duration = (\n",
    "        df_clean.groupby(\"SourceStimuliName\")[\"Timestamp\"]\n",
    "        .apply(lambda s: s.max() - s.min())\n",
    "        .reset_index(name=\"duration_ms\")\n",
    "    )\n",
    "\n",
    "    if group_duration.empty:\n",
    "        issues[group] = \"No stimuli with duration\"\n",
    "        continue\n",
    "\n",
    "    group_duration[\"duration_seconds\"] = group_duration[\"duration_ms\"] / 1000.0\n",
    "    group_duration[\"duration_minutes\"] = group_duration[\"duration_seconds\"] / 60.0\n",
    "    group_duration.insert(0, \"group\", group)\n",
    "    group_duration.rename(columns={\"SourceStimuliName\": \"stimulus_name\"}, inplace=True)\n",
    "    duration_tables.append(group_duration[[\"group\", \"stimulus_name\", \"duration_seconds\", \"duration_minutes\"]])\n",
    "\n",
    "if duration_tables:\n",
    "    stimulus_summary = pd.concat(duration_tables, ignore_index=True)\n",
    "    stimulus_summary.sort_values([\"group\", \"stimulus_name\"], inplace=True)\n",
    "    stimulus_summary[\"duration_seconds\"] = stimulus_summary[\"duration_seconds\"].round(2)\n",
    "    stimulus_summary[\"duration_minutes\"] = stimulus_summary[\"duration_minutes\"].round(2)\n",
    "    stimulus_summary.reset_index(drop=True, inplace=True)\n",
    "    stimulus_summary\n",
    "else:\n",
    "    print(\"No duration records computed.\")\n",
    "\n",
    "if issues:\n",
    "    pd.DataFrame(\n",
    "        {\"group\": list(issues.keys()), \"issue\": list(issues.values())}\n",
    "    ).sort_values(\"group\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9548d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>stimulus_name</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>A STAR IS BORN</td>\n",
       "      <td>248.67</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group A</td>\n",
       "      <td>HOME ALONE</td>\n",
       "      <td>115.12</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group A</td>\n",
       "      <td>MAD MAX FURY ROAD</td>\n",
       "      <td>226.47</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group A</td>\n",
       "      <td>THE CONJURING</td>\n",
       "      <td>171.31</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group A</td>\n",
       "      <td>THE TOWN</td>\n",
       "      <td>1744.51</td>\n",
       "      <td>29.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group      stimulus_name  duration_seconds  duration_minutes\n",
       "0  Group A     A STAR IS BORN            248.67              4.14\n",
       "1  Group A         HOME ALONE            115.12              1.92\n",
       "2  Group A  MAD MAX FURY ROAD            226.47              3.77\n",
       "3  Group A      THE CONJURING            171.31              2.86\n",
       "4  Group A           THE TOWN           1744.51             29.08"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38df137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ec5417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd2d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      59.82\n",
       "max    1811.54\n",
       "Name: duration_seconds, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary['duration_seconds'].agg(['min','max']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6057f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>unique_stimuli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group A</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group D</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group E</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group F</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  unique_stimuli\n",
       "0  Group A               6\n",
       "1  Group B               6\n",
       "2  Group C               6\n",
       "3  Group D               6\n",
       "4  Group E               6\n",
       "5  Group F               6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_per_group = stimulus_summary.groupby('group')['stimulus_name'].nunique().reset_index(name='unique_stimuli')\n",
    "stimuli_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbddf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/ashra/Documents/NeuralSense/NeuralData/clients/544_WBD_CXCU/results/stimulus_summary_biometric.csv')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimulus_summary_path = project_root / \"results\" / \"stimulus_summary_biometric.csv\"\n",
    "safe_write_csv(stimulus_summary, stimulus_summary_path)\n",
    "stimulus_summary_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322effdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf37ed",
   "metadata": {},
   "source": [
    "## Stimulus Annotation Overview\n",
    "- `stimulus_rename` links each group-specific stimulus from `stimulus_summary` to a clean `title` and its presentation `Form` (`Long` or `Short`).\n",
    "- Some titles appear in both forms; the long cut (≈30 min) includes the short-form key moment as an embedded segment.\n",
    "- `key_moments` pinpoints, for every long-form title, when the key moment begins (`Lead-up Duration`) and how long it lasts (`Key moment Duration_LF`).\n",
    "- These tables let us align short-form clips with the corresponding segment inside the long-form presentation for downstream comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f5419",
   "metadata": {},
   "source": [
    "## Stage 1: Demographics\n",
    "We reuse the vetted Stage 1 export produced in `analysis/assemble_uv.ipynb`.\n",
    "Loading the respondent roster directly from `results/uv_stage1.csv` keeps the biometric pipeline aligned with the self-report workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8bff719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>date_study</th>\n",
       "      <th>time_study</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>income_group</th>\n",
       "      <th>content_consumption</th>\n",
       "      <th>content_consumption_movies</th>\n",
       "      <th>content_consumption_series</th>\n",
       "      <th>content_consumption_short</th>\n",
       "      <th>grid_comments</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003_104.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>18:09:03</td>\n",
       "      <td>59</td>\n",
       "      <td>44-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_106.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>106</td>\n",
       "      <td>10/16/2025</td>\n",
       "      <td>19:35:05</td>\n",
       "      <td>30</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_116.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>116</td>\n",
       "      <td>10/18/2025</td>\n",
       "      <td>12:37:40</td>\n",
       "      <td>19</td>\n",
       "      <td>18-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>$35,000  $60,000 per year</td>\n",
       "      <td>3 to 12 hours per week</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006_14.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>10/11/2025</td>\n",
       "      <td>09:32:42</td>\n",
       "      <td>33</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic/Latino/Latina/Latinx</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>More than 24 hours per week</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_3.csv</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>09:19:22</td>\n",
       "      <td>34</td>\n",
       "      <td>28-43</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>$60,000 or more per year</td>\n",
       "      <td>12 to 24 hours per week</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>No EEG.</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_file group respondent  date_study time_study  age age_group  gender  \\\n",
       "0  003_104.csv     A        104  10/16/2025   18:09:03   59     44-59    Male   \n",
       "1  002_106.csv     A        106  10/16/2025   19:35:05   30     28-43    Male   \n",
       "2  001_116.csv     A        116  10/18/2025   12:37:40   19     18-27    Male   \n",
       "3   006_14.csv     A         14  10/11/2025   09:32:42   33     28-43    Male   \n",
       "4    007_3.csv     A          3  10/10/2025   09:19:22   34     28-43  Female   \n",
       "\n",
       "                       ethnicity                income_group  \\\n",
       "0                          White    $60,000 or more per year   \n",
       "1                          White    $60,000 or more per year   \n",
       "2                          White  $35,000  $60,000 per year   \n",
       "3  Hispanic/Latino/Latina/Latinx    $60,000 or more per year   \n",
       "4                          White    $60,000 or more per year   \n",
       "\n",
       "           content_consumption  content_consumption_movies  \\\n",
       "0  More than 24 hours per week                          10   \n",
       "1       3 to 12 hours per week                          25   \n",
       "2       3 to 12 hours per week                          25   \n",
       "3  More than 24 hours per week                          20   \n",
       "4      12 to 24 hours per week                          10   \n",
       "\n",
       "   content_consumption_series  content_consumption_short grid_comments  \\\n",
       "0                          90                          0           NaN   \n",
       "1                          50                         25           NaN   \n",
       "2                          50                         25           NaN   \n",
       "3                          40                         40       No EEG.   \n",
       "4                          70                         20       No EEG.   \n",
       "\n",
       "  Short Form Long Form  \n",
       "0    Mad Max  The Town  \n",
       "1    Mad Max  The Town  \n",
       "2    Mad Max  The Town  \n",
       "3    Mad Max  The Town  \n",
       "4    Mad Max  The Town  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wbdlib import uv as uv_utils\n",
    "\n",
    "uv_stage1_path = project_root / \"results\" / \"uv_stage1.csv\"\n",
    "if not uv_stage1_path.exists():\n",
    "    raise FileNotFoundError(f\"Stage 1 export not found at {uv_stage1_path}\")\n",
    "\n",
    "uv_stage1 = pd.read_csv(uv_stage1_path)\n",
    "uv_stage1[\"respondent\"] = (\n",
    "    uv_stage1[\"respondent\"].apply(uv_utils.first_segment).fillna(\"\").astype(str).str.strip()\n",
    ")\n",
    "uv_stage1 = uv_stage1.loc[uv_stage1[\"respondent\"] != \"\"].copy()\n",
    "uv_stage1[\"group\"] = (\n",
    "    uv_stage1[\"group\"].apply(uv_utils.first_segment).fillna(\"\").astype(str).str.strip()\n",
    ")\n",
    "uv_stage1[\"source_file\"] = uv_stage1[\"source_file\"].apply(uv_utils.first_segment)\n",
    "uv_stage1[\"source_file\"] = uv_stage1[\"source_file\"].apply(\n",
    "    lambda value: value.strip() if isinstance(value, str) and value.strip() else None\n",
    ")\n",
    "\n",
    "uv = uv_stage1.copy()\n",
    "uv_stage1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da774bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All expected Stage 1 columns present.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>respondent_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group  respondent_count\n",
       "0     A                12\n",
       "1     B                10\n",
       "2     C                16\n",
       "3     D                17\n",
       "4     E                14\n",
       "5     F                14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_expected_columns = [\n",
    "    \"source_file\",\"group\",\"respondent\",\"Short Form\",\"Long Form\",\n",
    "    \"age\",\"gender\",\"age_group\",\"ethnicity\",\"income_group\",\n",
    "    \"content_consumption\",\"content_consumption_movies\",\n",
    "    \"content_consumption_series\",\"content_consumption_short\"\n",
    "]\n",
    "missing_stage1_columns = [col for col in stage1_expected_columns if col not in uv_stage1.columns]\n",
    "\n",
    "if missing_stage1_columns:\n",
    "    print(f\"Warning: Missing expected Stage 1 columns: {missing_stage1_columns}\")\n",
    "else:\n",
    "    print(\"All expected Stage 1 columns present.\")\n",
    "\n",
    "stage1_group_counts = (\n",
    "    uv_stage1.groupby(\"group\")[\"respondent\"].nunique()\n",
    "    .reset_index(name=\"respondent_count\")\n",
    "    .sort_values(\"group\")\n",
    ")\n",
    "\n",
    "stage1_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a43b0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>respondent</th>\n",
       "      <th>Short Form</th>\n",
       "      <th>Long Form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>104</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B</td>\n",
       "      <td>10</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>The Town</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>D</td>\n",
       "      <td>102</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>The Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>E</td>\n",
       "      <td>100</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "      <td>Mad Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>F</td>\n",
       "      <td>107</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>Abbot Elementary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group respondent        Short Form         Long Form\n",
       "0      A        104           Mad Max          The Town\n",
       "12     B         10          The Town           Mad Max\n",
       "22     C          1          The Town  Abbot Elementary\n",
       "38     D        102  Abbot Elementary          The Town\n",
       "55     E        100  Abbot Elementary           Mad Max\n",
       "69     F        107           Mad Max  Abbot Elementary"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_long_pairs = (\n",
    "    uv_stage1.loc[:, [\"group\", \"respondent\", \"Short Form\", \"Long Form\"]]\n",
    "    .drop_duplicates(subset=[\"group\", \"Short Form\", \"Long Form\"])\n",
    "    .sort_values([\"group\", \"Short Form\", \"Long Form\"])\n",
    ")\n",
    "\n",
    "short_long_pairs.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb769cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate respondents detected.\n"
     ]
    }
   ],
   "source": [
    "duplicate_respondents = uv_stage1[uv_stage1.duplicated(subset=\"respondent\", keep=False)]\n",
    "if duplicate_respondents.empty:\n",
    "    print(\"No duplicate respondents detected.\")\n",
    "else:\n",
    "    duplicate_respondents.sort_values(\"respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92662888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 83,\n",
       " 'columns': 17,\n",
       " 'stage1_source': 'c:/Users/ashra/Documents/NeuralSense/NeuralData/clients/544_WBD_CXCU/results/uv_stage1.csv'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_summary = {\n",
    "    \"rows\": len(uv_stage1),\n",
    "    \"columns\": len(uv_stage1.columns),\n",
    "    \"stage1_source\": uv_stage1_path.as_posix(),\n",
    "}\n",
    "stage1_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fa7e8",
   "metadata": {},
   "source": [
    "## Stage 2: Sensor Data\n",
    "We retain the Stage 1 roster as the authoritative respondent list and validate sensor coverage before computing biometric features. After confirming file availability and mappings, we run the full feature extraction pipeline with the finalized windowing and naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6e5a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor file coverage: 100.00%\n",
      "Stage 1 respondents with missing sensor exports: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>group</th>\n",
       "      <th>source_file</th>\n",
       "      <th>sensor_file_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [respondent, group, source_file, sensor_file_found]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_sensor_files = {\n",
    "    path.name\n",
    "    for path in (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    "}\n",
    "stage1_sensor_reference = uv_stage1.loc[:, [\"respondent\", \"group\", \"source_file\"]].copy()\n",
    "stage1_sensor_reference[\"sensor_file_found\"] = stage1_sensor_reference[\"source_file\"].apply(\n",
    "    lambda fname: fname in available_sensor_files if fname else False\n",
    ")\n",
    "coverage_rate = stage1_sensor_reference[\"sensor_file_found\"].mean()\n",
    "print(f\"Sensor file coverage: {coverage_rate:.2%}\")\n",
    "missing_sensor_records = stage1_sensor_reference.loc[\n",
    "    ~stage1_sensor_reference[\"sensor_file_found\"]\n",
    "].sort_values([\"group\", \"respondent\"])\n",
    "print(f\"Stage 1 respondents with missing sensor exports: {len(missing_sensor_records)}\")\n",
    "missing_sensor_records.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "571f5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared sensor dataframes: ['df_group', 'df_clean']\n"
     ]
    }
   ],
   "source": [
    "# Load stimulus annotations and key-moment timing tables\n",
    "stimulus_map = pd.read_csv(project_root / \"data\" / \"stimulus_rename.csv\")\n",
    "stimulus_map[\"group_letter\"] = stimulus_map[\"group\"].str.extract(r\"Group\\s*([A-F])\", expand=False).str.upper()\n",
    "stimulus_map_lookup = stimulus_map.set_index([\"group_letter\", \"stimulus_name\"]).sort_index()\n",
    "\n",
    "key_moments_raw = pd.read_csv(project_root / \"data\" / \"key_moments.csv\")\n",
    "time_columns = [\"Lead-up Duration\", \"Key moment Duration_LF\"]\n",
    "key_moments = key_moments_raw[[\"title\", *time_columns]].dropna(subset=[\"title\"]).copy()\n",
    "\n",
    "def hhmmss_to_ms(value):\n",
    "    \"\"\"Convert hh:mm:ss strings to integer milliseconds (None on failure).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        duration = pd.to_timedelta(text)\n",
    "    except ValueError:\n",
    "        # Handle mm:ss formatted entries by padding hours when possible\n",
    "        parts = text.split(\":\")\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                duration = pd.to_timedelta(f\"00:{text}\")\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    return int(duration.total_seconds() * 1000)\n",
    "\n",
    "key_moments[\"lead_up_ms\"] = key_moments[\"Lead-up Duration\"].apply(hhmmss_to_ms)\n",
    "key_moments[\"key_moment_ms\"] = key_moments[\"Key moment Duration_LF\"].apply(hhmmss_to_ms)\n",
    "key_moment_lookup = key_moments.set_index(\"title\")[\"lead_up_ms\"].to_dict()\n",
    "key_duration_lookup = key_moments.set_index(\"title\")[\"key_moment_ms\"].to_dict()\n",
    "\n",
    "stimulus_map_lookup.head()\n",
    "\n",
    "# Clear in-memory sensor dataframes to free memory\n",
    "import gc\n",
    "_sensor_df_names = []\n",
    "for _name, _obj in list(globals().items()):\n",
    "    if isinstance(_obj, pd.DataFrame) and \"SourceStimuliName\" in _obj.columns:\n",
    "        _sensor_df_names.append(_name)\n",
    "        del globals()[_name]\n",
    "gc.collect()\n",
    "print(f\"Cleared sensor dataframes: {_sensor_df_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "433227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "fac_columns = [\n",
    "    \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\",\n",
    "    \"Surprise\", \"Engagement\", \"Sentimentality\",\n",
    "    \"Confusion\", \"Neutral\",\n",
    " ]\n",
    "fac_adaptive_metrics = {\n",
    "    \"AdaptiveEngagement\": \"Adaptive Engagement\",\n",
    "    \"PositiveAdaptiveValence\": \"Positive Adaptive Valence\",\n",
    "    \"NegativeAdaptiveValence\": \"Negative Adaptive Valence\",\n",
    "    \"NeutralAdaptiveValence\": \"Neutral Adaptive Valence\",\n",
    "}\n",
    "eeg_columns = [\n",
    "    \"High Engagement\",\n",
    "    \"Low Engagement\",\n",
    "    \"Distraction\",\n",
    "    \"Drowsy\",\n",
    "    \"Workload Average\",\n",
    "    \"Frontal Alpha Asymmetry\",\n",
    " ]\n",
    "eeg_alternate_columns = {\n",
    "    \"Frontal Alpha Asymmetry\": [\"Frontal Alpha Asymmetry\", \"Frontal Asymmetry Alpha\"]\n",
    "}\n",
    "eeg_metric_alias = {\n",
    "    \"High Engagement\": \"HighEngagement\",\n",
    "    \"Low Engagement\": \"LowEngagement\",\n",
    "    \"Distraction\": \"Distraction\",\n",
    "    \"Drowsy\": \"Drowsy\",\n",
    "    \"Workload Average\": \"Workload\",\n",
    "    \"Frontal Alpha Asymmetry\": \"FrontalAlphaAsymmetry\",\n",
    "}\n",
    "sensor_required_columns = {\n",
    "    \"FAC\": fac_columns + list(fac_adaptive_metrics.values()),\n",
    "    \"EEG\": eeg_columns,\n",
    "    \"GSR\": [\"Peak Detected\"],\n",
    "    \"ET\": [\n",
    "        \"Blink Detected\",\n",
    "        \"Fixation Dispersion\",\n",
    "        \"Fixation Index\",\n",
    "        \"Fixation Duration\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def _trapezoid_integral(values: np.ndarray, time_axis: np.ndarray) -> float:\n",
    "    \"\"\"Integrate using numpy.trapezoid when available, falling back to trapz.\"\"\"\n",
    "    integrate = getattr(np, \"trapezoid\", np.trapz)\n",
    "    return float(integrate(values, x=time_axis) / 1000.0)\n",
    "\n",
    "def prepare_stimulus_segment(df_sensor: pd.DataFrame, raw_name: str, form: str, title: str) -> pd.DataFrame:\n",
    "    \"\"\"Return the time-zeroed slice for the requested stimulus, clipping to key moments when needed.\"\"\"\n",
    "    if \"SourceStimuliName\" not in df_sensor.columns or \"Timestamp\" not in df_sensor.columns:\n",
    "        return pd.DataFrame()\n",
    "    subset = df_sensor.loc[df_sensor[\"SourceStimuliName\"] == raw_name].copy()\n",
    "    if subset.empty:\n",
    "        return subset\n",
    "    subset[\"Timestamp\"] = pd.to_numeric(subset[\"Timestamp\"], errors=\"coerce\")\n",
    "    subset = subset.dropna(subset=[\"Timestamp\"])\n",
    "    if subset.empty:\n",
    "        return subset\n",
    "    subset.sort_values(\"Timestamp\", inplace=True)\n",
    "    if \"SlideEvent\" in subset.columns:\n",
    "        slide_events = subset[\"SlideEvent\"].astype(str)\n",
    "        start_candidates = subset.loc[slide_events == \"StartMedia\", \"Timestamp\"]\n",
    "    else:\n",
    "        start_candidates = pd.Series(dtype=float)\n",
    "    if not start_candidates.empty:\n",
    "        start_timestamp = start_candidates.iloc[0]\n",
    "    else:\n",
    "        start_timestamp = subset[\"Timestamp\"].min()\n",
    "    subset[\"time_from_start\"] = subset[\"Timestamp\"] - start_timestamp\n",
    "    if form == \"Long\":\n",
    "        lead_ms = key_moment_lookup.get(title)\n",
    "        duration_ms = key_duration_lookup.get(title)\n",
    "        if lead_ms is None or duration_ms is None:\n",
    "            return pd.DataFrame()\n",
    "        window_start = lead_ms\n",
    "        window_end = lead_ms + duration_ms\n",
    "        subset = subset.loc[(subset[\"time_from_start\"] >= window_start) & (subset[\"time_from_start\"] <= window_end)].copy()\n",
    "        if subset.empty:\n",
    "            return subset\n",
    "        subset[\"time_from_start\"] = subset[\"time_from_start\"] - window_start\n",
    "    return subset\n",
    "\n",
    "def register_feature(container: Dict[str, float], form: str, title: str, sensor: str, metric: str, method: str, value: float) -> None:\n",
    "    if value is None:\n",
    "        return\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return\n",
    "    key = f\"{form}_{title}_{sensor}_{metric}_{method}\"\n",
    "    container[key] = value\n",
    "\n",
    "def compute_sensor_features(segment: pd.DataFrame, form: str, title: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute FAC, EEG, GSR, and ET summary statistics for a stimulus segment.\"\"\"\n",
    "    features: Dict[str, float] = {}\n",
    "    if segment.empty:\n",
    "        return features\n",
    "    if \"time_from_start\" not in segment.columns or segment[\"time_from_start\"].empty:\n",
    "        return features\n",
    "    # Duration in seconds for this stimulus window\n",
    "    duration_ms = float(segment[\"time_from_start\"].max() - segment[\"time_from_start\"].min())\n",
    "    if duration_ms <= 0:\n",
    "        return features\n",
    "    duration_seconds = duration_ms / 1000.0\n",
    "    features[f\"{form}_{title}_duration\"] = duration_seconds\n",
    "    duration_minutes = duration_seconds / 60.0\n",
    "    # Facial coding summaries\n",
    "    for metric in fac_columns:\n",
    "        if metric not in segment.columns:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[metric], errors=\"coerce\").dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[values.index, \"time_from_start\"].values\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Mean\", float(values.mean()))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"AUC\", _trapezoid_integral(values.values, time_axis))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Binary\", int(values.max() >= 50))\n",
    "    for metric, column_name in fac_adaptive_metrics.items():\n",
    "        if column_name not in segment.columns:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[column_name], errors=\"coerce\").dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[values.index, \"time_from_start\"].values\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"Mean\", float(values.mean()))\n",
    "        register_feature(features, form, title, \"FAC\", metric, \"AUC\", _trapezoid_integral(values.values, time_axis))\n",
    "    # EEG summaries\n",
    "    for metric in eeg_columns:\n",
    "        candidate_columns = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "        actual_column = next((col for col in candidate_columns if col in segment.columns), None)\n",
    "        if actual_column is None:\n",
    "            continue\n",
    "        values = pd.to_numeric(segment[actual_column], errors=\"coerce\")\n",
    "        valid = values.loc[values > -9000].dropna()\n",
    "        valid = valid.loc[valid < 9000].dropna()\n",
    "        if valid.empty:\n",
    "            continue\n",
    "        time_axis = segment.loc[valid.index, \"time_from_start\"].values\n",
    "        label = eeg_metric_alias.get(metric, metric)\n",
    "        register_feature(features, form, title, \"EEG\", label, \"Mean\", float(valid.mean()))\n",
    "        register_feature(features, form, title, \"EEG\", label, \"AUC\", _trapezoid_integral(valid.values, time_axis))\n",
    "    # GSR summaries\n",
    "    if \"Peak Detected\" in segment.columns:\n",
    "        peak_series = pd.to_numeric(segment[\"Peak Detected\"], errors=\"coerce\").fillna(0)\n",
    "        peak_mask = peak_series >= 1\n",
    "        register_feature(features, form, title, \"GSR\", \"PeakDetected\", \"Binary\", int(peak_mask.any()))\n",
    "        if peak_mask.any():\n",
    "            segments = (peak_mask != peak_mask.shift()).cumsum()\n",
    "            peak_blocks = segments.loc[peak_mask]\n",
    "            peak_count = int(peak_blocks.nunique())\n",
    "            register_feature(features, form, title, \"GSR\", \"Peaks\", \"Count\", peak_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"GSR\", \"Peaks\", \"PerMinute\", float(peak_count / duration_minutes))\n",
    "    # ET metrics\n",
    "    if \"Blink Detected\" in segment.columns:\n",
    "        blink_series = pd.to_numeric(segment[\"Blink Detected\"], errors=\"coerce\").fillna(0)\n",
    "        blink_mask = blink_series >= 1\n",
    "        if blink_mask.any():\n",
    "            segments = (blink_mask != blink_mask.shift()).cumsum()\n",
    "            blink_blocks = segments.loc[blink_mask]\n",
    "            blink_count = int(blink_blocks.nunique())\n",
    "            register_feature(features, form, title, \"ET\", \"Blink\", \"Count\", blink_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"ET\", \"Blink\", \"Rate\", float(blink_count / duration_minutes))\n",
    "    if \"Fixation Dispersion\" in segment.columns:\n",
    "        dispersion = pd.to_numeric(segment[\"Fixation Dispersion\"], errors=\"coerce\").dropna()\n",
    "        if not dispersion.empty:\n",
    "            register_feature(features, form, title, \"ET\", \"FixationDispersion\", \"Mean\", float(dispersion.mean()))\n",
    "    if \"Fixation Index\" in segment.columns:\n",
    "        fixation_index = pd.to_numeric(segment[\"Fixation Index\"], errors=\"coerce\").dropna()\n",
    "        if not fixation_index.empty:\n",
    "            fixation_count = int(fixation_index.nunique())\n",
    "            register_feature(features, form, title, \"ET\", \"Fixation\", \"Count\", fixation_count)\n",
    "            if duration_minutes > 0:\n",
    "                register_feature(features, form, title, \"ET\", \"Fixation\", \"PerMinute\", float(fixation_count / duration_minutes))\n",
    "    if \"Fixation Duration\" in segment.columns:\n",
    "        fixation_duration = pd.to_numeric(segment[\"Fixation Duration\"], errors=\"coerce\").dropna()\n",
    "        if not fixation_duration.empty:\n",
    "            register_feature(features, form, title, \"ET\", \"FixationDuration\", \"Mean\", float(fixation_duration.mean()))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feddd2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>FAC_data_missing</th>\n",
       "      <th>EEG_data_missing</th>\n",
       "      <th>GSR_data_missing</th>\n",
       "      <th>ET_data_missing</th>\n",
       "      <th>Short_The Big Bang Theory_duration</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_Mean</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_AUC</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Anger_Binary</th>\n",
       "      <th>Short_The Big Bang Theory_FAC_Contempt_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_Titanic_EEG_FrontalAlphaAsymmetry_AUC</th>\n",
       "      <th>Short_Titanic_GSR_PeakDetected_Binary</th>\n",
       "      <th>Short_Titanic_GSR_Peaks_Count</th>\n",
       "      <th>Short_Titanic_GSR_Peaks_PerMinute</th>\n",
       "      <th>Short_Titanic_ET_Blink_Count</th>\n",
       "      <th>Short_Titanic_ET_Blink_Rate</th>\n",
       "      <th>Short_Titanic_ET_FixationDispersion_Mean</th>\n",
       "      <th>Short_Titanic_ET_Fixation_Count</th>\n",
       "      <th>Short_Titanic_ET_Fixation_PerMinute</th>\n",
       "      <th>Short_Titanic_ET_FixationDuration_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.751817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>591.261358</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.098343</td>\n",
       "      <td>0.261135</td>\n",
       "      <td>156.0</td>\n",
       "      <td>156.333511</td>\n",
       "      <td>459.838233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent  FAC_data_missing  EEG_data_missing  GSR_data_missing  \\\n",
       "0          2                 0                 1                 0   \n",
       "1         58                 0                 0                 0   \n",
       "2        116                 0                 0                 0   \n",
       "\n",
       "   ET_data_missing  Short_The Big Bang Theory_duration  \\\n",
       "0                0                              186.82   \n",
       "1                0                                 NaN   \n",
       "2                0                                 NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_Mean  \\\n",
       "0                                       0.0   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_AUC  \\\n",
       "0                                      0.0   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Anger_Binary  \\\n",
       "0                                         0.0   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   Short_The Big Bang Theory_FAC_Contempt_Mean  ...  \\\n",
       "0                                     0.001964  ...   \n",
       "1                                          NaN  ...   \n",
       "2                                          NaN  ...   \n",
       "\n",
       "   Short_Titanic_EEG_FrontalAlphaAsymmetry_AUC  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                     4.751817   \n",
       "\n",
       "   Short_Titanic_GSR_PeakDetected_Binary  Short_Titanic_GSR_Peaks_Count  \\\n",
       "0                                    NaN                            NaN   \n",
       "1                                    NaN                            NaN   \n",
       "2                                    1.0                          590.0   \n",
       "\n",
       "   Short_Titanic_GSR_Peaks_PerMinute  Short_Titanic_ET_Blink_Count  \\\n",
       "0                                NaN                           NaN   \n",
       "1                                NaN                           NaN   \n",
       "2                         591.261358                          46.0   \n",
       "\n",
       "   Short_Titanic_ET_Blink_Rate  Short_Titanic_ET_FixationDispersion_Mean  \\\n",
       "0                          NaN                                       NaN   \n",
       "1                          NaN                                       NaN   \n",
       "2                    46.098343                                  0.261135   \n",
       "\n",
       "   Short_Titanic_ET_Fixation_Count  Short_Titanic_ET_Fixation_PerMinute  \\\n",
       "0                              NaN                                  NaN   \n",
       "1                              NaN                                  NaN   \n",
       "2                            156.0                           156.333511   \n",
       "\n",
       "   Short_Titanic_ET_FixationDuration_Mean  \n",
       "0                                     NaN  \n",
       "1                                     NaN  \n",
       "2                              459.838233  \n",
       "\n",
       "[3 rows x 943 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Compute pilot sensor features for selected respondents\n",
    "sensor_file_index = {\n",
    "    path.name: path\n",
    "    for path in (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    "}\n",
    "\n",
    "pilot_ids = [\"2\", \"58\", \"116\"]\n",
    "pilot_subset = uv_stage1.loc[uv_stage1[\"respondent\"].astype(str).isin(pilot_ids)].copy()\n",
    "pilot_subset[\"respondent_numeric\"] = pd.to_numeric(pilot_subset[\"respondent\"], errors=\"coerce\")\n",
    "pilot_subset = pilot_subset.sort_values([\"respondent_numeric\", \"respondent\"])\n",
    "\n",
    "pilot_feature_rows = []\n",
    "pilot_issue_log = []\n",
    "\n",
    "for _, row in pilot_subset.iterrows():\n",
    "    respondent_id = str(row[\"respondent\"]).strip()\n",
    "    group_letter = str(row.get(\"group\", \"\")).strip().upper() if pd.notna(row.get(\"group\")) else None\n",
    "    source_file = row.get(\"source_file\")\n",
    "    if not source_file or source_file not in sensor_file_index:\n",
    "        pilot_issue_log.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"stimulus\": None,\n",
    "            \"issue\": \"Sensor export not located.\",\n",
    "        })\n",
    "        continue\n",
    "    df_sensor, _ = read_imotions(sensor_file_index[source_file])\n",
    "    feature_row: Dict[str, float] = {\"respondent\": respondent_id}\n",
    "    try:\n",
    "        if df_sensor.empty or \"SourceStimuliName\" not in df_sensor.columns:\n",
    "            pilot_issue_log.append({\n",
    "                \"respondent\": respondent_id,\n",
    "                \"stimulus\": None,\n",
    "                \"issue\": \"Sensor export missing SourceStimuliName column.\",\n",
    "            })\n",
    "            continue\n",
    "        # Assess sensor coverage for this respondent\n",
    "        for sensor_label, required_columns in sensor_required_columns.items():\n",
    "            if sensor_label == \"EEG\":\n",
    "                missing_metrics = []\n",
    "                for metric in required_columns:\n",
    "                    candidates = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "                    if not any(column in df_sensor.columns for column in candidates):\n",
    "                        missing_metrics.append(metric)\n",
    "                feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_metrics))\n",
    "                if missing_metrics:\n",
    "                    metrics_display = \", \".join(missing_metrics)\n",
    "                    pilot_issue_log.append({\n",
    "                        \"respondent\": respondent_id,\n",
    "                        \"stimulus\": None,\n",
    "                        \"issue\": f\"Missing EEG columns: {metrics_display}.\",\n",
    "                    })\n",
    "                continue\n",
    "            missing_columns = [col for col in required_columns if col not in df_sensor.columns]\n",
    "            feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_columns))\n",
    "            if missing_columns:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": None,\n",
    "                    \"issue\": f\"Missing {sensor_label} columns: {', '.join(missing_columns)}.\",\n",
    "                })\n",
    "        unique_stimuli = sorted({str(s).strip() for s in df_sensor[\"SourceStimuliName\"].dropna().unique()})\n",
    "        for raw_stimulus in unique_stimuli:\n",
    "            lookup_key = (group_letter, raw_stimulus)\n",
    "            if lookup_key not in stimulus_map_lookup.index:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"Stimulus missing from rename map.\",\n",
    "                })\n",
    "                continue\n",
    "            map_row = stimulus_map_lookup.loc[lookup_key]\n",
    "            if isinstance(map_row, pd.DataFrame):\n",
    "                map_row = map_row.iloc[0]\n",
    "            title = map_row[\"title\"]\n",
    "            form = map_row[\"form\"]\n",
    "            if form == \"Long\" and (key_moment_lookup.get(title) is None or key_duration_lookup.get(title) is None):\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"Key moment timing not defined for long-form title.\",\n",
    "                })\n",
    "                continue\n",
    "            segment = prepare_stimulus_segment(df_sensor, raw_stimulus, form, title)\n",
    "            if segment.empty:\n",
    "                pilot_issue_log.append({\n",
    "                    \"respondent\": respondent_id,\n",
    "                    \"stimulus\": raw_stimulus,\n",
    "                    \"issue\": \"No data after windowing (check key moment timings).\",\n",
    "                })\n",
    "                del segment\n",
    "                continue\n",
    "            try:\n",
    "                features = compute_sensor_features(segment, form, title)\n",
    "                if not features:\n",
    "                    pilot_issue_log.append({\n",
    "                        \"respondent\": respondent_id,\n",
    "                        \"stimulus\": raw_stimulus,\n",
    "                        \"issue\": \"No features computed for segment.\",\n",
    "                    })\n",
    "                    continue\n",
    "                feature_row.update(features)\n",
    "            finally:\n",
    "                del segment\n",
    "        pilot_feature_rows.append(feature_row)\n",
    "    finally:\n",
    "        del df_sensor\n",
    "        gc.collect()\n",
    "\n",
    "pilot_features = pd.DataFrame(pilot_feature_rows)\n",
    "pilot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effd718c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/ashra/Documents/NeuralSense/NeuralData/clients/544_WBD_CXCU/results/uv_biometric_pilot_features.csv')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge pilot features back into the UV and review any issues\n",
    "uv_stage1[\"respondent\"] = uv_stage1[\"respondent\"].astype(str)\n",
    "if not pilot_features.empty:\n",
    "    pilot_features[\"respondent\"] = pilot_features[\"respondent\"].astype(str)\n",
    "pilot_uv = (\n",
    "    uv_stage1.loc[uv_stage1[\"respondent\"].isin(pilot_ids)]\n",
    "    .copy()\n",
    "    .merge(pilot_features, on=\"respondent\", how=\"left\")\n",
    ")\n",
    "\n",
    "pilot_output_path = project_root / \"results\" / \"uv_biometric_pilot_features.csv\"\n",
    "safe_write_csv(pilot_uv, pilot_output_path)\n",
    "pilot_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "056f38e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>Missing EEG columns: High Engagement, Low Enga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent stimulus                                              issue\n",
       "0          2     None  Missing EEG columns: High Engagement, Low Enga..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = pd.DataFrame(pilot_issue_log)\n",
    "issues_df.sort_values([\"respondent\", \"stimulus\"], na_position=\"last\") if not issues_df.empty else \"No issues logged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4ee526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent</th>\n",
       "      <th>title</th>\n",
       "      <th>observed_seconds</th>\n",
       "      <th>expected_seconds</th>\n",
       "      <th>diff_seconds</th>\n",
       "      <th>within_tolerance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The Town</td>\n",
       "      <td>262.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Mad Max</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>The Town</td>\n",
       "      <td>262.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent     title  observed_seconds  expected_seconds  diff_seconds  \\\n",
       "0          2  The Town             262.0             262.0          0.01   \n",
       "1         58   Mad Max             225.0             225.0          0.00   \n",
       "2        116  The Town             262.0             262.0          0.00   \n",
       "\n",
       "   within_tolerance  \n",
       "0              True  \n",
       "1              True  \n",
       "2              True  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate long-form durations against key-moment specifications\n",
    "tolerance_seconds = 3\n",
    "duration_columns = [col for col in pilot_features.columns if col.endswith(\"_duration\")]\n",
    "validation_records = []\n",
    "for _, feat_row in pilot_features.iterrows():\n",
    "    respondent_id = feat_row.get(\"respondent\")\n",
    "    for col in duration_columns:\n",
    "        value = feat_row.get(col)\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        form = col.split('_', 1)[0]\n",
    "        if form != \"Long\":\n",
    "            continue\n",
    "        title = col[len(form) + 1: -len(\"_duration\")]\n",
    "        expected_ms = key_duration_lookup.get(title)\n",
    "        if expected_ms is None:\n",
    "            continue\n",
    "        observed_ms = float(value) * 1000.0\n",
    "        diff_seconds = abs(observed_ms - expected_ms) / 1000.0\n",
    "        validation_records.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"title\": title,\n",
    "            \"observed_seconds\": round(observed_ms / 1000.0, 2),\n",
    "            \"expected_seconds\": round(expected_ms / 1000.0, 2),\n",
    "            \"diff_seconds\": round(diff_seconds, 2),\n",
    "            \"within_tolerance\": diff_seconds <= tolerance_seconds,\n",
    "        })\n",
    "duration_validation = pd.DataFrame(validation_records)\n",
    "duration_validation if not duration_validation.empty else \"No long-form durations to validate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457f5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sensor_feature_pipeline(stage1_df=None, respondent_ids=None, export_label=\"uv_stage2_full\", save_outputs=True):\n",
    "    \"\"\"Compute sensor features for the specified respondents and optionally persist outputs.\"\"\"\n",
    "    sensor_file_index = {\n",
    "        path.name: path\n",
    "        for path in (project_root / \"data\" / \"Export\").glob(\"Group */Analyses/*/Sensor Data/*.csv\")\n",
    "    }\n",
    "    if stage1_df is None:\n",
    "        base_stage1 = uv_stage1.copy()\n",
    "    else:\n",
    "        base_stage1 = stage1_df.copy()\n",
    "    base_stage1[\"respondent\"] = base_stage1[\"respondent\"].astype(str).str.strip()\n",
    "    base_stage1 = base_stage1.loc[base_stage1[\"respondent\"] != \"\"]\n",
    "    base_stage1[\"group\"] = base_stage1[\"group\"].astype(str).str.strip()\n",
    "    base_stage1[\"source_file\"] = base_stage1[\"source_file\"].apply(\n",
    "        lambda value: str(value).strip() if pd.notna(value) and str(value).strip() else None\n",
    "    )\n",
    "    if respondent_ids is None:\n",
    "        target_ids = sorted({str(r).strip() for r in base_stage1[\"respondent\"]})\n",
    "    else:\n",
    "        cleaned_ids = [str(r).strip() for r in respondent_ids if pd.notna(r)]\n",
    "        target_ids = sorted({identifier for identifier in cleaned_ids if identifier})\n",
    "    if not target_ids:\n",
    "        raise ValueError(\"No respondents provided for sensor feature processing.\")\n",
    "    subset = base_stage1.loc[base_stage1[\"respondent\"].isin(target_ids)].copy()\n",
    "    if subset.empty:\n",
    "        raise ValueError(\"No matching respondents found in Stage 1 roster for the requested IDs.\")\n",
    "    subset[\"respondent_numeric\"] = pd.to_numeric(subset[\"respondent\"], errors=\"coerce\")\n",
    "    subset = subset.sort_values([\"respondent_numeric\", \"respondent\"])\n",
    "    feature_rows = []\n",
    "    issue_rows = []\n",
    "\n",
    "    def log_issue(respondent_id, group, stimulus, message):\n",
    "        issue_rows.append({\n",
    "            \"respondent\": respondent_id,\n",
    "            \"group\": group,\n",
    "            \"stimulus\": stimulus,\n",
    "            \"issue\": message,\n",
    "        })\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        respondent_id = str(row[\"respondent\"]).strip()\n",
    "        group_letter = str(row.get(\"group\", \"\")).strip().upper() if pd.notna(row.get(\"group\")) else None\n",
    "        source_file = row.get(\"source_file\")\n",
    "        if not source_file:\n",
    "            log_issue(respondent_id, group_letter, None, \"Sensor export not located in Stage 1 roster.\")\n",
    "            continue\n",
    "        if source_file not in sensor_file_index:\n",
    "            log_issue(respondent_id, group_letter,None, f\"Sensor export {source_file} not found on disk.\")\n",
    "            continue\n",
    "        df_sensor, _ = read_imotions(sensor_file_index[source_file])\n",
    "        feature_row: Dict[str, float] = {\"respondent\": respondent_id}\n",
    "        try:\n",
    "            if df_sensor.empty or \"SourceStimuliName\" not in df_sensor.columns:\n",
    "                log_issue(respondent_id,group_letter, None, \"Sensor export missing SourceStimuliName column.\")\n",
    "                continue\n",
    "            for sensor_label, required_columns in sensor_required_columns.items():\n",
    "                if sensor_label == \"EEG\":\n",
    "                    missing_metrics = []\n",
    "                    for metric in required_columns:\n",
    "                        candidates = [metric, *eeg_alternate_columns.get(metric, [])]\n",
    "                        if not any(column in df_sensor.columns for column in candidates):\n",
    "                            missing_metrics.append(metric)\n",
    "                    feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_metrics))\n",
    "                    if missing_metrics:\n",
    "                        log_issue(respondent_id,group_letter, None, f\"Missing EEG columns: {', '.join(missing_metrics)}.\")\n",
    "                    continue\n",
    "                missing_columns = [col for col in required_columns if col not in df_sensor.columns]\n",
    "                feature_row[f\"{sensor_label}_data_missing\"] = int(bool(missing_columns))\n",
    "                if missing_columns:\n",
    "                    log_issue(respondent_id, group_letter,None, f\"Missing {sensor_label} columns: {', '.join(missing_columns)}.\")\n",
    "            unique_stimuli = sorted({str(s).strip() for s in df_sensor[\"SourceStimuliName\"].dropna().unique()})\n",
    "            for raw_stimulus in unique_stimuli:\n",
    "                lookup_key = (group_letter, raw_stimulus)\n",
    "                if lookup_key not in stimulus_map_lookup.index:\n",
    "                    log_issue(respondent_id,group_letter, raw_stimulus, \"Stimulus missing from rename map.\")\n",
    "                    continue\n",
    "                map_row = stimulus_map_lookup.loc[lookup_key]\n",
    "                if isinstance(map_row, pd.DataFrame):\n",
    "                    map_row = map_row.iloc[0]\n",
    "                title = map_row[\"title\"]\n",
    "                form = map_row[\"form\"]\n",
    "                if form == \"Long\" and (key_moment_lookup.get(title) is None or key_duration_lookup.get(title) is None):\n",
    "                    log_issue(respondent_id, group_letter,raw_stimulus, \"Key moment timing not defined for long-form title.\")\n",
    "                    continue\n",
    "                segment = prepare_stimulus_segment(df_sensor, raw_stimulus, form, title)\n",
    "                if segment.empty:\n",
    "                    log_issue(respondent_id,group_letter, raw_stimulus, \"No data after windowing (check key moment timings).\")\n",
    "                    del segment\n",
    "                    continue\n",
    "                try:\n",
    "                    features = compute_sensor_features(segment, form, title)\n",
    "                    if not features:\n",
    "                        log_issue(respondent_id,group_letter, raw_stimulus, \"No features computed for segment.\")\n",
    "                        continue\n",
    "                    feature_row.update(features)\n",
    "                finally:\n",
    "                    del segment\n",
    "            feature_rows.append(feature_row)\n",
    "        finally:\n",
    "            del df_sensor\n",
    "            gc.collect()\n",
    "\n",
    "    features_df = pd.DataFrame(feature_rows)\n",
    "    issues_df = pd.DataFrame(issue_rows)\n",
    "    merged_uv = (\n",
    "        base_stage1.loc[base_stage1[\"respondent\"].isin(target_ids)]\n",
    "        .copy()\n",
    "    )\n",
    "    merged_uv[\"respondent\"] = merged_uv[\"respondent\"].astype(str)\n",
    "    if not features_df.empty:\n",
    "        features_df[\"respondent\"] = features_df[\"respondent\"].astype(str)\n",
    "        merged_uv = merged_uv.merge(features_df, on=\"respondent\", how=\"left\")\n",
    "\n",
    "    if save_outputs:\n",
    "        results_dir = project_root / \"results\"\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        features_path = results_dir / f\"{export_label}_features.csv\"\n",
    "        uv_path = results_dir / f\"{export_label}_uv.csv\"\n",
    "        issues_path = results_dir / f\"{export_label}_issues.csv\"\n",
    "        safe_write_csv(features_df, features_path)\n",
    "        safe_write_csv(merged_uv, uv_path)\n",
    "        if not issues_df.empty:\n",
    "            issues_sorted = issues_df.sort_values([\"respondent\", \"stimulus\"], na_position=\"last\")\n",
    "            safe_write_csv(issues_sorted, issues_path)\n",
    "    gc.collect()\n",
    "    return features_df, issues_df, merged_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2884ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n",
      "C:\\Users\\ashra\\AppData\\Local\\Temp\\ipykernel_42864\\1364558604.py:57: DtypeWarning: Columns (3,4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, header=header_rows, low_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features for 83 respondents; 34 issues logged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83, 1076)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Stage 2 sensor feature extraction for the full respondent list\n",
    "biometric_features, biometric_issues, biometric_uv = run_sensor_feature_pipeline(\n",
    "    stage1_df=uv_stage1,\n",
    "    export_label=\"uv_biometric_stage2\",\n",
    "    save_outputs=True\n",
    ")\n",
    "print(\n",
    "    f\"Computed features for {len(biometric_features)} respondents; \"\n",
    "    f\"{len(biometric_issues)} issues logged.\"\n",
    ")\n",
    "biometric_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44c8c6",
   "metadata": {},
   "source": [
    "## UV Merge\n",
    "Combine the biometric feature matrix with the self-report unified view to produce a single dataset that carries both survey and sensor-derived metrics. Outputs are written with biometric-specific filenames to avoid clashing with the self-report notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6907ec53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_report_rows': 83,\n",
       " 'biometric_feature_rows': 83,\n",
       " 'merged_rows': 83,\n",
       " 'biometric_columns_added': 1075,\n",
       " 'respondents_missing_all_biometric_features': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv_self_report_path = project_root / \"results\" / \"uv_merged.csv\"\n",
    "if not uv_self_report_path.exists():\n",
    "    raise FileNotFoundError(f\"Self-report UV not found at {uv_self_report_path}\")\n",
    "\n",
    "uv_self_report = pd.read_csv(uv_self_report_path)\n",
    "uv_self_report[\"respondent\"] = uv_self_report[\"respondent\"].astype(str).str.strip()\n",
    "\n",
    "if biometric_features.empty:\n",
    "    print(\"No biometric features computed; skipping biometric merge.\")\n",
    "    uv_biometric_full = uv_self_report.copy()\n",
    "    added_columns = []\n",
    "else:\n",
    "    biometric_features_clean = biometric_features.copy()\n",
    "    biometric_features_clean[\"respondent\"] = biometric_features_clean[\"respondent\"].astype(str).str.strip()\n",
    "    overlap_columns = [\n",
    "        col\n",
    "        for col in biometric_features_clean.columns\n",
    "        if col != \"respondent\" and col in uv_self_report.columns\n",
    "    ]\n",
    "    if overlap_columns:\n",
    "        rename_map = {col: f\"biometric_{col}\" for col in overlap_columns}\n",
    "        biometric_features_clean = biometric_features_clean.rename(columns=rename_map)\n",
    "    added_columns = [col for col in biometric_features_clean.columns if col != \"respondent\"]\n",
    "    uv_biometric_full = uv_self_report.merge(\n",
    "        biometric_features_clean,\n",
    "        on=\"respondent\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "uv_biometric_full_path = project_root / \"results\" / \"uv_biometric_full.csv\"\n",
    "safe_write_csv(uv_biometric_full, uv_biometric_full_path)\n",
    "\n",
    "merge_summary = {\n",
    "    \"self_report_rows\": len(uv_self_report),\n",
    "    \"biometric_feature_rows\": len(biometric_features),\n",
    "    \"merged_rows\": len(uv_biometric_full),\n",
    "    \"biometric_columns_added\": len(added_columns),\n",
    "}\n",
    "if added_columns:\n",
    "    missing_all = int(uv_biometric_full[added_columns].isna().all(axis=1).sum())\n",
    "    merge_summary[\"respondents_missing_all_biometric_features\"] = missing_all\n",
    "merge_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
