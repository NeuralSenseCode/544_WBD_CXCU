# Time-Series Notebook Task Plan

## Task Description
Develop a new `analysis/time_series.ipynb` notebook that produces aggregated, form-aligned sensor time-series visualisations for the target biometric metrics. The notebook must reuse the existing Stage 2 sensor ingestion utilities, respect the key-moment alignment rules for long-form presentations, apply the legacy binning and filtering logic from `analysis/support/AdNeuroV2.py`, and render figures with the same styling conventions used in `analysis/analysis_self-report.ipynb`.

## Numbered Plan
1. **Re-establish Context** – Review `PROGRESS.md`, `ANALYSIS PROGRESS.md`, and the relevant sections in `analysis/assemble_uv_biometric.ipynb` to confirm current sensor-processing assumptions, naming conventions, and existing outputs that the time-series workflow should build upon.
2. **Audit Reference Implementations** – Examine `analysis/support/AdNeuroV2.py` to catalogue the historic binning, filtering, and aggregation logic (window sizes, smoothing, baseline handling) used for the legacy Frontal Asymmetry Alpha, High Engagement, and Workload Average metrics.
3. **Inventory `wbdlib` Helpers** – Review existing utilities in `analysis/wbdlib/` to identify reusable loaders, windowing, binning, and plotting helpers; document which ones can be called directly so new notebook code stays thin and any enhancements are made inside the library with backward-compatible signatures.
4. **Map Required Metrics** – Validate that each requested sensor column (FAC, EEG, ET, GSR) exists in the raw iMotions exports, note aliases (e.g., `Frontal Asymmetry Alpha` vs `Frontal Alpha Asymmetry`), and define additional preprocessing requirements (NA handling, scaling, thresholding) per metric family.
4. **Define Windowing Rules** – Use `stimulus_rename.csv` and `key_moments.csv` to codify stimulus-to-title mappings, the long-form key-moment start/end timestamps, and the short-form full-duration windows; confirm that long-form key-moment durations match their short-form counterparts.
5. **Design Respondent-Level Extraction** – Outline the procedure to load each respondent’s sensor CSV, isolate the relevant stimulus segments (short-form full window, long-form key moment), and standardise timestamps so all segments start at zero prior to binning.
6. **Specify Binning Strategy** – Determine bin widths, alignment rules, and minimum data density thresholds per metric (reusing or extending the AdNeuroV2 approach) to ensure comparable sample counts across respondents and forms before aggregation; note where existing `wbdlib` binning helpers can be reused or require extension.
7. **Plan Signal Processing Steps** – Document any smoothing, filtering, or normalisation that must occur after binning (e.g., moving averages, detrending) and decide whether processing differs by sensor family; record parameter choices for reproducibility and flag any library updates needed to support them.
8. **Aggregate Across Respondents** – Define how binned data will be combined (mean, median, confidence bands), how to handle unequal respondent counts per bin, and how to capture diagnostics (e.g., contributor counts per bin, missing respondents) for quality checks.
9. **Visualisation Specification** – Draft the plotting blueprint for dual-line time-series charts (Short vs Long key moment) that mirrors `analysis/analysis_self-report.ipynb` aesthetics (fonts, palettes, legend placement, annotations) and plan supporting tables/exports (e.g., aggregated time-series CSVs under `results/`).
10. **Notebook Structure & Outputs** – Sketch the cell layout for `analysis/time_series.ipynb` (imports, config, data prep, processing functions, validation, plotting) and list all artefacts to generate (figures, CSV summaries, issue logs) so implementation can proceed methodically in the next phase while keeping helper logic within `wbdlib`.

## Next Phase: Notebook Implementation & Visualisation
1. **Create Notebook Scaffold** *(completed)* – Instantiate `analysis/time_series.ipynb` with configuration cells that mirror `analysis/analysis_self-report.ipynb` styling (font settings, seaborn/matplotlib themes, palette definitions) and declare all required imports.
2. **Surface Library Helpers** *(completed)* – Import and, where needed, extend `wbdlib` utilities instead of redefining logic in the notebook; expose new helpers such as `bin_biometric_time_series` via `wbdlib.__all__`, re-home legacy smoothing/binning logic from `AdNeuroV2.py` into the library, and keep notebook code as orchestration only.
3. **Assemble Respondent-Level Pipelines** *(completed)* – Implement functions that ingest each respondent’s sensor CSV, extract the short-form window and long-form key moment, apply binning, and perform post-bin signal processing while logging data sufficiency diagnostics; ensure the notebook pulls exclusively from raw sensor exports (not the unified view) and caches per-respondent outputs.
4. **Build Aggregation Framework** *(not started)* – Develop routines to combine respondent bins into cohort-level series (means plus SEM/CI bands), track contributing respondent-count per bin, and write intermediary CSV exports for audit.
5. **Render Time-Series Figures** *(not started)* – Generate dual-line plots (Short vs Long key moment) for every target metric/title combination, ensuring axes, legends, colour assignments, and typography match the self-report notebook conventions; capture figures to disk.
6. **Validate & Document Outputs** *(not started)* – Add cells that summarise processing coverage (missing files, bins below threshold), display sample diagnostics, and save a concise README-style recap of generated artefacts within `tasks/13-11-25 Time series/` for traceability.

- 2025-11-13: Completed Step 1 – refreshed context by reviewing `PROGRESS.md`, `ANALYSIS PROGRESS.md`, and `analysis/assemble_uv_biometric.ipynb` to reconfirm current sensor-processing assumptions, naming conventions, and existing biometric outputs.
- 2025-11-13: Completed Step 2 – reviewed `analysis/support/AdNeuroV2.py`, catalogued legacy binning (1-second windows with overlapping smoothing), EEG detrending, and FAC baseline adjustments to inform the planned time-series workflow.
- 2025-11-13: Completed Step 3 – inventoried `analysis/wbdlib` helpers (sensor IO, stimulus mapping, binning utilities, plotting configs) and noted candidate functions for reuse such as `reshape_biometric_long`, `summarise_biometric_structure`, and duration alignment routines; flagged gaps where extensions may be required for broader metric coverage.
- 2025-11-13: Completed Step 4 – reviewed `stimulus_rename.csv` and `key_moments.csv`, verified long-form titles all have key-moment definitions, and documented window start/end rules to align long-form key moments with full short-form durations.
- 2025-11-13: Completed Step 5 – drafted the respondent-level extraction procedure: reuse `run_sensor_feature_pipeline` components for CSV loading, call `prepare_stimulus_segment` for windowing, and capture per-respondent segments before binning; noted required adaptations for additional metrics and confirmed availability gaps via `reshape_biometric_long` (will drive follow-up checks in later steps).
- 2025-11-13: Completed Step 6 – confirmed raw sampling interval (~5 ms) and defined bin widths: 0.5 s bins for Frontal Alpha Asymmetry, 1 s bins for other EEG/FAC/ET/GSR metrics, with a minimum 50% bin coverage threshold and plans for count-weighted aggregation when respondent data fall short; documented rollover handling for bins that straddle key-moment endpoints.
- 2025-11-13: Completed Step 7 – outlined post-binning processing: carry forward AdNeuro-style smoothing (two-pole Butterworth low-pass at 0.06 Hz for Workload averages, 3-bin moving mean for other EEG metrics, z-score normalisation optional per title), leave binary FACS/GSR metrics unsmoothed, and capture both raw and smoothed outputs so notebooks can toggle between views; flagged the need to surface these operations via `wbdlib` helpers rather than notebook-defined code.
- 2025-11-13: Completed Step 8 – specified respondent aggregation: compute mean and median per bin, add SEM and 95% CI using contributor counts, retain `n_contributors` and `total_counts` diagnostics, and fall back to median-only when <5 respondents populate a bin; plan to emit both wide (per metric) and long (tidy) exports plus an issues log for bins failing coverage thresholds.
- 2025-11-13: Completed Step 9 – drafted the dual-line plotting spec: reuse `COLOR_MAP` (Short vs Long), set fonts to match `analysis/analysis_self-report.ipynb` (`Fira Sans`/`Fira Mono`), apply shared axis/legend styling, overlay SEM ribbons, annotate key moments, and plan optional facet grids (one metric per row) plus static PNG exports written under `results/time_series/`.
- 2025-11-13: Completed Step 10 – sketched the `analysis/time_series.ipynb` section flow (setup, configuration, data inputs, processing helpers, per-respondent extraction, aggregation, visualisation, diagnostics), ensuring each block reuses `wbdlib` helpers, emits CSV/figure artefacts, and logs milestones back to the task Progress section for traceability.
- 2025-11-13: Implementation Step 1 – instantiated `analysis/time_series.ipynb` with introductory markdown, roadmap outline, and styling/import scaffold (Century Gothic fonts, seaborn theme, shared COLOR_MAP palette) matching the self-report notebook conventions.
- 2025-11-14: Removed the unified-view sensor cell from `analysis/time_series.ipynb` so the notebook now relies solely on raw per-respondent exports; captured follow-up actions to expose the new binning helper via `wbdlib`, reinstate AdNeuro-style smoothing/binning inside the library, and continue with aggregation, visualisation, and diagnostics.
- 2025-11-14: Implementation Step 2 – promoted key time-series helpers into `analysis/wbdlib` (stimulus mapping, key-moment parsing, Butterworth filters) and refactored the notebook extraction scaffold to call the shared utilities, keeping analysis code orchestration-focused.
- 2025-11-14: Implementation Step 3 – introduced respondent-level processing pipelines via `wbdlib.process_sensor_time_series`, returning cached bins, diagnostics, and issue logs; rewired the notebook to use the shared helper and persist per-respondent outputs for later aggregation.
- 2025-11-19: Implementation Step 3 validation – tightened sensor CSV loading to only required columns, coerced numeric metrics, adjusted bin coverage heuristics, and confirmed the per-respondent extraction cell completes without errors while logging an empty issues file.
- 2025-11-19: Implementation Step 4 (in progress) – added `wbdlib.aggregate_binned_time_series`, wired the notebook to write aggregated cohort-level bins plus contributor summaries, and persisted aggregated caches under `results/time_series_cache/` ready for visualisation.
